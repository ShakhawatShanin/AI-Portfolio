{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4045d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a741ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF files\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8353925c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shanin/Desktop/SHANIN/MAIN/ALL_CODE/LLM'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd  # '/home/shanin/Desktop/SHANIN/MAIN/ALL_CODE/LLM/Portfolio'\n",
    "\n",
    "import os \n",
    "os.chdir(\"../\")\n",
    "\n",
    "%pwd  # '/home/shanin/Desktop/SHANIN/MAIN/ALL_CODE/LLM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e55b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1'}, page_content='AMDNet23: A combined deep Contour-based Convolutional Neural Network and Long \\nShort Term Memory system to diagnose Age-related Macular Degeneration \\nMd. Aiyub Ali1, Md. Shakhawat Hossain1, Md.Kawar Hossain1, Subhadra Soumi Sikder1, \\nSharun Akter Khushbu1 and Mirajul Islam1 \\n1 Department of Computer Science and Engineering, Daffodil International University, Dhaka \\n1341, Bangladesh \\nCorrespondence: Mirajul Islam; merajul15-9627@diu.edu.bd \\nAbstract \\nIn light of the expanding population, an automated framework of disease detection can assist doctors in the \\ndiagnosis of ocular diseases, yields accurate, stable, rapid outcomes, and improves the success rate of early \\ndetection. The work initially intended the enhancing the quality of fundus images by employing an adaptive \\ncontrast enhancement algori thm (CLAHE) and Gamma correction. In the preprocessing techniques, \\nCLAHE elevates the local contrast of the fundus image and gamma correction increases the intensity of \\nrelevant features. This study operates on a AMDNet23 system of deep learning that combi ned the neural \\nnetworks made up of convolutions (CNN) and short-term and long-term memory (LSTM) to automatically \\ndetect aged macular degeneration (AMD) disease from fundus ophthalmology. In this mechanism, CNN is \\nutilized for extracting features and LSTM is utilized to detect the extracted features. The dataset of this \\nresearch is collected from multiple sources and afterward applied quality assessment techniques, 2000 \\nexperimental fundus images encompass four distinct classes equitably. The proposed hybri d deep \\nAMDNet23 model demonstrates to detection of AMD ocular disease and the experimental result achieved \\nan accuracy 96.50%, specificity 99.32%, sensitivity 96.5%, and F1 -score 96.49.0%. The system achieves \\nstate-of-the-art findings on fundus imagery dat asets to diagnose AMD ocular disease and findings \\neffectively potential of our method. \\n \\nKeywords: AMDNet23, Fundus image classification, CNN-LSTM, ocular diseases, automated diagnosis, \\nconvolutional neural networks, long short-term memory,  early detection, Medical imaging, diagnosis. \\n \\nIntroduction \\nOver the past two decades, ocular diseases ( ODs) that can cause blindness have become extremely \\nwidespread. ODs encompass a wide range of c onditions that can affect various components of the ocular, \\nincluding the corneal tissue, lens, retina, optic nerves and periorbital tissues. Ocular diseases include \\nabnormalities such as cataracts, untreated nearsightedness, trachoma, macular degeneration associated with \\naging, and diabetes -associated retinopathy. These ailments play a substantial role in global retinal \\ndegeneration and visual impairment. [1]. The worldwide prevalence of near - or farsightedness vision \\ndeficiency affects over 2.2 billion in dividuals [2]. Approximately half of the total cases, amounting to at \\nleast 1 billion folks, as reported by the World Health Organization (WHO), suffer from vision impairments \\nthat could have been evaded or remain unattended. Among these individuals, aroun d 88.4 million have \\nuntreated refractive errors leading to adequate to extensive distant impaired vision, nearly ninety -four \\nmillion owned cataracts,  and eight million individuals are possessed by aged-related macular degeneration, \\ndiabetic retinopathy (3.9 million). [3] Despite significant investment, the number of individuals living with \\nvision loss might increase to 1.7 billion by 2050, from the 1.1 billion people accomplished in the year 2020. \\nAge-associated macular degeneration (AMD) predominantly strikes the older demographic, resulting in the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2'}, page_content='gradual deterioration of the macula, a crucial part of the retina in charge of the central region of perception. \\nThe consequences of AMD manifest as central vision abnormalities, including blurred or distorted vision, \\nwhich significantly impede various daily activities[4].  \\nAccurate and earlier identification of AMD disease explicitly a vital role in safeguarding irreversible \\ndamage to vision and initiating timely treatment and safeguarding ocular health. Machine  learning \\ntechniques have advanced to the point where early identification of aged macular degeneration eye illness \\nby an automated system has significant advantages over manual detection [5] . As aids in diagnosing eye \\ndiseases, digital pictures of the eye  and computational intelligence (CI)-based technologies assist doctors \\nin diagnosis. In the realm of diagnosing eye diseases, digital eye images and computational intelligence \\n(CI)-based technologies serve as indispensable tools, enabling doctors to enhanc e their diagnostic \\ncapabilities[6]. In medical imaging, there are also various approaches are employed including fundus \\nphotography, [7] optical coherence tomography (OCT), and imaging modalities specifically designed for \\nthe eye. These imaging technologie s allow for detailed visualization and analysis of ocular structures, \\nfacilitating the identification of characteristic features and abnormalities associated with age -related \\nmacular degeneration diseases.  \\nSeveral researchers have demonstrated a critical task in ophthalmology, facilitating the early detection and \\ndiagnosis of aged macular degeneration (AMD) ocular disease using fundus image.  \\nResearchers have focused on deep learning [8-10], The fields of  vision for computing [11,12]  and the use \\nof predictive learning of machines [13,14] method have used to develop robust classification models to \\nidentify retinal images into AMD disease categories accurately. The incorporation of deep learning \\nmethodologies [57,58] plays a pivotal role in accurately class ifying diverse ocular diseases, thereby \\nensuring the advancement of intelligent healthcare practices in the field of ophthalmology [15]. \\nTherefore, this paper seeks to demonstrate a novel system employing a AMDNet23 framework, the deep \\nmechanism of CNN and LSTM networks is combined for the automated identification of AMD from fundus \\nphotography. Within this approach, CNN performs the purpose of extracting fundus features, and LSTM \\nundertakes the crucial task of classification AMD constructed on the extracte d features. Internal memory \\ninside the LSTM network empowers it to learning knowledge gained from significant experiences with \\nextended period of condition. In the fully interconnected networks, each layer is linked comprehensively, \\nand the nodes in betwee n layers construction are unconnected, and  LSTM nodes connection within a \\ndirected graph accompined a temporal order, which serves as an input with a specific form [16] . The hybrid \\ntwo dimensional CNN and LSTM system combination improves classification of AMD ophthalmology and \\nassists clinical decision, the dataset collected from several sources and preprocessing technique for the \\nimage quality enhancemnet to classify AMD efficiently. The assets of this research have been articulated \\nin the following.  \\n \\na) Constructing a combination of CNN -LSTM based AMDNet23 framework for the automated diagnosis \\nof  AMD and aiding clinical physician in the early detection of patients. \\nb) The collection data has investigated by employing the contour -based quality assessmen t technique in \\nidentifying the structure of fundus photography, Ocular illumination levels fundus images are automatically \\neliminated. \\nc) To enhance image quality, CLAHE improves the visibility of subtle details and enhances local contrast \\nand Gamma correction adjusts the intensity levels, improving image quality and facilitating better diagnosis \\nof AMD.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3'}, page_content=\"d) AMDNet23 hybrid framework for detection of AMD utilizing fundus image ophthalmology, data \\ncomprising 2000 images equitively. \\ne) An empirical evaluation is accessible encompassing accuracy, specificity, sensitivity, F1-measure, and a \\nconfusion matrix to assess the effectiveness of the proposed method. \\n \\nThe rest of the contents of this article are arranged a manner as follows: Section II covers the related works \\nof this research. Section III Section III articulates the proposed AMDNet23 methodology, including data \\ncollection and preprocessing techniques, and a comparison of some existing models. Section IV covers the \\nexperimental findings and discussion, inc luding state-of-the-art and transfer learning comparisons. The \\nconclusion is presented in Section V. \\n \\n \\n \\nRelated work \\nIn the pursuit of identifying the ocular disease, researchers have harnessed the power of deep learning \\ntechniques. These methods leverage fundus ophthalmology to facilitate the diagnosis of ocular diseases. \\nThis reviewed literature presents cutting -edge systems that developed deep -learning techniques for \\ndetecting AMD, diabetes, and cataracts. \\nM Sahoo et. al[17] proposed an innovative ensemb le-based prediction model called weighted majority \\nvoting (WMV) for the exclusive diagnosis of Dry -AMD. This approach intelligently combines the \\npredictions from various base classifiers, utilizing assigned weights to each classifier. The WMV model \\ndemonstrates remarkable accuracy, achieving 96.15% and 96.94% accuracy rates, respectively. P \\nMuthukannan et. al. [18] introduced a computer aided approach that leverages the Flower pollination \\noptimization approach (FPOA) in accompanied with a CNN mechanism for preprocessing, specifically \\nutilizing the maximum entropy transformation on the ODIR public dataset. The model's performance was \\nthen benchmarked against other optimized models, demonstrating superior accuracy at 95.27%. In a study \\nby Serener et al. [19], their goal was to employ OCT images and deep neural networks to detect both dry \\nand wet AMD. Regarding the purpose, two architectures—AlexNet and ResNet—were used. The outcomes \\nrevealed that the eighteen -layer ResNet model correctly identified AMD with an astounding accuracy of \\n94%, whereas the AlexNet model produced an accuracy of 63%. \\nThere are several deep learning methods for detecting cataracts, because of the drawbacks of feature \\nextraction and preprocessing, these methods don't always produce adequat e results. Kumar et al. [20] \\nproposed several models to improve clinical decision -making for ophthalmologists. Paradisa et al. [21] \\nFundus images applied the Concatenate model with For feature extraction, Inception -ResNet V2 and \\nDenseNet121 are implemented , and MLP is deployed for classification and average accuracy was 91%. \\nFaizal et al.  [22] An automated cataract detection algorithm using CNN achieves high accuracy (95%) by \\nanalyzing visible wavelength and anterior segment images, enabling cost -effective early detection of \\nvarious cataract types. Pahuja et al. [23]  To enhance the model performance, data augmentation and \\nmethods to extract features have been performed. Therefore they used CNN and SVM models for the \\ndetection of cataract on a dataset compr ising normal and cataract retinal images, achieving high accuracy \\nof 87.5% for SVM and 85.42% for CNN. Hence, et al. [24] used a CNN model to diagnose cataract \\npathology with digital camera images. The model achieves high accuracy (testing: 0.9925, training: 0.9980) \\nwhile optimizing processing time. It demonstrates the potential of CNNs for cataract diagnosis. Although \\net al [25] used color fundus images to detect cataracts.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4'}, page_content=\"A variety of computer vision engineering approaches are used to forecast the Diabet ic retinopathy (DR)'s \\noccurrences and phases automatically. Mondal et al. [26] Their model is a collaborative deep neural system \\nfor automated diabetes-related retinopathy (DR) diagnosis and categorization using two models: modified \\nDenseNet101 and ResNeXt. Experiments were conducted on APTOS19 and DIARETDB1 datasets, with \\ndata augmentation using GAN-based techniques. Results show higher accuracy with accuracy for each of \\nthe five classes reached 86.08%, whilst for each of the two classes the score was 96.98%. Whereas, a ML-\\nFEC model with pre-trained CNN architecture was proposed for Diabetic Retinopathy (DR) detection using \\nResNet50, ResNet152, and SqueezeNet1. On testing with DR datasets, ResNet50 achieved 93.67%, \\nSqueezeNet1 achieved 91.94%, and ResNet152  achieved 94.40% accuracy, demonstrating its suitability \\nfor clinical implementation and large-scale screening programs. Using a novel CNN model, Babenko et al. \\n[27] were able to multi-class categorize retinal fundus pictures from a publically accessible dataset with an \\naccuracy of 81.33% for diabetic eye disease. Priorly  based on UNet architecture, et al. [28] achieved \\n95.65% accuracy in identifying red lesions and 94% accuracy in classifying DR levels of severity. The \\napproach was examined utilizing publically accessible datasets: IDRiD (99% specificity, 89% sensitivity) \\nand MESSIDOR (94% accuracy, 93.8% specificity, 92.3% sensitivity). \\nMethodology \\nAging macular degenerative disorder (AMD) is an advancing retinal condition that predominantly impacts \\nindividuals over the age of 50. This eye disease can significantly affect eyesight, leading to various visual \\nissues like blurred or distorted vision, where straight lines might appear wavy or twisted. Moreover, it \\ncauses a loss of central vision, the emergence of dark or empty spots at the center of vision, and alterations \\nin color perception. Thus taking proactive steps to prevent eye diseases is crucial for maintaining clear and \\nvibrant sight throughout our lives. In recent years, neural networks containing layers of convolution (CNNs) \\nhave demonstrated considerable potential in the processing of medical images. It has the remarkable \\ncapacity to recognize and extract meaningful features from images automatically.  Figure 1 outlines the \\nsteps in developing the proposed CNN-based methods for AMD eye disease detection:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5'}, page_content='Figure 1:  Overall proposed-based Method \\nA. Data Collection: \\nThe case of normal class represents the absence of any specific eye disease or condition. A healthy eye \\nfunctions optimally, providing clear and unimpaired vision.  Diabetes, a systemic disease characterized by \\nelevated blood sugar levels, can lead to various ocular complications. Diabetic ocular disorders, notably \\ndiabetic retinopathy, occur when the blood vessels found in the retina undergo damage as a consequence of \\nelevated blood sugar concentration [29]. Older individuals are predominantly affected by age -associated \\nmacular degeneration (AMD) and involves the progressive deterioration of the macula, a small but crucial \\nof the core vision-related region of the retina. AMD can lead to blurred or distorted central vision, impacting \\nactivities [30]. A cataract is another common eye condition, particularly associated with aging. It involves \\nclouding the crystalline lens inside the eye, leading to inconsistent or foggy vision [31]. Fig. 2. Shows the \\nsample images of Normal, Cataract, AMD and Diabetes respectively.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6'}, page_content=\"Figure 2: Types of fundus ophthalmology \\nTo train a robust CNN model, a diverse and well -annotated dataset of AMD and non-AMD eye images is \\nessential. The dataset employed in this study containing a total of 2000 images, was put together by \\nassessing the quality of the images from six other public datasets. Those datasets are ODIR[32], DR -\\n200[33], Fundus Dataset[34] , RFMiD[35], ARIA[36], and Eye_Diseases_Classification[37]. From these \\ndatasets. The quality assessment was done using contour techniques[38]. The contour -based approach \\nfocuses on the sharpness and clarity of edges, as they play a crucial role in human as sess the quality of \\nimagery. Which included illumination level, visibility structure, color and contrast, and direct eye image. \\nFigure 3 represents a sample of the assessed image quality. \\n \\nFigure 3:  Contour-based approach \\nHere it can be seen that sharp, well-defined edges contribute to high -quality images, while blurry or \\ndistorted edges indicate poor quality. Such poor -quality images would negatively impact the machine's \\nperception. We put together a dataset consisting of four classes: Normal, Diabetes, AMD, and Cataract,\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7'}, page_content='where each class contains 500 images. Table 1 indicates the quantity of accessible and selected images \\n(within the first bracket) from those six public datasets. \\nDatasets Normal Diabetes AMD Cataract \\nODIR[33] 2873(168) 1608(300) 266(266) 293(256) \\nDR-200[34] 1000(332) 1000(150)   \\nFundus Dataset[35]   46(46) 100(44) \\nRFMiD[36]  376(50) 100(100)  \\nARIA[37]   101(88)  \\nEye_Diseases_Classification[38]    1038(200) \\n \\nB.Data Pre-processing: \\nPreprocessing, which is the strong suits of the prop osed work, was focused on enhancing image quality, \\nand some of the preprocessing techniques applied to this work were not used by the previously proposed \\ncataract disease detection works. The data were preprocessed in different color spaces (as shown in Figure \\n4) for extracting the features while bettering the practicability of our models. Among the RGB(G), HSV(V), \\nand LAB(L) color spaces, the vessels were visible in the LAB(L) color space. As a result, the LAB(L) color \\nspace was chosen.  \\n \\nFigure 4: Color spaces \\nLater, some preprocessing algorithms like CLAHE and Gamma correction[39] were applied to enhance \\nimage quality by adjusting the brightness and contrast. We experimented with several parameters for these \\nalgorithms and finally got the satisfying result for CLAHE(2.0, (8,8)). For gamma values of 0.5, the image \\nis found to become darkened. Moreover, for gamma values of 2.0, the image is found quite faded. To \\novercome this problem CLAHE is used to enhance regional contrasting, making the image more visua lly \\nappealing and informative[40]. Figure 5 shows the resulting images for our experimented algorithms along \\nwith the finalized CLAHE (2.0, (8,8)) for our model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8'}, page_content='Figure 5: Gamma and CLAHE based quality enhancement \\nHistogram comparison between the applied Context-limited adapted equalization of histograms (CLAHE) \\npreprocessing algorithm and a normal image in Figure 6 can help illustrate the effects of CLAHE on \\nenhancing local contrast and improving image quality.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9'}, page_content='Figure 6: Hitogram comparisons \\nBy comparing the histograms, we can observe the changes in pixel intensity distribution before and after \\napplying CLAHE. In the normal image, the histogram exhibits a relatively uniform distribution of pixel \\nintensities, with some variations depending on th e content of the image. CLAHE is implemented as a \\ncomponent in the preprocessing, it adapts the contrast enhancement locally, making it particularly effective \\nin improving the contrast of regions with varying intensities. This helps reveal hidden details a nd textures \\nthat might have been obscured in the original image. \\n \\nImages MSE PSNR SSIM \\n2376_left.jpg 2388.13 14.35 0.48 \\n84_right.jpg 2189.98 14.72 0.65 \\n980_right.jpg 927.54 18.45 0.53 \\n71_left.jpg 709.37 19.62 0.54 \\n \\nThe effectiveness of the quality of image preprocessing, Table 2 displays the readings of the metrics mean \\nsquare errors (MSE), Peak Signal -to-Noise Ratio (PSNR), and Structures Similarity Index (SSIM) [41].'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10'}, page_content=\"These metrics compare the preprocessed image to the original image to determine the level of distortion or \\nsimilarity. \\nMean Squared Error (MSE ): MSE generates  the resultant mean squared disparity between the \\npreprocessed and original image's pixel values. Lesser MSE reading indicate greater similarity between the \\nimages. MSE is calculated using the formula:  \\n𝑀𝑆𝐸 = 1\\n𝑚∗𝑛 ∑ ∑[𝐼(𝑥,𝑦)−𝐾(𝑥,𝑦)]2\\n𝑛−1\\n𝑦=0\\n𝑚−1\\n𝑥=0\\n \\nwhere𝑚∗𝑛 represents the image dimensions, 𝐼(𝑥,𝑦) and 𝐾(𝑥,𝑦) indicate preprocessed image’s pixel \\nvalues and original images at coordinates(𝑥,𝑦). \\nPeak Signal-to-Noise Ratio (PSNR):  The optical appealing of preprocessed photographs is frequently \\nassessed utilizing the PSNR measure. It calculates a measure of the peak power ratio of the signal's strength \\nto noise, which assessed in decibels (dB). Increased PSNR values indicated a greater similarity between the \\nimages. PSNR is calculated using the formula: \\n𝑃𝑆𝑁𝑅 = 10𝑙𝑜𝑔10\\n𝑀𝐴𝑋2\\n𝑀𝑆𝐸  \\nwherein MAX is the highest pixel value that is permitted to be used (for example, 255 in 8-bit photographs). \\nStructural Similarity Index (SSIM): SSIM evaluates the luminosity, contrary, and structural similarities \\nbetween the preprocessed image and original image. The value of 1 denotes complete similarity, using \\nSSIM readings varying from -1 to 1. Higher SSIM values indicate better similarity between the images. \\nSSIM is calculated using a combination of mean, variance, and covariance of the image patches. \\n𝑆𝑆𝐼𝑀 = (2𝜇𝑥𝜇𝑦 +𝑐1)(2𝜎𝑥𝑦 +𝑐2)\\n(𝜇𝑥2 +𝜇𝑦2 +𝑐1)(𝜎𝑥2 +𝜎𝑦2 +𝑐2) \\nwhere c1 and c2 are constants to prohibit division by zero, σ and μ stand representing the standard deviations \\nand mean respectively. \\nBy calculating MSE, PS NR, and SSIM before and after image preprocessing, The effectiveness of the \\npreprocessing techniques in preserving image quality or reducing noise, artifacts, or other undesired effects \\ncan be determined. Lesser MSE readings, greater PSNR readings, and greater SSIM values indicate better \\nimage quality preservation. \\n \\n \\nC. Comparison of some existing models  \\nTransfer Learning: \\nIn this research study, a handful of models were trained and evaluated. Some of those models are discussed \\nbelow in the following sections: \\n \\ni.ViTB16\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11'}, page_content=\"The ViTB16 model, also known as Vision Transformer Base with a depth of 16 layers, is a deep learning \\narchitecture specifically designed for image classification tasks [42]. 224*224 pixels were made up of the \\nsize of the input images. A grid  of patches containing fixed sizes is used to divide the input image. Each \\nindividual patch is positioned linearly to obtain a lower-dimensional representation. The patch embeddings \\nare enhanced by employing positional encoding to provide the model with spatial information. The model \\nis capable of finding relationships dependencies between different patches thanks to a self -attention \\nmechanism. It calculates attention scores between all pairs of patches and applies weighted averaging to \\naggregate information. Layer normalization is applied after the self -attention mechanism to normalize the \\noutput and improve training stability. SGD (Stochastic Gradient Descent) optimizer was employed for \\ntraining the model, and the learning rate was 0.0001. \\nii.DenseNet121, DenseNet169:  \\nDenseNet[43] is a famous deep -learning architecture known for its dense connections between layers, \\nenabling effective feature reuse and alleviating the vanishing gradient problem. DenseNet121 and \\nDenseNet169 have 121 and 169 layers, respectively, making DenseNet121 a relatively shallow variant than \\nDenseNet169. DenseNet121 has fewer parameters compared to DenseNet169, which makes it more \\nmemory-efficient and faster to train. DenseNet121 performs well on various image classification tasks but \\nmay not capture as fine -grained features as deeper models. DenseNet169 performs better than \\nDenseNet121, especially when the dataset is larger and more complex. Choosing between DenseNet121 \\nand DenseNet169 for a particular purpose like AMD classification, it is essential to consider the size and \\ncomplexity of the dataset. Since the dataset used in this study was small, DenseNet121 should have been \\nthe model to pick, but we experimented with all DenseNet variants. \\niii. InceptionResnetV2 \\nA powerful convolutional neural network conception that incorporates the Inception and ResNet \\nmodules is termed the InceptionResNetV2 system [44]. It was proposed as an extension to the \\noriginal Inception and ResNet models designed to improve image classification efficient tas ks. \\nThe InceptionResNetV2 model was initialized with pre -trained ImageNet weights, excluding the \\ntop classification layers. The pre -trained layers were frozen to prevent them from being updated \\nduring training. The Adam optimizer was utilized for bettering  the model and the training was \\ndone with an epoch size of 100. \\nD. AMDNet23: \\nAMDNet23 Combining CNNs which are convolutional neural systems and long -term short-term memory \\nnetworks (LSTM) is designated as CNN -LSTM where the strengths of CNNs in image featu re extraction \\nare combined with the temporal modeling capabilities of LSTM networks[45]. Before feeding the images \\nto the model, employed a diverse set of augmentation techniques to enhance the training data. These \\nincluded randomized horizontal and vertical flipping through a probability of fifty percent each and applied \\nrandom brightness adjustments by varying the brightness level within a range of -0.1 to +0.1. To further \\nincrease variation, used random contrast adjustments with factors ranging from 0.8 to 1.2, as well as random \\nsaturation adjustments within the bounds of 0.8 to 1.2. Moreover, introduced random hue adjustments to \\nadd subtle color variations. Lastly, to augment the dataset further, performed translation -based width and \\nheight shifting with a specific range to the input images. The augment strategy emphasizes data diversity \\nand improves the model's broad ability to diagnose unknown data[46]. The model is designed with a depth \\nof 23 layers.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12'}, page_content=\"Input Layer: The input to the AMDNet23 model is a collection of eye images captured from patients. The \\ninput images are represented as a tensor X with dimensions (N, W, H, C), where N corresponds to the eye \\nimage’s number, and W and H represent The width and length of the images(The model received imagery \\nthat measured 256 X 256 in size.) respectively. C denotes the number of color channels in the eye images. \\nThis tensor X is then passed into the model's input layer.     \\nConvolutional Layers (CNN): After the initial input layer, the CNN component of the mod el comprises \\nmultiple convolutional layers[47]. Every individual layer of convolution utilizes a set of adjustable filters \\nto process the input images. The resulting Features of the map from the 𝑖𝑡ℎ convolutional layer are denoted \\nas 𝐹𝑖, with 𝑖 ranging from 1 to 𝑛. The output feature map 𝐹𝑖 can be computed as follows: \\n𝐹𝑖 = 𝐶𝑜𝑛𝑣2𝐷(𝑋,𝑊𝑖)+𝑏𝑖 \\nWhere Conv2D refers to the convolution operation, 𝑊𝑖 represents the tr ainable parameters (weights) \\nspecific to the 𝑖𝑡ℎ convolutional layer, and 𝑏𝑖 represents the corresponding biases associated with that layer. \\nThe output feature maps 𝐹𝑖 have spatial dimensions (W', H') and C' channels. \\nThe model contained six  convolutional blocks. The first four convolutional blocks consisted of 2 \\nconvolutional layers and 1 batch normalization layer each. The filters were 32, 64, 128, and 256 for the first \\nfour blocks, where the kernel size was 3 X 3. The fifth and sixth convo lutional blocks consisted of 3 \\nconvolution layers and 1 batch normalization layer. All the convolution layers of the fifth and sixth blocks \\nconsisted of 512 filters. \\nPooling Layers:  After the layers based on convolution, layers of pooled [48] are frequentl y used to \\ndownsample the feature maps. Let us denote the output feature maps after pooling as 𝑃𝑖, where 𝑖 ranges \\nfrom 1 to 𝑝 (the total number of pooling layers). Each pooling layer performs a downsampling operation \\non the input feature maps. After applying all pooling layers, the resulting feature maps can be denoted as \\n𝑃𝑝 and have spatial dimensions (W'', H'') and C'' channels. The pool size for max-pooling layers was 2 X 2 \\nfor all the convolutional blocks. \\nLSTM Layer: The system conveys the development and significance of networks having long-term short-\\nterm memory (LSTM), an advancement residing in conventional Recurrent neural networks to delve into \\nthe motivation behind LSTM's creation, specifically to address the vanishing gradient problem , which \\nformerly hindered the effective training of RNNs on long sequences [49]. Behind LSTM it introduces \\nmemory cells, enabling the network to retain information over extended periods. The mechanism empowers \\nLSTMs in effectively capturing long-term dependencies within the input data. The cell state adds a long -\\nterm memory to flows the entire sequence [50]. It enables information to be retained or discarded selectively \\nutilizing the input entrance, forget gatekeeper, and output gateway, which constitute th e three main gates. \\nThe LSTM cell computations can be mathematically represented as follows, where 𝑡 denotes the current \\ntime step, 𝑥𝑡 rrepresents what was the input entered at time 𝑡, ℎ𝑡denotes the previous hidden stated, and \\n𝑐𝑡signifies the cell state: \\n \\n \\n \\n𝑖𝑡 = 𝜎(𝑊𝑖[𝑥𝑡,ℎ𝑡−1]+𝑏𝑖)...(1)\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13'}, page_content='𝐶 𝑡 = 𝑡𝑎𝑛ℎ(𝑊𝑐[𝑥𝑡,ℎ𝑡−1]+𝑏𝑐)...(2) \\n𝐶𝑡 = 𝑓𝑡𝐶𝑡−1 +𝑖𝑡𝐶 𝑡...(3) \\nThe input gate (1) uses a sigmoid function to combine the previous output ℎ𝑡−1and the present time input \\n𝑥𝑡, deciding the proportion of information to be incorporated into the cell state and (2) employes to obtain \\nnew information through the tanh layer to be added into current cell state 𝐶 𝑡. The current cell state 𝐶 𝑡, and \\nlong term information 𝐶𝑡−1 are combination into 𝐶𝑡(3) whereas 𝑤𝑖 determines the sigmoid output and 𝐶 𝑡 \\ndetermines to tanh output. “Forget” gate (4) investigates how much of the previous cell state should be \\nretained and carried over to the next time step by assessing probability where 𝑊𝑓 and 𝑏𝑓 refers to the offset \\nand weight matrix and offset respectively. \\n𝑓𝑡 = 𝜎(𝑊𝑓[𝑥𝑡,ℎ𝑡−1]+𝑏𝑓)...(4) \\nThe output gate of the LSTM investigates by ℎ𝑡−1 and 𝑥𝑡inputs following (4) and (5) passed through the \\nactivation function to determine what portion of information to be appeared from the current LSTM unit at \\ntimestamp t. \\n𝑜𝑡 = 𝜎(𝑊𝑜[𝑥𝑡,ℎ𝑡−1]+𝑏𝑜)...(5) \\nℎ𝑡 = 𝑜𝑡𝑡𝑎𝑛ℎ(𝐶𝑡)...(6)  \\nIn the above equation, 𝑊𝑜refers to the matrices of the output gate and 𝑏𝑜refers LSTM bias respectively.  \\nOutput Layer: The output layer delivers the ultimate prediction regarding the existence or non -existence \\nof AMD in the input eye images. We can represent the input tensor to the output layer as 𝐻𝑜𝑢𝑡, obtained by \\nreshaping 𝐻𝑙𝑠𝑡𝑚 to have dimensions (NT, D). A function of activation throughout softmax is positioned \\nfollowing its dense layer to the output section. The dense layer takes the input 𝐻𝑜𝑢𝑡 and transforms it to \\ngenerate the output tensor Y, which has dimensions (NT, K). Here, K specifies the number of output classes \\nthe model is classifying.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14'}, page_content='Figure 7: CNN-LSTM system \\nThe AMDNet23 model (Figure 7) for AMD ocular disease detection leverages the complementary strengths \\nof CNNs in spatial feature extraction and LSTMs in modeling sequential dependencies.  \\nLayer Type Kernel Size Kernel Input Size \\n1 Convolution2D 3 X 3 32 256 X 256 X 3 \\n2 Convolution2D 3 X 3 32 256 X 256 X 32'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15'}, page_content='3 Maxpooling2D 2 X 2 - 256 X 256 X 32 \\n4 Convolution2D 3 X 3 64 128 X 128 X 32 \\n5 Convolution2D 3 X 3 64 128 X 128 X 64 \\n6 Maxpooling2D 2 X 2 - 128 X 128 X 64 \\n7 Convolution2D 3 X 3 128 64 X 64 X 64 \\n8 Convolution2D 3 X 3 128 64 X 64 X 128 \\n9 Maxpooling2D 2 X 2 - 64 X 64 X 128 \\n10 Convolution2D 3 X 3 256 32 X 32 X 128 \\n11 Convolution2D 3 X 3 256 32 X 32 X 256 \\n12 Maxpooling2D 2 X 2 - 32 X 32 X 256 \\n13 Convolution2D 3 X 3 512 16 X 16 X 256 \\n14 Convolution2D 3 X 3 512 16 X 16 X 512 \\n15 Convolution2D 3 X 3 512 16 X 16 X 512 \\n16 Maxpooling2D 2 X 2 - 16 X 16 X 512 \\n17 Convolution2D 3 X 3 512 8 X 8 X 512 \\n18 Convolution2D 3 X 3 512 8 X 8 X 512 \\n19 Convolution2D 3 X 3 512 8 X 8 X 512 \\n20 Maxpooling2D - - 8 X 8 X 512 \\n21 LSTM - - 16 X 512'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16'}, page_content=\"22 FC - 64 524352 \\n23 Output - 4 260 \\nIn this research, An innovative and novel technique was devised to automatically detect AMD by \\nleveraging four distinct types of fundus images. This unique architecture synergizes the power of \\nNeural Networks of Convolutional and Long Short -Term Memory. The CNN module is \\nadministered for extracting intricate features from fundus imaging, and the LSTM module serves \\nas the classifier. The proposed AMDNet23 hybrid network for AMD detection consists of 23 \\nlayers: It includes 14 convolutional layers placed and 6 layers used for pooling,one fully \\ninterconnected  layer of (FC), a layer of LS TM and a single output layer with a sense of softmax \\nfunctionality. In our construction, an individual convolution block is made comprising between \\ntwo or three 2 -dimensional CNN's, A layer with a level of pooling and a layer comprising a \\ntwentieth percent dropout rate have of dropouts. Utilizing a convolutional layer with 3x3 kernels \\nand the ReLU function, the feature extraction is carried out efficiently. The input image undergoes \\ndimension reduction using A layer for maximum pooling of 2 × 2 kernels. The  resulting output \\nstructure was discovered (none, 4, 4, 512). the input size inside the layer of LSTM transforms to \\n(16, 512) whenever incorporating the reshaped approach. Combining these two neural network \\narchitectures allows the model to effectively ana lyze eye images, capturing both local spatial \\npatterns and temporal relationships, ultimately enabling accurate AMD diagnosis. The \\nsummarized architecture is presented in Table 2. \\nEvaluation Criteria \\nIn this study, as many as 13 models were experimented an d the evaluation of all those models will be \\npresented in this section of the paper. Considering the following evaluation criteria, the performance, \\nreliability, and clinical relevance of a AMD detection system can be assessed and also can be determined \\nits suitability for assisting medical professionals in accurately detecting and diagnosing AMD. \\nAccuracy: The accuracy of the AMD detection system in correctly classifying images as AMD, diabetes, \\ncataracts is a crucial evaluation criterion. It evaluates the  correctness of the system's detection computed \\noverall. \\n𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = 𝑇𝑃 +𝑇𝑁\\n𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁 \\nPrecision: The proportion of properly identified AMD situations is examined to measure precision out of \\nall predicted AMD cases.  \\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝑇𝑃\\n𝑇𝑃 +𝐹𝑃 \\nSensitivity and Specificity : Sensitivity is typically referred to by the term the true positive rate, which \\ngauges the system to identify AMD cases correctly. True negative rate, which is often referred to as \\nspecificity, assesses its capacity of identifying non-AMD conditions. Both metrics provide insights into the \\nsystem's performance in different classes and help assess its ability to avoid false positives and false \\nnegatives.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17'}, page_content='𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦 = 𝑇𝑃\\n𝑇𝑃+𝐹𝑁 \\n𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦 = 𝑇𝑁\\n𝑇𝑁+𝐹𝑃 \\nF1 Score: The F1 score offers a comprehensive measurement that addresses the balance between precision \\nand memory and thus represents a harmonious average of precision and recall. It is advantageous in realities \\nwhereby there occurs a disparity in class or in cases when the costs of false positives and false negatives \\nfluctuate. \\n𝐹1𝑆𝑐𝑜𝑟𝑒 = 𝑇𝑃\\n𝑇𝑃+1\\n2(𝐹𝑃+𝐹𝑁)\\n \\nWhen evaluating model performance, it is crucial to consid er a combination of these evaluation criteria \\naccordance with the precise specifications of the completion and the area of expertise. Selecting appropriate \\nmetrics and interpreting the results will help determine the effectiveness and suitability of the model. Table \\n2 showcases the retained value for these evaluation metrics: \\nModel Accuracy Precision Sensitivity Specificity F1 Score \\nViTB16 95.25% 95.26% 95.25% 98.66% 95.24% \\nMobileViT_XXS 83.00% 82.58% 83.00% 98.28% 82.41% \\nInceptionResNetV2 47.50% 36.26% 47.50% 82.67% 40.50% \\n EfficientNetB7 92.75% 92.94% 92.75% 98.97% 92.62% \\n EfficientNetB6 92.75% 92.74% 92.75% 97.00% 92.69% \\nDenseNet121 82.25% 82.41% 82.25% 95.00% 81.46% \\nDenseNet169 81.25% 81.24% 81.25% 92.74% 80.31% \\nDenseNet201 84.75% 84.45% 84.75% 93.46% 84.52% \\nInceptionV3 72.25% 71.53% 72.25% 90.16% 70.71% \\nMobileNetV2 71.75% 71.68% 71.75% 88.01% 71.19% \\nVGG16 89.75% 89.82% 89.75% 94.68% 89.78%'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18'}, page_content=\"VGG19 89.00% 88.96% 89.00% 95.62% 88.80% \\nResNet50 93.25% 93.37% 93.25% 96.73% 93.17% \\nAMDNet23 96.50% 96.51% 96.50% 99.32% 96.49% \\nIn conclusion, the AMDNet23 model for AMD ocular disease detection demonstrated strong performance \\nin accurately identifying AMD from eye images. Its high accuracy, precision, and recall values, along with \\nthe robust AUC-ROC score, validate its potential as a reliable tool for early detection and intervention. The \\nmodel's efficiency makes it suitable for practical deployment in healthcare settings, contributing to \\nimproved patient care and timely treatment of AMD retinal disease. \\nResult & Discussion: \\nIn this undermentioned portion, the findings of the proposed mechanism along with the comparison with \\nsome cutting-edge studies will be comprised. The collected data were divided into sets for conducting \\ntraining and testing to constr uct and examine the proposed system. This approach was initially trained to \\nleverage 80% of the data and evaluated utilizing 20% of the collected information. To ensure enhanced \\nproductivity several sets of parameters were experimented. The parameter setting that provided us with the \\nmost advantageous outline for the proposed model is given below \\n \\n Batch size 32 \\n Epochs 100  \\n Learning rate 0.001  \\n Decay rate 0.95  \\nDecay step 1 \\n \\nFigure 5 and Figure 6 show both training and test sets of data, the accuracy and loss curves. The graphical \\nrepresentations of epoch versus accuracy and epoch versus losses are valuable insights for monitoring and \\nunderstanding the progress of the proposed method.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19'}, page_content=\"The graph exhibits how the model's accuracy varies while an increasing amount of training epochs raises. \\nThe predictability of the model on either a set of training data or a testing set appears on the axis in the \\nvertical direction, along with the number of epochs denoted throughout the horizontal direct ion. It is \\nobserved from Figure 5 reveals the overall number of epochs rises over training, the model's accuracy \\nelevates as it learns from the training data. At first, the accuracy continues to improve with each epoch, it \\nsuggests that the model can benef it from additional training. Later the accuracy eventually plateaus. This \\nindicates that the model has converged and further training may not significantly improve accuracy.  \\nThe epoch vs loss graph demonstrates the association between the process of train ing number of epochs \\nand the loss or error of the model. The loss of performance is a disparity among the estimated of model \\noutline and the intended outline. The loss level is portrayed through the y-axis, whereas the total amount of \\nepochs is displayed through the x-axis. In Figure 6, the loss is seen initially high as the model makes random \\npredictions. The training messages, the loss decreases, reflecting the model's improved performance and \\nability to make more accurate predictions. \\nFigure X a represents The AMDNet23 model's confusion matrix, resulting in was determined on the basis \\nof the ablation investigation, Adam, and learning rate, experiences the greatest level of accuracy and is \\nconfigured in a great potential manner.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20'}, page_content='The row of values reflect s what is actually labeled \\nattached to the images, while the column-specific values \\nreveal the quantities provided the predictive model \\nestimates. The diagonal values indicate correct \\npredictions (TP). However, the model had the great \\nresults for AMD. Acco rding to the confusion matrix, \\nTwo of the AMD pictorials was mistakenly classified as \\ncataract-related and diabetes, whereas 98 among the 100 \\nAMD investigations possessed effectively diagnosed.  \\nNext, 99 among a possible 100 anticipated involving \\ncataracts taught correctly where one as AMD. Of 100 \\ndiabetes images, Seven images had been mistakenly \\nidentified, involving 3 referred to as AMD along with 4 \\nas belonging to the healthy class, exposing of 93 \\ncorrectly assigned.  In closing least, among the 100 \\nnormal images, 96 were perfectly identified, and 1 had \\nbeen misinterpreted as images related to AMD and 3 associated to diabetes. \\n \\n \\n \\n \\n \\n \\n \\n \\nState-of-the-art work comparison \\nTable 3 provides a concisely summarizes the main approaches in the existing literature for diagnosing AMD \\ndisease and our proposed method. These approaches primarily involve conventional methods and deep \\nlearning algorithms, which utilize retinal images for diagnosis. \\n \\nAuthor Year Method No. of images Accuracy \\n \\nTK Yoo et. al. [51] \\n \\n2018 \\n \\nVGG19-RF \\n \\n3000 \\n3- Class \\n95% accuracy \\n \\nHuiying Liu et. al [52] \\n \\n2019 \\n \\nDeepAMD \\n \\n4725 \\n6- Class \\n70% accuracy \\nFelix Grassman et. al. [53]  \\n2020 \\nEnsemble \\nnetworks net \\n \\n3654  \\n13- class \\n63% accuracy'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21'}, page_content='N Chea and Y Nam [54] \\n \\n2021 \\nOptimal residual \\ndeep neural \\nnetworks  \\n \\n2335 \\n4- Class \\n85.79% accuracy \\n \\nC Domínguez et. al [55] \\n \\n2023 \\nTransformer-based \\nsystem \\n \\n4896 \\n3-Class \\n82.55% accuracy \\n \\nP Zang et. Al [56] \\n \\n2023 \\nDeep-Learning \\nbased aided \\nsystem \\n \\nNot specified \\n4-Class \\n80% accuracy \\n \\nProposed Method \\n \\n2023 \\n  \\nAMDNet23 \\n \\n2000 \\n4- Class \\n96.5% accuracy \\n \\nThe AMDNet23 model proposed in the study achieves a high accuracy rate of 96.5%, surpassing \\nother state-of-the-art works currently available. As a result, It can be presumed that this proposed \\nmethod is effective for early -stage detection and diagnosis of AMD, and this novel method also \\ndiagnoses Cataracts and diabetic retinopathy utilizing fundus ophthalmology datasets, \\ndemonstrating superior accuracy. \\nComparison of the AMDNet23 model with the transfer Learning models: \\nThe outline demonstrated and effectively potential of hybrid AMDNet23 network for precisely \\ndetecting AMD eye disease from images. The capacity of the model to precisely detect AMD \\ninstances is demonstrated by the excellent precision, accuracy, recall and F1 -score a cquired.  \\nCombination of CNNs and LSTMs allows for the extracting of both spatial and temporal features, \\ncapturing the subtle patterns and changes associated with AMD. Figure 5 exhibits the comparison \\nof performance between the model we proposed and a several pre-trained prepared.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22'}, page_content='Conclusion \\nIn essence, this study proposed a AMDNet23 model for detecting and diagnosing AMD disease using \\nseveral image datasets. The model achieved a high accuracy rate of 96.5%, surpassing other state -of-the-\\nart works in the field. Furthermore, when compared with pre -trained models, the novel deep AMDNet23 \\nmethod also showed superior accuracy for AMD detection, and the system is efficient to diagnose cataracts \\nand diabetic retinopathy respectively. In the future, incorpora ting additional modalities or features can \\npotentially enhance the performance of AMD detection models. Combining fundus images with other \\nclinical data, which could include patient demographics, health records, or genetic information, may \\nimprove accuracy and enable a more comprehensive awareness of the disease. In broadly , the findings of \\nthis research clearly demonstrates effectively of the proposed AMDNet23 model in accurately detecting \\nand diagnosing AMD cases. This model holds promise for early detect ion and diagnosis of AMD ocular \\ndisease, which could assist clinicians and aid in timely intervention and treatment for affected individuals. \\n \\n \\nREFERENCES \\n \\n1. Ackland, P., Resnikoff, S., & Bourne, R. (2017). World blindness and visual impairment: \\ndespite many successes, the problem is growing. Community eye health, 30(100), 71.  \\n2. “Vision Impairment and Blindness.” World Health Organization, 13 Oct. 2022, \\nwww.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment.  \\n3. “Vision Atlas.” The International Agency for the Prevention of Blindness, 4 Jan. 2023, \\nwww.iapb.org/learn/vision-atlas/.  \\n4. “Common Eye Disorders and Diseases.” Centers for Disease Control and Prevention, 19 \\nDec. 2022, www.cdc.gov/visionhealth/basics/ced/index.html.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23'}, page_content='5. Sarki, R., Ahmed, K., Wang, H., & Zhang, Y. (2020). Automatic detection of diabetic eye \\ndisease through deep learning using fundus images: a survey. IEEE access, 8, 151133-\\n151149.  \\n6. Kumar, S. M., & Gunasundari, R. (2023). Computational intelligence in eye disease \\ndiagnosis: a comparative study. Medical & Biological Engineering & Computing, 61(3), \\n593-615.  \\n7. Iqbal, Shahzaib, et al. “Recent Trends and Advances in Fundus Image Analysis: A \\nReview.” Computers in Biology and Medicine, vol. 151, 2022, p. 106277, \\nhttps://doi.org/10.1016/j.compbiomed.2022.106277.  \\n8. Tan, J. H., Bhandary, S. V., Sivaprasad, S., Hagiwara, Y., Bagchi, A., Raghavendra, \\nU., ... & Acharya, U. R. (2018). Age-related macular degeneration detection using deep \\nconvolutional neural network. Future Generation Computer Systems, 87, 127-135.  \\n9. Sogawa, T., Tabuchi, H., Nagasato, D., Masumoto, H., Ikuno, Y., Ohsugi, H., ... & \\nMitamura, Y. (2020). Accuracy of a deep convolutional neural network in the detection of \\nmyopic macular diseases using swept-source optical coherence tomography. Plos one, \\n15(4), e0227240.  \\n10. Serener, A., & Serte, S. (2019, April). Dry and wet age-related macular degeneration \\nclassification using oct images and deep learning. In 2019 Scientific meeting on \\nelectrical-electronics & biomedical engineering and computer science (EBBT) (pp. 1-4). \\nIEEE. \\n11. Yim, J., Chopra, R., Spitz, T., Winkens, J., Obika, A., Kelly, C., ... & De Fauw, J. (2020). \\nPredicting conversion to wet age-related macular degeneration using deep learning. \\nNature Medicine, 26(6), 892-899. \\n12. Schmidt-Erfurth, U., Waldstein, S. M., Klimscha, S., Sadeghipour, A., Hu, X., Gerendas, \\nB. S., ... & Bogunović, H. (2018). Prediction of individual disease conversion in early \\nAMD using artificial intelligence. Investigative ophthalmology & visual science, 59(8), \\n3199-3208.  \\n13. Schmidt-Erfurth, U., Bogunovic, H., Sadeghipour, A., Schlegl, T., Langs, G., Gerendas, \\nB. S., ... & Waldstein, S. M. (2018). Machine learning to analyze the prognostic value of \\ncurrent imaging biomarkers in neovascular age-related macular degeneration. \\nOphthalmology Retina, 2(1), 24-30.  \\n14. Bogunović, H., Montuoro, A., Baratsits, M., Karantonis, M. G., Waldstein, S. M., \\nSchlanitz, F., & Schmidt-Erfurth, U. (2017). Machine learning of the progression of \\nintermediate age-related macular degeneration based on OCT imaging. Investigative \\nophthalmology & visual science, 58(6), BIO141-BIO150.  \\n15. Jaiswal, A. K., Tiwari, P., Kumar, S., Al-Rakhami, M. S., Alrashoud, M., & Ghoneim, A. \\n(2021). Deep learning-based smart IoT health system for blindness detection using \\nretina images. IEEE Access, 9, 70606-70615.  \\n16. Wang, Ying, et al. “Deep Back Propagation–Long Short-Term Memory Network Based \\nUpper-Limb SEMG Signal Classification for Automated Rehabilitation.” Biocybernetics'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24'}, page_content='and Biomedical Engineering, vol. 40, no. 3, 2020, pp. 987–1001, \\nhttps://doi.org/10.1016/j.bbe.2020.05.003.  \\n17. Sahoo, M., Mitra, M., & Pal, S. (2023). Improved detection of dry age-related macular \\ndegeneration from optical coherence tomography images using adaptive window based \\nfeature extraction and weighted ensemble based classification approach. \\nPhotodiagnosis and Photodynamic Therapy, 42, 103629.  \\n18. Muthukannan, P. (2022). Optimized convolution neural network based multiple eye \\ndisease detection. Computers in Biology and Medicine, 146, 105648.  \\n19. Serener, A., & Serte, S. (2019, April). Dry and wet age-related macular degeneration \\nclassification using oct images and deep learning. In 2019 Scientific meeting on \\nelectrical-electronics & biomedical engineering and computer science (EBBT) (pp. 1-4). \\nIEEE.  \\n20. Kumar, Y. and Gupta, S., 2023. Deep transfer learning approaches to predict glaucoma, \\ncataract, choroidal neovascularization, diabetic macular edema, drusen and healthy \\neyes: an experimental review. Archives of Computational Methods in Engineering, 30(1), \\npp.521-541.  \\n21. Paradisa, Radifa Hilya, et al. \"Deep feature vectors concatenation for eye disease \\ndetection using fundus image.\" Electronics 11.1 (2022): 23.  \\n22. Faizal, Sahil, et al. \"Automated cataract disease detection on anterior segment eye \\nimages using adaptive thresholding and fine tuned inception-v3 model.\" Biomedical \\nSignal Processing and Control 82 (2023): 104550.  \\n23. Pahuja, Rahul, et al. \"A Dynamic Approach of Eye Disease Classification Using Deep \\nLearning and Machine Learning Model.\" Proceedings of Data Analytics and \\nManagement: ICDAM 2021, Volume 1. Springer Singapore, 2022.  \\n24. Chaudhary, R., & Kumar, A. (2022, June). Cataract Detection using Deep Learning \\nModel on Digital Camera Images. In 2022 IEEE International Conference on Cybernetics \\nand Computational Intelligence (CyberneticsCom) (pp. 489-493). IEEE  \\n25. Khan, Md Sajjad Mahmud, et al. \"Cataract detection using convolutional neural network \\nwith VGG-19 model.\" 2021 IEEE World AI IoT Congress (AIIoT). IEEE, 2021.  \\n26. Mondal, Sambit S., et al. \"EDLDR: An Ensemble Deep Learning Technique for Detection \\nand Classification of Diabetic Retinopathy.\" Diagnostics 13.1 (2023): 124.  \\n27. Babenko, Boris, et al. \"Detection of signs of disease in external photographs of the eyes \\nvia deep learning.\" Nature Biomedical Engineering (2022): 1-14  \\n28. Saranya, P., R. Pranati, and Sneha Shruti Patro. \"Detection and classification of red \\nlesions from retinal images for diabetic retinopathy detection using deep learning \\nmodels.\" Multimedia Tools and Applications (2023): 1-21.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25'}, page_content='29. Alyoubi, W. L., Abulkhair, M. F., & Shalash, W. M. (2021). Diabetic retinopathy fundus \\nimage classification and lesions localization system using deep learning. Sensors, \\n21(11), 3704.  \\n30. Acharya, U. R., Mookiah, M. R. K., Koh, J. E., Tan, J. H., Noronha, K., Bhandary, S. \\nV., ... & Laude, A. (2016).Novel risk index for the identification of age-related macular \\ndegeneration using radon transform and DWT features. Computers in biology and \\nmedicine, 73, 131-140.  \\n31. Zaki, W. M. D. W., Mutalib, H. A., Ramlan, L. A., Hussain, A., & Mustapha, A. (2022). \\nTowards a Connected Mobile Cataract Screening System: A Future Approach. Journal \\nof Imaging, 8(2).  \\n32. Larxel. “Ocular Disease Recognition.” Kaggle, 24 Sept. 2020, \\nkaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k. \\n33. Dugas, Emma, et al. “Diabetic Retinopathy Detection.” Kaggle, 2015, \\nkaggle.com/competitions/diabetic-retinopathy-detection. \\n34. Saeed, and Rimsha. “Fundus-Dataset.Zip.” Figshare, 11 Nov. 2021, \\ndoi.org/10.6084/m9.figshare.16986166.v1.  \\n35. Awsaf. “RFMID Train Dataset.” Kaggle, 26 Nov. 2020, \\nwww.kaggle.com/datasets/awsaf49/rfmid-train-dataset.  \\n36. “Eyecharity.Com Is for Sale.” HugeDomains.Com, www.eyecharity.com/aria_online. \\nAccessed 19 July 2023.  \\n37. Doddi, Guna Venkat. “Eye_diseases_classification.” Kaggle, 28 Aug. 2022, \\nwww.kaggle.com/datasets/gunavenkatdoddi/eye-diseases-classification.  \\n38. Watson, Debbie. Contouring: a guide to the analysis and display of spatial data. \\nElsevier, 2013.  \\n39. Rahman, Tawsifur, et al. \"Exploring the effect of image enhancement techniques on \\nCOVID-19 detection using chest X-ray images.\" Computers in biology and medicine 132 \\n(2021): 104319.  \\n40. Sahu, S., Singh, A. K., Ghrera, S. P., & Elhoseny, M. (2019). An approach for de-noising \\nand contrast enhancement of retinal fundus image using CLAHE. Optics & Laser \\nTechnology, 110, 87-98.  \\n41. Sara, Umme, Morium Akter, and Mohammad Shorif Uddin. \"Image quality assessment \\nthrough FSIM, SSIM, MSE and PSNR—a comparative study.\" Journal of Computer and \\nCommunications 7.3 (2019): 8-18.  \\n42. Saranya, P., R. Pranati, and Sneha Shruti Patro. \"Detection and classification of red \\nlesions from retinal images for diabetic retinopathy detection using deep learning \\nmodels.\" Multimedia Tools and Applications (2023): 1-21.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26'}, page_content='43. Team, Keras. “Keras Documentation: DenseNet.” Keras, \\nkeras.io/api/applications/densenet/. Accessed 10 June 2023.  \\n44. Dominic, Nicholas, et al. \"Transfer learning using inception-ResNet-v2 model to the \\naugmented neuroimages data for autism spectrum disorder classification.\" Commun. \\nMath. Biol. Neurosci. 2021 (2021): Article-ID.  \\n45. Tsiouris, Κostas Μ., et al. \"A long short-term memory deep learning network for the \\nprediction of epileptic seizures using EEG signals.\" Computers in biology and medicine \\n99 (2018): 24-37.  \\n46. Xu, M., Yoon, S., Fuentes, A., & Park, D. S. (2023). A comprehensive survey of image \\naugmentation techniques for deep learning. Pattern Recognition, 109347.  \\n47. Hasan AM, Jalab HA, Meziane F, Kahtan H, Al-Ahmad AS. Combining deep and \\nhandcrafted image features for MRI brain scan classification. IEEE Access 2019;7: \\n79959–67.  \\n48. Lundervold AS, Lundervold A. An overview of deep learning in medical imaging focusing \\non MRI. Z Med Phys 2019;29:102–27. https://doi.org/10.1016/j. Zemedi.2018.11.002.  \\n49. Hochreiter, S. (1998). The vanishing gradient problem during learning recurrent neural \\nnets and problem solutions. International Journal of Uncertainty, Fuzziness and \\nKnowledge-Based Systems, 6(02), 107-116.  \\n50. Chen, G. (2016). A gentle tutorial of recurrent neural network with error \\nbackpropagation. arXiv preprint arXiv:1610.02583.  \\n51. Yoo, T. K., Choi, J. Y., Seo, J. G., Ramasubramanian, B., Selvaperumal, S., & Kim, D. \\nW. (2019). The possibility of the combination of OCT and fundus images for improving \\nthe diagnostic accuracy of deep learning for age-related macular degeneration: a \\npreliminary experiment. Medical & biological engineering & computing, 57, 677-687.  \\n52. Liu, H., Wong, D. W., Fu, H., Xu, Y., & Liu, J. (2019). DeepAMD: detect early age-related \\nmacular degeneration by applying deep learning in a multiple instance learning \\nframework. In Computer Vision–ACCV 2018: 14th Asian Conference on Computer \\nVision, Perth, Australia, December 2–6, 2018, Revised Selected Papers, Part V 14 (pp. \\n625-640). Springer International Publishing.  \\n53. Grassmann, F., Mengelkamp, J., Brandl, C., Harsch, S., Zimmermann, M. E., Linkohr, \\nB., ... & Weber, B. H. (2018). A deep learning algorithm for prediction of age-related eye \\ndisease study severity scale for age-related macular degeneration from color fundus \\nphotography. Ophthalmology, 125(9), 1410-1420.  \\n54. Chea, N., & Nam, Y. (2021). Classification of Fundus Images Based on Deep Learning \\nfor Detecting Eye Diseases. Computers, Materials & Continua, 67(1).  \\n55. Domínguez, C., Heras, J., Mata, E., Pascual, V., Royo, D., & Zapata, M. Á. (2023). \\nBinary and multi-class automated detection of age-related macular degeneration using'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2023-08-30T13:14:26+06:00', 'author': 'Ksushbu', 'moddate': '2023-08-30T13:14:26+06:00', 'source': 'Portfolio/data/amdnet23.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27'}, page_content='convolutional-and transformer-based architectures. Computer Methods and Programs in \\nBiomedicine, 229, 107302.  \\n56. Zang, P., Hormel, T. T., Hwang, T. S., Bailey, S. T., Huang, D., & Jia, Y. (2023). Deep-\\nLearning–Aided Diagnosis of Diabetic Retinopathy, Age-Related Macular Degeneration, and \\nGlaucoma Based on Structural and Angiographic OCT. Ophthalmology Science, 3(1), 100245.  \\n57. Da Yan, Shengbin Wu, Mirza Tanzim Sami, Abdullateef Almudaifer, Zhe Jiang, Haiquan Chen, \\nD. Rangaprakash, Gopikrishna Deshpande, Yueen Ma, “Improving Brain Dysfunction Prediction \\nby GAN: A Functional-Connectivity Generator Approach”, IEEE International Conference on \\nBig Data (Big Data), Orlando, FL, USA, 2021. \\n58. Mirza Tanzim Sami, Da Yan, Huang Huang, Xinyu Liang, Guimu Guo, Zhe Jiang, “Drone-Based \\nTower Survey by Multi-Task Learning”, IEEE International Conference on Big Data (Big Data), \\nOrlando, FL, USA, 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-25T06:36:04+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-25T06:36:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Portfolio/data/PhD.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Shakhawat Hossain\\n♂phone+880 1778198423 /envel⌢peshakhawat15-14283@diu.edu.bd /linkedinshanin-hossain /githubShakhawatShanin\\nEducation\\nDaffodil International University Dhaka, Bangladesh\\nB.Sc. in Computer Science and Engineering CGPA: 3.60/4.00 (Jan. 2020 – Dec 2023)\\nRelevant Coursework:Data Structure, Data Mining and Machine Learning, Big Data and IoT, Artificial Intelligence,\\nNumerical Methods, Algorithms, Object Oriented Programming, Programming and Problem Solving\\nUndergraduate Major:Artificial Intelligence\\nUndergraduate Thesis: Graph-Based Automatic Breast Tumor Classification Through Ultrasound Imaging Using Ra-\\ndiomics Features.\\nResearch Interests\\n• Machine Learning\\n• Deep Learning\\n• Artificial Intelligence\\n• Computer Vision\\n• Health Informatics\\n• Medical Imaging\\n• Image Preprocessing\\n• Pattern Recognition\\n• Generative Artificial Intelligence\\nExperience\\nUniversity of Queensland Brisbane, Australia\\nResearch Assistant May 2024 – Present (Remote)\\n• At the AI and Digital Health Technology Lab, I developed a cutting-edge brain glioma grading system utilizing hybrid\\ngraph networks. This advanced model, trained on radiomic biomarkers extracted from 3D MRI scans, employs LIME to\\nensure accurate and interpretable grading, delivering clinically reliable outcomes.\\nHawkEyes Digital Monitoring Limited Dhaka, Bangladesh\\nAI Engineer Jan 2024 – Present\\n• Data Handling: Performed data validation and cleaning to ensure dataset reliability, and maintained consistent\\nlabeling quality across projects for developing robust AI.\\n• Computer Vision: I led computer vision projects focused on advanced image recognition, classification, segmentation,\\nand object detection. By leveraging algorithms like YOLO, UNet, custom CNN-LSTM, and OpenCV.\\n• Generative AI: Developed Lip Sync video model, AI-powered HDML employee information system, and interactive\\nchatbot designed for automated responses.\\n• OCR: Designed and implemented OCR pipelines to extract handwritten diverse text from scanned memo images.\\nProjects\\nBAT Bangladesh | Instance Segmentation, OOB Detection, Warp Perspective, Sequence Generation, Sorting\\n• Led the analysis of cigarette displays for regional campaigns using image processing, developing sequence analysis\\nalgorithms to ensure compliance with merchandising standards. This work improved the accuracy of audits and\\noptimized campaign management for BAT.\\nUnilever Bangladesh | Python, YoloV8, FastAPI, Asynchronous programming, Logging\\n• Implemented an AI-based trade merchandiser platform for Unilever Bangladesh with an accuracy rate of 98.00%.\\n• The system features a user-friendly dashboard for seamless management of trade merchandising activities, ensuring\\nefficient inventory control, meticulous task execution, and centralized digital recording, analysis, and reporting.\\nCardioCare | ML, FlaskAPI, HTML, Tailwind\\n• Developed a heart failure prediction model using machine learning, served via a Flask API with a responsive HTML and\\nTailwind CSS front-end.\\nChessCrack | OpenCV, UNet, YoloV8, Stockfish, NumPy, JS\\n• This project develops an intelligent system that automates the analysis of a physical chessboard image to predict the best\\nmove. It uses a YOLO-based model to detect and classify chess pieces, converts the board state into a FEN string, and\\nthen passes it to the Stockfish engine to determine the optimal move. The system is integrated into a web application via\\na Flask API, allowing users to upload a chessboard image and receive a move recommendation.\\nOfficeVision | FaceRecognition, Uvicorn, Llama, Langchain, Streamlit, Huggingface\\n• Implemented an advanced image recognition solution to accurately identify employees from images, enabling efficient and\\nautomated retrieval of employee details such as roles, contact information, and department.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-25T06:36:04+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-25T06:36:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Portfolio/data/PhD.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='Technical Skills\\nLanguages: Python, C++, HTML/CSS, MySQL\\nData Analysis and Visualization : NumPy, Pandas, Matplotlib, Seaborn\\nAssociated Frameworks: TensorFlow, PyTorch, Scikit-learn, Keras, OpenCV, Transfer Learning, Hugging Face\\nTools: Git, GitHub, Jupyter Notebook, Visual Studio Code, Latex, Colab, Roboflow\\nDevelopment Tools: FastAPI, FlaskAPI, RestAPI, MLOps, LLM, NLP\\nPublications\\n1. Md. Aiyub Ali, Md. Shakhawat Hossain , Md.Kawsar Hossain, Subhadra Soumi Sikder, Sharun Akter\\nKhushbu, Mirajul Islam, ”AMDNet23: Hybrid CNN-LSTM Deep Learning Approach with Enhanced\\nPreprocessing for Age-Related Macular Degeneration (AMD) Detection”, Intelligent Systems with\\nApplications journal, Elsevier. https://doi.org/10.1016/j.iswa.2024.200334\\n2. Md. Aiyub Ali, Md Shakhawat Hossain, Taslima Ferdaus Shuva, Muhammad Ali Abdullah Almoyad,\\nNabil Anan Orka, Md. Tanvir Rahman, Risala Tasin Khan, M. Shamim Kaiser, and Mohammad Ali Moni.\\n“RGNN3D: A Hybrid Radiomic Graph Neural Network for 3D MRI Glioma Grading” IEEE Transactions on\\nBiomedical Engineering. [In Review]\\nReferences\\nDr. Mohammad Ali Moni, PhD (Cambridge)\\nProfessor,\\nHead of the Group, AI and Digital Health Technology\\nFaculty of Health and Behavioural Science,\\nThe University of Queensland, St Lucia, QLD 4072, Australia\\nEmail: m.moni@uq.edu.au'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-25T06:36:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-25T06:36:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Portfolio/data/Job.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='SHAKHAWAT HOSSAIN\\nAI Engineer\\n/ne+8801778198423\\n shanin-hossain\\n/gtbShakhawatShanin\\n shaninhossain2@gmail.com\\nSUMMARY\\nI am an AI Engineer at HawkEyes Digital Monitoring Lim-\\nited, specializing in optimizing Computer Vision, NLP,\\nOCR, and AI models. I ensure top-notch quality and\\nseamlessly deploy solutions on cloud platforms and ap-\\nplications. Passionate about leveraging cutting-edge\\ntechnology to solve real-world problems, I actively learn\\nnew technologies and coding practices to push the\\nboundaries of AI innovation. I have successfully solved\\nover 300+ programming challenges on platforms such\\nas Codeforces, URI, and DIU Bluesheet.\\nSKILLS\\nLanguages: C, C++, Python, Java, MySQL\\nData Analysis: NumPy, Pandas, Matplotlib, Seaborn\\nFrameworks: TensorFlow, PyTorch, Scikit-learn, Keras,\\nOpenCV, Hugging Face\\nTools: Git, Jupyter, VS Code, LaTeX, Roboflow,\\nColab, MS PowerPoint, Word\\nWeb Tools: FastAPI, Flask, RestAPI, ReactJS, Tail-\\nwind\\nEDUCATION\\n01/2020 – 12/2023 B.Sc. in Computer Science and Engineering Daffodil International University (DIU)\\nCGPA: 3.60/4.00\\nMajor: Artificial Intelligence\\nThesis: Graph-Based Breast Tumor Classification Through Ultrasound Imaging Using Radiomics Features.\\nEXPERIENCE\\n02/2024 – Present AI Engineer HawkEyes Digital Monitoring Limited\\nPrepared datasets, trained and fine-tuned models, and optimized accuracy for real-world AI applications.\\n05/2024 – Present Research Assistant (Remote) University of Queensland\\nDeveloped a hybrid graph-based brain glioma grading system using 3D MRI datasets.\\n01/2024 – 02/2024 Junior Front-End Developer M4yours Dev\\nDesigned a responsive news portal template for publishers and bloggers.\\n01/2023 – 12/2023 Research Lab Member HIRL Lab (DIU)\\nWorked on medical image disease detection, sentiment analysis, and integrated computer vision with\\nadvanced ML algorithms.\\nPROJECTS\\nBAT Bangladesh YOLO Segmentation, Detection, Warp Perspective, Sequence Generation, LLAMA2, Langchain\\n• BAT AI-Based Cigarette Brand Detection and Competitor Analysis System:\\nBuilt an object detection system using YOLO for cigarette brand identification, sequence validation,\\nand competitor analysis.\\n• Chatbot Development:\\nDeveloped an NLP chatbot for dynamic conversations across multiple platforms, handling queries\\nand providing real-time customer support with personalized responses.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-25T06:36:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-25T06:36:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Portfolio/data/Job.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='Unilever\\nBangladesh\\nPython, NL TK, FastAPI, Pytesseract, Asynchronous Programming\\n• OCR Based Billing System for Laver Bazar:\\nDeveloped an OCR application to capture invoice images and extract item names, quantities, and\\nprices. Automated data digitization and storage, enhancing inventory and financial management by\\nreducing errors.\\n• Voice Recognition System for Word Detection and Counting:\\nDeveloped a voice recognition application that processes audio input and identifies the frequency\\nof specific words or phrases.\\nGazipur Police ML, MTCNN, Keras FaceNet, OpenCV, NumPy\\n• Face Recognition System for Gazipur Metropolitan Police (GMP):\\nDesigned a real-time facial recognition system that enables secure access control, user verification,\\nand efficient recognition history management.\\nACHIVEMENT\\n2025 Best Performing Team (AI Team) HawkEyes Digital Monitoring Limited\\nPUBLICATIONS\\n• Md. Aiyub Ali, Md. Shakhawat Hossain , Md.Kawsar Hossain, Subhadra Soumi Sikder, Sharun Akter Khushbu, Mirajul\\nIslam, \"AMDNet23: Hybrid CNN-LSTM Deep Learning Approach with Enhanced Preprocessing for Age-Related Mac-\\nular Degeneration (AMD) Detection\", Intelligent Systems with Applications journal, Elsevier.\\nhttps:/ /doi.org/10.1016/j.iswa.2024.200334\\n• Md. Aiyub Ali, Md Shakhawat Hossain , Taslima Ferdaus Shuva, Muhammad Ali Abdullah Almoyad, Nabil Anan Orka,\\nMd. Tanvir Rahman, Risala Tasin Khan, M. Shamim Kaiser, and Mohammad Ali Moni. “RGNN3D: A Hybrid Radiomic\\nGraph Neural Network for 3D MRI Glioma Grading” Knowledge-Based Systems. [In Review]\\nREFERENCES\\n• Dr. Mohammad Ali Moni, PhD (Cambridge): Professor and Head, AI and Digital Health Technology, University of\\nQueensland, Australia\\nEmail: m.moni@uq.edu.au'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='RGNN3D: A Hybrid Radiomic Graph Neural Network for 3D\\nMRI Glioma Grading\\nMd. Aiyub Ali a,b, Md Shakhawat Hossaina,b, Taslima Ferdaus Shuvab, Muhammad Ali Abdullah\\nAlmoyadc, Nabil Anan Orkaa, Risala Tasin Khane, M. Shamim Kaiser e, Md. Tanvir Rahman a,d,∗,\\nMohammad Ali Monia,b,f,g,h,∗∗\\naSchool of Health and Rehabilitation Sciences, The University of Queensland, St\\nLucia, Brisbane, 4072, Queensland, Australia\\nbDepartment of Computer Science and Engineering, Daffodil International University, Daffodil Smart\\nCity, Savar, 1341, Dhaka, Bangladesh\\ncDepartment of Basic Medical Sciences, College of Applied Medical Sciences, King Khalid\\nUniversity, Guraiger, 62521, Abha, Saudi Arabia\\ndDepartment of Information and Communication Technology, Mawlana Bhashani Science and Technology\\nUniversity, Santosh, Tangail, 1902, Dhaka, Bangladesh\\neInstitute of Information Technology, Jahangirnagar University, Savar, 1342, Dhaka, Bangladesh\\nfSchool of Information Technology, Washington University of Science and Technology, Alexandria, 22314, Virginia, USA\\ngAI and Cyber Futures Institute, Charles Sturt University, Bathurst, 2795, New South Wales, Australia\\nhRural Health Research Institute, Charles Sturt University, Orange, 2800, New South Wales, Australia\\nAbstract\\nThe diagnosis of glioma, a complex and often deadly brain tumor, involves extensive medical examinations.\\nStill, accurately grading and classifying gliomas is difficult, as different areas within the same tumor can\\nexhibit varying characteristics. The integration of radiomics, a clinically relevant feature extraction method,\\nwith machine learning (ML) is becoming increasingly popular in addressing this issue, but several research\\ngaps persist. To this end, this study proposes a novel deep neural network, RGNN3D, that combines Graph\\nNeural Networks with LSTM layers to precisely grade gliomas in 3D magnetic resonance imaging (MRI)\\ndata. To train our proposed model, we meticulously extracted 112 radiomic biomarkers. Utilizing the\\nbiomarkers, RGNN3D constructs a graph, channels essential information within layers, and preserves only\\npertinent information through its integrated memory cells. The proposed framework attained an accuracy\\nof 98.58%, aligning with the performance of previous state-of-the-art architectures and surpassing prior\\nradiomic-based ML models. We further employed an explainable AI approach (LIME) to highlight the most\\nsignificant features, assisting radiologists in making more informed decisions. In short, RGNN3D offers a\\nreliable and robust computer-aided solution for potential clinical application in the automated identification\\nof gliomas.\\nKeywords: Radiomics, Biomarkers, Graph Neural Networks, Glioma Grading.\\n1. Introduction1\\nGliomas, which originate from glial cells, are the most prevalent primary intracranial tumors in adults2\\n[1]. Gliomas account for approximately 74.6% of all malignant brain tumors [2]. Patients with low-grade3\\n∗Corresponding author\\n∗∗Corresponding author\\nEmail addresses: aiyubali15-13456@diu.edu.bd (Md. Aiyub Ali), shakhawat15-14283@diu.edu.bd (Md Shakhawat\\nHossain), shuva.cse@diu.edu.bd (Taslima Ferdaus Shuva), maabdulllah@kku.edu.sa (Muhammad Ali Abdullah Almoyad),\\nn.orka@uq.edu.au (Nabil Anan Orka), risala@juniv.edu (Risala Tasin Khan), mskaiser@juniv.edu (M. Shamim Kaiser),\\ntanvirrahman@mbstu.ac.bd (Md. Tanvir Rahman ), m.moni@uq.edu.au (Mohammad Ali Moni )\\nPreprint submitted to Knowledge-Based Systems September 25, 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='gliomas (LGG, grades I and II) generally have a survival rate averaging seven years [3]. In stark contrast,4\\nonly 3–5% of patients with glioblastoma (GBM, grade IV) survive beyond five years, with a median survival5\\ntime of approximately 12 months [4]. Intraoperative grading of gliomas, especially distinguishing between6\\nGBM and LGG, is crucial for making informed diagnostic decisions in clinical practice. To this end, magnetic7\\nresonance imaging (MRI) is an essential technique for screening, treatment planning, and assessing tumor8\\nresponse to therapy because it assists in analyzing the phenotypic and structural differences of gliomas9\\n[5]. In the same vein, radiomic features have shown a lot of promise for distinguishing between different10\\ngrades of glioma. Radiomics is an emerging field that extracts high-throughput quantitative features from11\\nmedical images, particularly MRI [6]. Radiomic analysis involves separating the tumor area from the rest of12\\nthe image and extracting clinically useful information about its shape, appearance, size, intensity, location,13\\nand texture [7]. For example, high heterogeneity in intensity and texture features can sometimes indicate14\\nGBM [8, 9]. Although the combination of MRI and radiomic analysis offers vital clinical information15\\nregarding gliomas, identifying patterns among the hundreds of variables and understanding how each one16\\naffects glioma grading is challenging. To this end, machine learning (ML) algorithms emerge as a viable17\\nsolution. For instance, earlier studies used various ML classifiers such as logistic regression (LR) [10],18\\nsupport vector machines (SVM) [11, 12], random forest (RF) [2, 13, 14], LASSO [15], and multi-layer19\\nperceptron [12, 16]. These models performed comparably to state-of-the-art convolutional neural networks,20\\nshowing the promise of radiomic-based automated glioma grading. Han Li et al. [17] introduced a transfer21\\nlearning-based optimizer (MSAS-DMOA) to improve adaptability in dynamic tasks. Peishu Wu et al. [18]22\\nproposed GLA-TD, a CNN-transformer model using attention and tensor decomposition for efficient medical23\\nimage analysis. These works align with our RGNN3D approach in enhancing learning and interpretability.24\\nHowever, notable research gaps still persist. First of all, despite the availability of numerous types of radiomic25\\nfeatures, prior research has focused on distinct radiomic feature types. For example, some studies used only26\\ntexture-based features [19, 20, 21], while others relied on wavelet-based features [22, 11]. To the best of27\\nour knowledge, no studies holistically explored radiomics, i.e., modeling multiple classes of radiomic features28\\nsimultaneously. Second, existing studies lack interpretability. While often accurate, black-box models do not29\\nfoster trust between clinicians and these models because the variables critical to the decision-making process30\\nremain unclear. Finally, no framework has yet outperformed current state-of-the-art classifiers regardless of31\\nearlier studies’ comparable efficacy. Given the aforementioned research gaps, we propose a novel framework,32\\nRGNN3D, which comprises graph convolutional networks (GCNs) and long short-term memory (LSTM)33\\ncells to form a hybrid classifier. The advantages of the proposed hybrid model are twofold. The primary34\\nbenefit of GCNs, or graph neural networks in general, is the ability to embed complex relational data into35\\na graph and pass only the relevant information to the subsequent layers [23]. In addition, LSTM layers use36\\nmemory cells to retain pertinent input data and avoid the vanishing gradient problem [24]. We train our37\\nproposed model on 112 clinically significant radiomic biomarkers extracted from 3D MRI scans and later38\\nemploy local interpretable model-agnostic explanations (LIME) [25]. LIME quantitatively explains which39\\nradiomic features have the most significant impact on grading LGG and GBM, ensuring utmost reliability.40\\nThe key contributions of this study are summarized as follows:41\\n• Introduced RGNN3D, a novel hybrid method that combines LSTMs and GCNs, leading to highly42\\naccurate glioma grading from complex, multimodal 3D MRI scans.43\\n• Demonstrated that GCNs captured 112 significant radiomic biomarkers, while the LSTMs serve long-44\\nrange dependencies and alleviate gradient vanishing by gated memory mechanisms.45\\n• To enhance model transparency, we incorporate LIME-based interpretability analysis, which identifies46\\nand ranks the most influential features contributing to the grading outcomes.47\\n• Identified the top 10 radiomic biomarkers that play a significant role in terms of glioma grading, which48\\nenriches the clinical trust.49\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='3 Features Integration 2 Radiomic Based Feature Extraction\\nWavelet \\nFilter\\nLoG \\nFilter\\nLBP \\nFilter\\nImage \\nIntensity\\nShape-based \\n(n=26)\\nGLDM \\n(n=14)\\nNGTDM \\n(n=5)\\nGLSZM \\n(n=16)\\nGLRLM \\n(n=16)\\nGLCM \\n(n=24)\\nShape \\nbased \\nfeatures\\nHistogram \\ndescriptors \\nfeatures\\nTexture \\nfeatures\\nTotal \\nExtracted \\nFeatures: \\n112\\nGenerated \\nnodes \\nof \\n704 \\nx \\n112 \\nEdge \\ntable \\n(Adj. \\nmatrix) \\nof\\n \\n64 \\nx \\n64 \\nProposed \\nRGNN3D\\n \\nmodel\\n4\\nHybrid \\nModel \\nDevelopment\\nCombines \\nGraph \\nand \\nSequential \\nLearning\\nEnd-to-End \\nTrainable\\nFlexible \\nto \\nDifferent \\nGraph \\nStructures\\nEvaluation \\n& \\nAnalysis\\n5\\n 1 3D MRI & ROI\\nT1\\nT2\\nT1CE\\nFLAIR\\nFirst \\norder \\nstatistics \\n(n=19)\\nRadiomic \\nBiomarker \\nTables\\nT1\\nT2\\nT1CE\\nFLAIR\\nComparison \\nwith \\nstate-of-the-art \\nmethods\\n     \\nExperimented \\non \\nfour \\nindividual \\nstages\\nT1, \\nT1CE, \\nT2, \\nFLAIR\\nAccuracy, \\nSpecificity, \\nSensitivity,\\nAUC,F1-Score\\nConfusion \\nmatrix\\nROC \\ncurve\\nModel \\ninterpretation \\nanalysis\\nFigure 1: Comprehensive workflow of the proposed radiomic-based hybrid RGNN3D Model for glioma grading.\\n2. Materials and Methods50\\nThe overall methodology adopted in this study has five key stages as shown in Fig. 1: (i) 3D MRI &51\\nROI involves acquiring 3D MRI scans and identifying regions of interest (ROI) for GBM and LGG, (ii)52\\nRadiomic Based Feature Extraction employs the PyRadiomics toolbox to extract features from the ROIs53\\nusing various filters, (iii) Features Integration incorporates shape-based features, histogram descriptors,54\\nand texture features to create a comprehensive feature set, (iv) Hybrid Model Development proposes the55\\nRGNN3D model that combines graph neural networks and LSTM-based sequential learning, ensuring end-56\\nto-end trainability and interpretability, and (v) Evaluation & Analysis assesses the performance of the57\\nproposed framework with interpretability. In the following subsections, we delve into the specifics of every58\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='a b\\nGBM\\nLGG\\nFLAIR\\nT1\\nT1CE\\nT2\\nROI\\nBraTS \\n2019\\nBraTS \\n2020\\nCombined \\nBraTS\\nGBM\\nLGG\\nTOTAL\\n552\\n152\\n704\\n259\\n76\\n335\\n76\\n293\\n369\\nFigure 2: Dataset used in this study: (a) Sample images from each category; (b) Integration of BRATS 2019 and BRATS 2020\\nstep.59\\n2.1. Dataset Description60\\nThis study used the Brain Tumor Segmentation (BraTS) datasets, where multimodal 3D MRI scans of61\\nGBM and LGG are available. The BraTS’2019 [26] contains data of 369 subjects with GBM (n = 293)62\\nand LGG (n = 76). Besides, BraTS’2020 [27] comprises 335 subjects with GBM (n = 259) and LGG63\\n(76). We merged these two datasets, and the compilation incorporated a total of 704 subjects: GBM (n64\\n= 552) and LGG (n = 152), as shown in Fig. 2 (b). The merged dataset includes neuroimaging files with65\\nmultiparametric 3D MRI scans, encompassing (a) T2 Fluid Attenuated Inversion Recovery (T2-FLAIR):66\\nsuppresses cerebrospinal fluid signals, enhancing lesion visibility, (b) Native (T1): captures the brain’s67\\nbaseline anatomy, highlighting tissue density differences, (c) Post-contrast T1-weighted (T1CE): acquires68\\nT1-weighted images with a contrast agent, highlighting areas of increased vascularization and blood-brain69\\nbarrier disruption to identify abnormal tissue, such as glioma lesions, (d) T2-weighted (T2): provides detailed70\\ninformation on edema, cysts, and tissue abnormalities, aiding in understanding glioma characteristics, and71\\n(e) ROI: allows feature extractions specifically from the tumor region or areas of interest that are relevant,72\\nprecise, and clinically meaningful. Sample MRI scans from each category are also visualized in Fig. 2 (a).73\\n2.2. Feature Extraction and Integration74\\nRadiomics, a transformative approach in medical imaging, involves the extraction and analysis of clini-75\\ncally significant information embedded within medical images, transcending what is perceptible to the human76\\neye [28]. To address this issue, we used the PyRadiomics [29] tool to identify relevant radiomic features in77\\nour study. Here, for each subject, we get 112 radiomic features corresponding to seven distinct types as78\\ndescribed in Table 1: First Order, Shape-Based, Gray Level Co-occurrence Matrix (GLCM), Gray Level79\\nRun Length Matrix (GLRLM), Gray Level Size Zone Matrix (GLSZM), Neighbouring Gray Tone Difference80\\nMatrix (NGTDM), and Gray Level Dependence Matrix (GLDM). The extracted features encompass a range81\\nof statistical metrics, shape-based attributes, and matrix-based analyses, offering insights into voxel intensity82\\ndistribution, geometric properties, spatial relationships, and textural patterns within ROI. However, each83\\ntype of these radiomic features contributes to unique and valuable information, and their integration ensures84\\na holistic understanding of the clinically significant attributes. To achieve this, for each MRI stage (T1, T2,85\\nT1CE, FLAIR), we organize these extracted features into tables (Radiomic Biomarker Table: 704 x 112)86\\nwhere each row corresponds to a specific subject, and each column represents different radiomic features.87\\n2.3. RGNN3D88\\nWe introduce a Radiomic Graph Neural Network (RGNN3D) architecture that integrates LSTM cells89\\nwithin its message-passing framework to enhance the learning of node embeddings as depicted in Fig. 3.90\\nThis approach allows the model to capture both short-term and long-term dependencies inherent in graph91\\nstructures. The design of the RGNN3D model consists of several layers, including an LSTM layer, GCN92\\nlayers with appropriate message passing among each block, and a dense output layer. The input layer receives93\\n112 features and constructs a graph. Here, the node feature vector and the graph’s adjacency matrix are94\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='Table 1: Types of Radiomic Features Extracted in Our Study.\\nFeature Type Description\\nFirst Order Captures fundamental statistical metrics, offering voxel intensity distribu-\\ntion in ROIs.\\nShape-based Encompasses shape-related attributes in 3D and 2D, which understand the\\nbrain’s geometric properties.\\nGLCM Reveals voxel intensity spatial relationship patterns.\\nGLRLM Quantifies consecutive voxel lengths with identical intensity values.\\nGLSZM Offers size and spatial distribution of homogeneous intensity regions.\\nNGTDM Highlights tone differences between neighboring voxels and textural.\\nGLDM Analyzes voxel pair dependence.\\n.\\n.\\n.\\nF3\\nF2\\nF1\\nF8\\nF7\\nF6\\nF3\\nF8\\nF7\\n.\\n.\\n.\\nGCNConv3\\nf1\\nf2\\nf3\\n.\\n.\\n.\\nf91\\nf1\\nf2\\nf3\\n.\\n.\\n.\\nf91\\nf1\\nf2\\nf3\\n.\\n.\\n.\\nf112\\nL\\nG\\nL\\nL\\nGraph \\nConstruction\\nLGG\\nGBM\\nG\\nG\\nG\\nG\\nAdjacency \\nMatrix\\nNode \\nFeatures\\n Graph Convolutional with LSTM Network\\nOutput\\nFilter \\n2\\n[64, \\n2]\\nL\\nG\\nG\\n.\\n.\\n.\\nL\\nFilter \\n32 \\n[64,64]\\nLSTM \\n64\\n[64,64]\\nFilter \\n32 \\n[64,32]\\nFilter \\n16 \\n[64,16]\\nAggregate\\nUpdate\\nMessage \\nPassing\\nLSTM \\n64\\nAggregate\\nUpdate\\nMessage \\nPassing\\nAggregate\\nUpdate\\nMessage \\nPassing\\nDense \\nLayer\\nSoftmax\\nNeighbor \\nInformation \\nFlow\\nFigure 3: Proposed RGNN3D model architecture.\\ninitially fed into the Feed Forward Network (FFN) [30] blocks, followed by passing these to the GCN layers95\\nafter processing. This architecture includes three GCN layers, each employing the ReLU activation function96\\nwithin a message-passing framework. The embeddings updated by the LSTM cell and GCN layers are97\\nthen channeled back into the FFN blocks, producing the final node embeddings as logit values. Finally, a98\\nSoftmax activation function is applied to generate a probability distribution for the potential node labels.99\\nThe inclusion of LSTM enables gradient flow across time steps due to its gating mechanisms (input, forget,100\\nand output gates), which retain essential information and reduce vanishing gradient effects. This makes it101\\nsuitable for preserving long-range dependencies and refining spatial-temporal feature representations learned102\\nfrom the graph structure.103\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='2.4. Graph Representation and Initial Node Embeddings104\\nWe generate a graph with 704 rows and 112 columns using the radiomic biomarker table, representing105\\nimage features and the target class. Each patient is considered a single node in the graph and the corre-106\\nsponding radiomic feature vector serves as its initial embedding. In this regard, G = (V, E) is a graph, where107\\nV is the set of patient nodes and E represents the set of edges. The node feature matrix X ∈ R704×112, that108\\nencodes the radiomic features for all patients.109\\nTo avoid an artificial dependency from different patients, we employ an identity adjacency matrix A =110\\nI704. Because we decided to connect by self-loops. In this matter, each node maintains independence during111\\nmessage passing, where there is no possibillity of information leakage from other patients. This clinical112\\npractice especially relevant in the healthcare domain, where patient to patient connections are not always113\\nmeaningful. We ensure a simple graph structure but clinically efficient. This approach confirms that node114\\nembeddings are rely on radiomic features of each patient’s record.115\\nTheoretically, initial representation of a node v ∈ V is given by:116\\nh(0)\\nv = xv ∈ Rd,\\nwhere d = 112 is the dimensionality of the feature vector. After that, these embeddings pass through a series117\\nof graph convolution and recurrent (LSTM gates) operations. That enables the model to capture higher118\\nlevel abstractions from radiomic features.119\\nFor better computational efficiency and scalability, we generate a mini batch (each batch is 64 training120\\nsizes). In this regard, each batch feeds to the model during iteration. For a batch size B, we construct the121\\ncorresponding subgraph with a feature matrix Xb ∈ RB×112 and adjacency matrix Ab = IB. This approach122\\nensures the message passing within a batch. It is also prevents corss batch dependencies during training.123\\nWhile we use an identity adjacency matrix in this study, the framework is flexible and robust. That can124\\nextend to more complex graphs such as kNN based graphs or fully connected graphs but they did not offer125\\nreliable performance. By using a simple identity graph with LSTM cells, the approach is truly robust to the126\\ndataset. It is also adaptable to broader clinical applications. Overall, it supports the clinical generalisability.127\\n2.4.1. Graph Convolutions with LSTM Network128\\nOne of the core innovations of the RGNN3D model lies in its use of LSTM cells for aggregating and129\\nupdating node embeddings inside GCN blocks. The underlying message-passing process involves aggregat-130\\ning node features from its neighbors and updating the node’s embedding using LSTM cells, followed by131\\nnormalization and the final nonlinear embeddings. These steps are described as follows:132\\n(a) Aggregation of Neighboring Node Features:For a node u at iteration i, the aggregated message m(i)\\nu133\\nfrom its neighbors N(u) is computed as:134\\nm(i)\\nu =\\nX\\nv∈N(u)\\nW(i)\\nNeih(i−1)\\nv (1)\\nwhere W(i)\\nNei is a learnable weight matrix.135\\n(b) LSTM-based Node Updating:The updated embedding h(i)\\nu for node u is obtained using an LSTM cell136\\n[31] that takes the node’s previous embedding h(i−1)\\nu and the aggregated message m(i)\\nu as inputs:137\\nh(i)\\nu = LSTM(h(i−1)\\nu , m(i)\\nu ) (2)\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='The internal operations of the LSTM cell are defined by the following equations:138\\ni(i)\\nu = σ(W(i)\\ni h(i−1)\\nu + U(i)\\ni m(i)\\nu + b(i)\\ni ) (3)\\nf(i)\\nu = σ(W(i)\\nf h(i−1)\\nu + U(i)\\nf m(i)\\nu + b(i)\\nf ) (4)\\no(i)\\nu = σ(W(i)\\no h(i−1)\\nu + U(i)\\no m(i)\\nu + b(i)\\no ) (5)\\nc(i)\\nu = f(i)\\nu ⊙ c(i−1)\\nu + i(i)\\nu ⊙ tanh(W(i)\\nc h(i−1)\\nu +\\nU(i)\\nc m(i)\\nu + b(i)\\nc )\\n(6)\\nh(i)\\nu = o(i)\\nu ⊙ tanh(c(i)\\nu ) (7)\\nHere, i(i)\\nu , f(i)\\nu , o(i)\\nu are the input, forget, and output gates, respectively, and c(i)\\nu is the cell state. The139\\nparameters W(i)\\ni , U(i)\\ni , b(i)\\ni , etc., are the learnable weights and biases of the LSTM cell at iteration i.140\\n(c) Normalization of Node Embeddings:After several iterations of message passing, the node embeddings141\\nare normalized to improve their representational power:142\\nhnorm\\nu = h(i)\\nu\\n∥h(i)\\nu ∥\\n(8)\\nwhere ∥h(i)\\nu ∥ denotes the Euclidean norm of the vector h(i)\\nu .143\\n(d) Final Node Embeddings via GCN Layer:The normalized embeddings are then passed through a Graph144\\nConvolutional Network (GCN) [32] layer to obtain the final nonlinear node embeddings:145\\nH(i+1) = σ(AH(i)WGCN) (9)\\nwhere A is the adjacency matrix, where ones are on the diagonal and zeros elsewhere. This approach146\\nensures that each node is only connected to itself, focusing on the node-specific features without147\\nconsidering inter-node dependencies. Here, H(i) is the matrix of node embeddings at iteration i,148\\nWGCN is the weight matrix of the GCN layer, and σ is a nonlinear activation function.149\\n2.4.2. Output Layer and Grading150\\nThe final embeddings H(i+1) from the GCN layer are passed through a dense output layer to predict the151\\nnode labels. The dense layer applies a linear transformation followed by a nonlinear activation function of152\\nSoftmax to generate a probability distribution for each node’s label. The output for a node u is computed153\\nas follows:154\\nyu = σ(Wouthnorm\\nu + bout) (10)\\nHere, Wout is the weight matrix of the output layer, bout is the bias term, and σ denotes the Softmax155\\nactivation function, which maps the output to each probability value representing the grading of glioma.156\\n2.5. Implementation157\\nThe implementation of the RGNN3D model commenced with the combined BraTS dataset comprising158\\n704 records, which was further divided into training and testing sections in an 80:20 ratio. The model was159\\noptimized using the Adam optimizer, with a batch size of 64, and trained over 50 epochs. The Glorot160\\nUniform initializer was employed for the kernel initialization. Sparse Categorical Cross-Entropy was utilized161\\nfor the loss function. The implementation was executed on a system equipped with an RTX 3060 GPU and162\\n32GB of RAM, running the NVIDIA driver version 535.104.05 with CUDA version 12.2.163\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='3. Results and Discussion164\\nFirst, we start with the performance analysis across all four 3D MRI stages (T1, T1CE, T2, and FLAIR)165\\nusing standard classification performance metrics [33] i.e., precision, recall, F1-score, and accuracy. From166\\nthe confusion matrices as shown in Fig. 4 we observe: (i) for classifying LGG, the proposed model acquired167\\nidentical performance across all stages with 28 accurate cases and only two misclassifications, (ii) for GBM,168\\nwe get no false negative result for both T1 and T2 stages. However, there were only 2 and 6 misclassifications169\\nout of 141 cases with T1CE and FLAIR stages, respectively. from the confusion matrices, it is clear that GBM170\\nis more consistently detected compared to LGG, which demonstrated strong power for aggressive gliomas.\\n105\\n6\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nFLAIR\\n0\\n100\\n75\\n50\\n25\\nPREDICTED\\nACTUAL\\n111\\n0\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nT2\\n0\\n100\\n75\\n50\\n25\\nPREDICTED\\nACTUAL\\n109\\n2\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nT1CE\\n0\\n100\\n75\\n50\\n25\\nPREDICTED\\nACTUAL\\n111\\n0\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nT1\\n0\\n100\\n75\\n50\\n25\\nPREDICTED\\nACTUAL\\nFigure 4: Confusion Matrices of RGNN3D Model for MRI Stages T1, T2, T1CE, and FLAIR\\n171\\nUsing these matrices, we calculate different scores and discover a similar trend for each evaluation criterion172\\nas depicted in Fig. 5. The proposed RGNN3D system achieved outstanding accuracy scores of 98.58% for173\\nT1 and T2, 97.16% for T1CE, and 94.33% for FLAIR. The F1-scores also reinforce the system’s robustness,174\\nmaintaining values of 99.10% for T1 and T2, 98.20% for T1CE, and 96.33% for FLAIR. Furthermore, the175\\nprecision values reached 100% for F1 and F2, which are able to completely avoid false-positive decisions for176\\nglioma grading. Even for T1CE and FLAIR modalities, precision scores remain high at 98.20% and 94.59%177\\nrespectively. Moreover, our system shows strong recall: 98.23% (T1), 98.23% (T2), 98.20% (T1CE), and178\\n98.13% (FLAIR). Overall, we notice the best and identical performance of the system with both T1 and T2179\\n(accuracy: 98.58%, precision: 100%, f1-score: 99.10%, and recall: 98.23%) for each category.180\\nBesides, the accuracy and loss curves (outlined in Fig. 6) illustrate the performance of the proposed181\\nmodel over 50 epochs. Here, we notice that the training accuracy reaches 100.00%, and we get the validation182\\naccuracy to be 98.58%. We noted that the training loss decreases smoothly for 4 different MRI stages, but183\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='ACCURACY\\nF1-SCORE\\nPRECISION\\nRECALL\\n98.58%\\n97.16%\\n98.58%\\n94.33%\\n99.10%\\n98.20%\\n99.10%\\n96.33%\\n100.00%\\n98.20%\\n100.00%\\n94.59%\\n98.23%\\n98.20%\\n98.23%\\n98.13%\\nFLAIR\\nT2\\nT1CE\\nT1\\nFigure 5: Comparison of Scores of RGNN3D Model for 4 MRI Stages\\nTraining Loss Training Accuracy\\nValidation \\nLoss\\nValidation \\nAccuracy\\nEpochs\\nEpochs\\nEpochs\\nEpochs\\nLoss\\nAccuracy\\n \\nLoss\\nAccuracy\\nFLAIR\\nT1CE\\nT2\\nT1\\nFLAIR\\nT1CE\\nT2\\nT1\\nFLAIR\\nT1CE\\nT2\\nT1\\nFLAIR\\nT1CE\\nT2\\nT1\\n0.7\\n0.6\\n0.5\\n0.4\\n0.3\\n0.2\\n0.1\\n0.0\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n10\\n20\\n30\\n40\\n50\\n0.0\\n0.2\\n0.4 \\n0. \\n6\\n0. \\n8\\n1.0\\n0.0\\n0.2\\n0.4 \\n0. \\n6\\n0. \\n8\\n1.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\n0.0\\nFigure 6: Accuracy and Loss Curves of RGNN3D Model for 4 MRI Stages\\nthe validation losss shows differently. Here, for T1 and T2, the validation loss follows training loss closely,184\\nwhich indicates stable learning in our proposed RGNN3D method. In T1CE, the validation loss shows185\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='98.59%\\n1\\n0.90\\n0.92\\n0.94\\n0.96\\n0.98\\n1.00\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n94.37%\\n100.00%\\n98.57%\\n100.00%\\n100.00%\\n98.57%\\n92.66%\\n98.57%\\n97.14%\\n10-FOLD CROSS VALIDATION of T1\\nBest \\nVal \\nAccuracy\\nper \\nFold\\nOverall \\nBest\\nAccuracy \\n(97.87%)\\nOverall \\nStd \\nDev\\n(±2.31%)\\n98.59%\\n1\\n0.90\\n0.92\\n0.94\\n0.96\\n0.98\\n1.00\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n92.96%\\n98.57%\\n98.57%\\n97.14%\\n100.00%\\n98.57%\\n97.14%\\n95.71%\\n98.14%\\n10-FOLD \\nCROSS \\nVALIDATION \\nof \\nT2\\nBest \\nVal \\nAccuracy\\nper \\nFold\\nOverall \\nBest\\nAccuracy \\n(97.44%)\\nOverall \\nStd \\nDev\\n(±1.87%)\\nFigure 7: Cross-Validation Performance of Glioma Grading with T1 and T2 MRI Modalities\\na bit higher, but it has stability. For the FLAIR stage, it significantly rises. This is why, the FLAIR186\\nmodality achieved the lowest accuracy compared to other modalities. Overall, the training loss curve shows187\\na consistent decrease compared to the validation loss trend, where we only observe consistency with T1 and188\\nT2. This phenomenon also supports its effectiveness with our architecture.189\\nWe extend our investigation into contemporary glioma grading systems that are not explicitly focused on190\\nradiomics features for 10-fold cross-validation. Among all the configurations tested, T1 and T2 were chosen191\\nsince they attained the highest overall accuracies. Figure 7 shows T1’s (upper part) validation accuracies192\\n(x-axis: fold numbers 1-10, y-axis: accuracy %), ranging from 92.66% (fold 7) to 100% (folds 3, 5, 8).193\\nThe overall average accuracy across all folds was 97.87% with a standard deviation of ± 2.31%, indicating194\\nstrong performance with some variation across folds. Figure 2 (lower part) shows T2’s accuracies, which195\\nranged from 92.96% (fold 2) to 100% (fold 3), with an average accuracy of 97.44% ± 1.87%, reflecting196\\nslightly lower but more consistent results compared to T1. Furthermore, the ROC curves concerning MRI197\\nmodality are displayed in Fig. 8 to confirm the proposed RGNN3D model’s strength in identifying glioma198\\ngrades. Remarkably, both GBM and LGG perform exceptionally prominent; their ROC curves for T1 and199\\nT2 approach the optimal top-left corner, which is consistent with their high accuracy and the prior described200\\nF1-scores. Conversely, FLAIR demonstrates a marginally lower AUC, confirming its comparatively poorer201\\nperformance. Taken together, our proposed architecture demonstrates better performance with T1 and T2202\\nstages compared to T1CE and FLAIR while grading glioma tumors.203\\nSubsequently, to illustrate the interpretability of our proposed architecture, we implement LIME [34].204\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='ROC Curve of T1\\nROC \\ncurve \\n(AUC=0.96)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\n ROC Curve of T2\\nROC \\ncurve \\n(AUC=0.97)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\nROC \\nCurve \\nof \\nT1CE\\nROC \\ncurve \\n(AUC=0.97)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\nROC \\ncurve \\n(AUC=0.96)\\nROC \\nCurve \\nof \\nFLAIR\\nROC \\ncurve \\n(AUC=0.97)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\nROC \\ncurve \\n(AUC=0.94)\\nFigure 8: Performance Analysis of RGNN3D Model for 4 MRI Stages in ROC Curves\\nAs shown in Fig. 9, the prioritization of specific radiomic features in LIME-based interpretations is driven205\\nby contribution to model performance where all radiomic features are clinically relevant. Still, the top three206\\nmost essential features make a significant contribution to the grading of glioma in these specific data points.207\\nFor GBM, the features “Mesh Volume”, “Maximum 2D diameter”, and “Maximum 3D diameter” are crucial208\\nfactors among the 112 features, underscoring their significant roles in distinguishing glioma. These features209\\nprovide critical information about the tumor’s size, shape, and spatial dimensions. For LGG, the features210\\n“Minor Axis Length”, “Maximum 2D diameter”, and “Gray Level Non-Uniformity” are the top features,211\\nemphasizing their interpretability and actionable insights in clinical decision-making. The shape and texture212\\nfeatures captured by these metrics are vital in differentiating between glioma grades and understanding the213\\nheterogeneity within the tumor. While the remaining features also contribute valuable information, they214\\nare comparatively less significant for several reasons, such as redundancy, specificity, statistical significance,215\\nclinical validation, and model performance.216\\nAt this point, we concentrate on the previous studies utilizing radiomic features for glioma tumor grading.217\\nFrom the current literature, we observe that most of the related studies focused on statistical ML models218\\nsuch as LR [10], SVM [11, 12], and RF [2, 13, 14]. Consequently, we aim to evaluate the performance of219\\nstatistical ML models on our dataset. Consequently, we implement five ML models (K-NN, LR, DT, RF,220\\nand SVM) with the acquired radiomic features using our dataset. Here, we use T1 stage MRI images as we221\\ngot good results during our prior analysis. Table 2 presents a comparative summary of the above-mentioned222\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='Mesh \\nVolume\\nMaximum \\n2D \\ndiameter \\n(1.00)\\nKurtosis \\n(-0.60)\\n10th \\npercentile \\n(-0.34)\\nLow \\nGray \\nLevel \\nZone \\nEmphasis \\n(-0.40)\\nLarge \\nDependence \\nLow \\nGray \\nLevel \\nEmphasis \\n(-0.21)\\nMaximum \\n3D \\ndiameter \\n(0.64)\\nSmall \\nArea \\nHigh \\nGray \\nLevel \\nEmphasis \\n(-0.12)\\nElongation \\n(-1.28)\\nSmall \\nDependence \\nHigh \\nGray \\nLevel \\nEmphasis \\n(-0.11)\\nMesh \\nVolume \\n(0.63)\\n0.07\\n0.04\\n0.04\\n0.04\\n0.05\\n0.05\\n0.05\\n0.05\\n0.06\\n0.27\\nMaximum \\n2D \\ndiameter \\nSmall \\nDependence \\nHigh \\nGray \\nLevel \\nEmphasis\\nElongation\\nSmall \\nArea \\nHigh \\nGray \\nLevel \\nEmphasis\\nMaximum \\n3D \\ndiameter\\nLarge \\nDependence \\nLow \\nGray \\nLevel \\nEmphasis\\nLow \\nGray \\nLevel \\nZone \\nEmphasis\\nKurtosis\\n10th \\npercentile\\nGBM\\n1.00\\nLGG\\n0.00\\nFEATURES \\nCONTRIBUTION \\nON \\nGBM\\nGRADING \\nABILITIES \\nOF \\nGBM\\nGBM \\nGRADING\\nMesh \\nVolume\\nMinor \\nAxis \\nLength \\n(2.55)\\nGray \\nLevel \\nNon-Uniformity \\n(3.08)\\nJoint \\nEntropy \\n(0.51)\\n90th \\npercentile \\n(0.23)\\nMaximum \\n2D \\ndiameter \\n(2.12)\\nSum \\nof \\nSquares \\n(-0.10)\\nJoint \\nAverage \\n(0.96)\\nZone \\nEntropy \\n(1.52)\\nHigh \\nGray \\nLevel \\nZone \\nEmphasis \\n(0.02)\\nMesh \\nVolume \\n(-0.76)\\n0.12\\n0.09\\n0.08\\n0.09\\n0.10\\n0.10\\n0.11\\n0.11\\n0.11\\n0.15\\nMinor \\nAxis \\nLength\\nHigh \\nGray \\nLevel \\nZone \\nEmphasis\\nZone \\nEntropy\\nJoint \\nAverage\\nSum \\nof \\nSquares\\nMaximum \\n2D \\ndiameter\\n90th \\npercentile\\nGray \\nLevel \\nNon-Uniformity\\nJoint \\nEntropy\\nGBM\\n0.00\\nLGG\\n1.00\\nFEATURES \\nCONTRIBUTION \\nON \\nLGG\\nGRADING \\nABILITIES \\nOF \\nLGG\\nLGG \\nGRADING\\nb\\n a\\nFigure 9: Analysis of Interpretability and Contribution of Radiomic Biomarkers for Glioma Grading using LIME: a) Explain-\\nability on grading GBM, b) Explainability on grading LGG\\nTable 2: Comparative Performance Analysis of Conventional ML Models with RGNN3D for Glioma Grading.\\nModel Precision Recall Specificity F1-Score AUC Accuracy\\nK-NN 96.30% 86.67% 99.10% 91.23% 92.88% 95.72%\\nLR 96.00% 80.00% 99.10% 87.27% 89.55% 92.69%\\nDT 90.32% 93.33% 97.30% 91.80% 95.32% 95.55%\\nRF 100.00% 93.33% 100.00% 96.55% 96.67% 96.79%\\nSVM 89.66% 86.67% 97.30% 88.14% 96.67% 92.87%\\nRGNN3D 100.00% 98.23% 100.00% 98.10% 97.00% 98.58%\\nML models and RGNN3D. After a thorough investigation, we noticed that RGNN3D outperformed all other223\\nML models across nearly all metrics, demonstrating superior performance in glioma grading. These results224\\nindicate the efficacy of our proposed model in accurately grading glioma tumors compared to other ML225\\nmodels.226\\nAs evidenced in Table 3, the proposed system achieves a testing accuracy of 98.58% on the BraTS 2019-20227\\ndataset, ensuring head-to-head competency. For image-based data-learning tasks, deep convolutional neural228\\nnetworks remain the gold standard. Still, our proposed neural network, trained on only radiomic features,229\\nshowcases comparable performance, narrowly missing out on the highest accuracy by less than 0.5%. In230\\nfact, RGNN3D outperforms the only other graph-based model in the table by around 5% in accuracy.231\\nThe comprehensive comparative analysis establishes RGNN3D as a reliable, accurate, and interpretable232\\nalternative to the existing state-of-the-art.233\\n3.1. Ablation Study234\\nTo validate the design of our RGNN3D model, we performed an ablation study that focuses on optimizers,235\\nactivation functions, and GCN block configurations (model depth). As shown in Table 4, the Adam optimizer236\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='Table 3: Comparative Analysis of State-of-the-Art Glioma Grading Methods with the Proposed RGNN3D Architecture.\\nArchitecture Dataset Result\\nMM-XGB, 2023 [35] BraTS 2020 93.00%\\nSASG-GCN, 2023 [36] TCGA-LGG 93.62%\\nSGD with ADASYN, 2023 [37]BraTS 2020 96.00%\\nCNN Based, 2023 [38] BraTS 2017-19 97.85%\\nMMD-VAE, 2022 [39] BraTS 2019 98.46%\\nTEWMA-CNN, 2022 [40] BraTS 2015,21 98.76%\\nTD-CNN-LSTM, 2022 [41] BraTS 2019-21 98.90%\\nProposed RGNN3D, 2025BraTS 2019-20 98.58%\\nnot only achieved the best accuracy, but it also ensured the fastest operation (33s x 50). It proves the237\\neffectiveness of performance and efficiency. While Admax and Nadam achieved competitive accuracies of238\\n95.03% and 97.87% respectively, they took longer computational times (52–83s for every epoch). In contrast,239\\nSGD optimizer is both slower and less accurate (39.83%) compared to others. For activation functions, ReLU\\nTable 4: Ablation Study: Impact of Optimizers, Activation Functions, and GCN Block Configurations on RGNN3D Perfor-\\nmance.\\nNo Variant Training Time ×Epoch Test Accuracy Findings\\nOptimizers\\n1 Adam 33 s ×50 98.58% Best accuracy\\n2 Adamax 83 s ×50 95.03% Good accuracy\\n3 Nadam 52 s ×50 97.87% Good accuracy\\n4 RMSprop 59 s ×50 97.16% Good accuracy\\n5 SGD 62 s ×50 39.83% Poor accuracy\\nActivation Functions\\n6 Sigmoid 64 s ×50 95.03% Good accuracy\\n7 Elu 67 s ×50 97.87% Good accuracy\\n8 ReLU 33 s ×50 98.58% Best accuracy\\n9 Tanh 60 s ×50 97.16% Good accuracy\\n10 Leaky ReLU 62 s ×200 97.16% Good accuracy\\nModel Blocks\\n11 1 GCN block 42 s ×50 93.17% Poor accuracy\\n12 2 GCN blocks 50 s ×50 97.87% Good accuracy\\n13 3 GCN blocks 62 s ×50 97.16% Good accuracy\\n14 RGNN3D (3 GCN + LSTM) 33 s ×50 98.58% Best accuracy\\nLSTM Cell Sizes\\n15 LSTM (8 units) 59 s ×50 97.87% Good accuracy\\n16 LSTM (16 units) 30 s ×50 97.16% Good accuracy\\n17 LSTM (32 units) 33 s ×50 98.58% Best accuracy\\n18 LSTM (64 units) 63 s ×50 98.58% Best accuracy but time consuming\\n240\\nactivation achieves 98.58% test accuracy with the lowest training time (33s in every single epoch). Although241\\nElu, Tanh, and Leaky ReLU achieved good accuracy, Sigmoid has shown poor performance.242\\nRegarding model architecture, a single GCN block is not good (93.17%) in terms of accuracy but it243\\nconsumes reasonable training time (42 s per epoch). Afterward, we added another GCN block to our model.244\\nBy doing it, performance was improved considerably (97.87%), while it took more training time, around 50245\\ns in every epoch. Then, we integrated another GCN block into our method, but the performance fell slightly246\\nand the time also increased. To decrease the computation power and increase robustness, we integrated247\\nthe LSTM layer into our model. The proposed RGNN3D model (3 GCN + LSTM) not only achieved248\\nthe highest accuracy (98.58%) but also trained efficiently (33s × 50 epochs), which offered the benefit of249\\ncombining spatial and temporal learning. It is clear that LSTM enhances temporal learning with the highest250\\nperformance, and it also maintains computational power efficiently.251\\nTo experiment on the impact of recurrent layers, we conducted an ablation study on different LSTM252\\ncell sizes (8, 16, 32 and 64 units). Smaller LSTM configurations with 8 and 16 units achieved slightly lower253\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='accuracies of 97.87% and 97.16% respectively, while they are computationally efficient. In contrast, the unit254\\nsize 64 also achieves the best accuracy, but it takes the highest time as computational power. It means a255\\nlarger LSTM cell does not offer performance benefits. We noted that using LSTM with a 32 cell size gives us256\\nthe best accuracy (98.58%), aslo the least time consuming (33 s per epoch). That makes the most balanced257\\nconfiguration for our proposed (RGNN3D) model. So, these findings support our architectural choices and258\\ndemonstrate the strength of RGNN3D for glioma grading.259\\n3.2. Limitations260\\nThis study has a few limitations. The model was trained on a limited dataset (BraTS 2019-20), which261\\nmay affect its generalizability across diverse patient groups. Performance on FLAIR images was lower,262\\nsuggesting room for improvement in preprocessing. While LIME helps explain predictions, deeper clinical263\\ninterpretability remains a challenge. Lastly, the model has not yet been tested in real clinical environments,264\\nwhich is essential for confirming its practical utility.265\\n4. Conclusion266\\nThis study innovated a novel hybrid architecture that could be used to grade glioma tumors reliably and267\\naccurately utilizing 3D MRI data. Hence, one of the key strengths of the RGNN3D model is its ability to268\\neffectively integrate and utilize medical radiomic features, which are critical in the accurate characterization269\\nof tumor heterogeneity and progression. We believe the RGNN3D architecture would serve as an intuitive270\\ndecision support system for medical experts by significantly improving diagnostic precision. Our evaluation271\\nacross four MRI stages (T1, T1CE, T2, and FLAIR) reveals that both T1 and T2 stages can be utilized to get272\\nthe highest performance in terms of grading accuracy, demonstrating their robustness in capturing critical273\\ntumor characteristics. Future research should focus on integrating multi-parametric MRI data to leverage274\\nthe strengths of each modality. Additionally, improving preprocessing and feature extraction techniques275\\nfor FLAIR images may assist in mitigating current limitations and enhancing their utility in grading. The276\\nmodel’s robustness could be further enhanced with a larger dataset that includes diverse patient glioma277\\ngrading reports, ensuring broader applicability and accuracy. We also believe that our proposed system278\\ncould be applicable in clinical settings and digital healthcare, especially in rural or isolated places with279\\nlimited access to specialist physicians.280\\nReferences281\\n[1] D. Ricard, A. Idbaih, F. Ducray, M. Lahutte, K. Hoang-Xuan, J.-Y. Delattre, Primary brain tumours in adults, The282\\nLancet 379 (9830) (2012) 1984–1996.283\\n[2] R. Kumar, A. Gupta, H. S. Arora, G. N. Pandian, B. Raman, Cghf: A computational decision support system for glioma284\\nclassification using hybrid radiomics-and stationary wavelet-based features, IEEE Access 8 (2020) 79440–79458.285\\n[3] E. B. Claus, K. M. Walsh, J. K. Wiencke, A. M. Molinaro, J. L. Wiemels, J. M. Schildkraut, M. L. Bondy, M. Berger,286\\nR. Jenkins, M. Wrensch, Survival and low-grade glioma: the emergence of genetic information, Neurosurgical focus 38 (1)287\\n(2015) E6.288\\n[4] Q. T. Ostrom, L. Bauchet, F. G. Davis, I. Deltour, J. L. Fisher, C. E. Langer, M. Pekmezci, J. A. Schwartzbaum, M. C.289\\nTurner, K. M. Walsh, et al., The epidemiology of glioma in adults: a “state of the science” review, Neuro-oncology 16 (7)290\\n(2014) 896–913.291\\n[5] H. Hyare, S. Thust, J. Rees, Advanced mri techniques in the monitoring of treatment of gliomas, Current treatment292\\noptions in neurology 19 (2017) 1–15.293\\n[6] Z. Liu, S. Wang, D. Dong, J. Wei, C. Fang, X. Zhou, K. Sun, L. Li, B. Li, M. Wang, et al., The applications of radiomics294\\nin precision diagnosis and treatment of oncology: opportunities and challenges, Theranostics 9 (5) (2019) 1303.295\\n[7] M. E. Mayerhoefer, A. Materka, G. Langs, I. H¨ aggstr¨ om, P. Szczypi´ nski, P. Gibbs, G. Cook, Introduction to radiomics,296\\nJournal of Nuclear Medicine 61 (4) (2020) 488–495.297\\n[8] J.-b. Qin, Z. Liu, H. Zhang, C. Shen, X.-c. Wang, Y. Tan, S. Wang, X.-f. Wu, J. Tian, Grading of gliomas by using radiomic298\\nfeatures on multiple magnetic resonance imaging (mri) sequences, Medical science monitor: international medical journal299\\nof experimental and clinical research 23 (2017) 2168.300\\n[9] G. Cui, J. J. Jeong, Y. Lei, T. Wang, T. Liu, W. J. Curran, H. Mao, X. Yang, Machine-learning-based classification of301\\nglioblastoma using mri-based radiomic features, in: Medical imaging 2019: computer-aided diagnosis, Vol. 10950, SPIE,302\\n2019, pp. 1063–1068.303\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='[10] H.-H. Cho, H. Park, Classification of low-grade and high-grade glioma using multi-modal image radiomics features, in:304\\n2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), IEEE,305\\n2017, pp. 3081–3084.306\\n[11] Q. Chen, L. Wang, L. Wang, Z. Deng, J. Zhang, Y. Zhu, Glioma grade prediction using wavelet scattering-based radiomics,307\\nIEEE Access 8 (2020) 106564–106575.308\\n[12] P. Sun, D. Wang, V. C. Mok, L. Shi, Comparison of feature selection methods and machine learning classifiers for radiomics309\\nanalysis in glioma grading, Ieee Access 7 (2019) 102010–102020.310\\n[13] J. Cheng, J. Liu, H. Yue, H. Bai, Y. Pan, J. Wang, Prediction of glioma grade using intratumoral and peritumoral radiomic311\\nfeatures from multiparametric mri images, IEEE/ACM Transactions on Computational Biology and Bioinformatics 19 (2)312\\n(2020) 1084–1095.313\\n[14] H.-h. Cho, S.-h. Lee, J. Kim, H. Park, Classification of the glioma grading using radiomics analysis, PeerJ 6 (2018) e5982.314\\n[15] S. Priya, Y. Liu, C. Ward, N. H. Le, N. Soni, R. Pillenahalli Maheshwarappa, V. Monga, H. Zhang, M. Sonka, G. Bathla,315\\nMachine learning based differentiation of glioblastoma from brain metastasis using mri derived radiomics, Scientific reports316\\n11 (1) (2021) 10478.317\\n[16] U. Baid, S. U. Rane, S. Talbar, S. Gupta, M. H. Thakur, A. Moiyadi, A. Mahajan, Overall survival prediction in318\\nglioblastoma with radiomic features using machine learning, Frontiers in computational neuroscience 14 (2020) 61.319\\n[17] H. Li, Z. Wang, C. Lan, P. Wu, N. Zeng, A novel dynamic multiobjective optimization algorithm with non-inductive320\\ntransfer learning based on multi-strategy adaptive selection, IEEE transactions on neural networks and learning systems321\\n(2023).322\\n[18] P. Wu, H. Li, L. Hu, J. Ge, N. Zeng, A local-global attention fusion framework with tensor decomposition for medical323\\ndiagnosis, IEEE/CAA Journal of Automatica Sinica 11 (6) (2024) 1536–1538.324\\n[19] T. Xie, X. Chen, J. Fang, H. Kang, W. Xue, H. Tong, P. Cao, S. Wang, Y. Yang, W. Zhang, Textural features of dy-325\\nnamic contrast-enhanced mri derived model-free and model-based parameter maps in glioma grading, Journal of Magnetic326\\nResonance Imaging 47 (4) (2018) 1099–1111.327\\n[20] S. Bisdas, C. Tisca, C. Sudre, E. Sanverdi, D. Roettger, J. M. Cardoso, Non-invasive in vivo prediction of tumour grade and328\\nidh mutation status in gliomas using dynamic susceptibility contrast (dsc) perfusion-and diffusion-weighted mri. (2018).329\\n[21] Q. Tian, L.-F. Yan, X. Zhang, X. Zhang, Y.-C. Hu, Y. Han, Z.-C. Liu, H.-Y. Nan, Q. Sun, Y.-Z. Sun, et al., Radiomics330\\nstrategy for glioma grading using texture features from multiparametric mri, Journal of Magnetic Resonance Imaging331\\n48 (6) (2018) 1518–1528.332\\n[22] C. Su, J. Jiang, S. Zhang, J. Shi, K. Xu, N. Shen, J. Zhang, L. Li, L. Zhao, J. Zhang, et al., Radiomics based on333\\nmulticontrast mri can precisely differentiate among glioma subtypes and predict tumour-proliferative behaviour, European334\\nradiology 29 (2019) 1986–1996.335\\n[23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, S. Y. Philip, A comprehensive survey on graph neural networks, IEEE336\\ntransactions on neural networks and learning systems 32 (1) (2020) 4–24.337\\n[24] A. Graves, A. Graves, Long short-term memory, Supervised sequence labelling with recurrent neural networks (2012)338\\n37–45.339\\n[25] M. T. Ribeiro, S. Singh, C. Guestrin, ” why should i trust you?” explaining the predictions of any classifier, in: Proceedings340\\nof the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016, pp. 1135–1144.341\\n[26] Multimodal Brain Tumor Segmentation Challenge 2019: Data — CBICA — Perelman School of Medicine at the University342\\nof Pennsylvania — med.upenn.edu, www.med.upenn.edu/cbica/brats2019/data.html, [Accessed 23-06-2024].343\\n[27] Multimodal Brain Tumor Segmentation Challenge 2020: Data — CBICA — Perelman School of Medicine at the University344\\nof Pennsylvania — med.upenn.edu, www.med.upenn.edu/cbica/brats2020/data.html, [Accessed 23-06-2024].345\\n[28] J. E. Van Timmeren, D. Cester, S. Tanadini-Lang, H. Alkadhi, B. Baessler, Radiomics in medical imaging—“how-to”346\\nguide and critical reflection, Insights into imaging 11 (1) (2020) 91.347\\n[29] J. J. Van Griethuysen, A. Fedorov, C. Parmar, A. Hosny, N. Aucoin, V. Narayan, R. G. Beets-Tan, J.-C. Fillion-Robin,348\\nS. Pieper, H. J. Aerts, Computational radiomics system to decode the radiographic phenotype, Cancer research 77 (21)349\\n(2017) e104–e107.350\\n[30] T. T. Truong, D. Dinh-Cong, J. Lee, T. Nguyen-Thoi, An effective deep feedforward neural networks (dfnn) method for351\\ndamage identification of truss structures using noisy incomplete modal data, Journal of Building Engineering 30 (2020)352\\n101244.353\\n[31] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation 9 (8) (1997) 1735–1780.354\\n[32] T. N. Kipf, M. Welling, Semi-supervised classification with graph convolutional networks, International Conference on355\\nLearning Representations (ICLR), 2017.356\\n[33] M. A. Ali, M. S. Hossain, M. K. Hossain, S. S. Sikder, S. A. Khushbu, M. Islam, Amdnet23: Hybrid cnn-lstm deep357\\nlearning approach with enhanced preprocessing for age-related macular degeneration (amd) detection, Intelligent Systems358\\nwith Applications 21 (2024) 200334.359\\n[34] A. Palkar, C. C. Dias, K. Chadaga, N. Sampathila, Empowering glioma prognosis with transparent machine learning and360\\ninterpretative insights using explainable ai, IEEE Access 12 (2024) 31697–31718.361\\n[35] F. Ullah, M. Nadeem, M. Abrar, F. Amin, A. Salam, A. Alabrah, H. AlSalman, Evolutionary model for brain cancer-362\\ngrading and classification, IEEE Access (2023).363\\n[36] L. Liu, J. Chang, P. Zhang, H. Qiao, S. Xiong, Sasg-gcn: self-attention similarity guided graph convolutional network for364\\nmulti-type lower-grade glioma classification, IEEE Journal of Biomedical and Health Informatics (2023).365\\n[37] M. Renugadevi, K. Narasimhan, C. Ravikumar, R. Anbazhagan, G. Pau, K. Ramkumar, M. Abbas, N. Raju, K. Satish,366\\nS. Prabu, Machine learning empowered brain tumor segmentation and grading model for lifetime prediction, IEEE Access367\\n(2023).368\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-09-25T05:16:01+00:00', 'moddate': '2025-09-25T05:16:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': 'Portfolio/data/rgnn3d.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='[38] H. A. Hafeez, M. A. Elmagzoub, N. A. B. Abdullah, M. S. Al Reshan, G. Gilanie, S. Alyami, M. U. Hassan, A. Shaikh,369\\nA cnn-model to classify low-grade and high-grade glioma from mri images, IEEE Access 11 (2023) 46283–46296.370\\n[39] J. Cheng, M. Gao, J. Liu, H. Yue, H. Kuang, J. Liu, J. Wang, Multimodal disentangled variational autoencoder with game371\\ntheoretic interpretability for glioma grading, IEEE Journal of Biomedical and Health Informatics 26 (2) (2021) 673–684.372\\n[40] S. Divya, L. Padma Suresh, A. John, Enhanced deep-joint segmentation with deep learning networks of glioma tumor for373\\nmulti-grade classification using mr images, Pattern Analysis and Applications 25 (4) (2022) 891–911.374\\n[41] S. Montaha, S. Azam, A. R. H. Rafid, M. Z. Hasan, A. Karim, A. Islam, Timedistributed-cnn-lstm: A hybrid approach375\\ncombining cnn and lstm to classify brain tumor on 3d mri scans performing ablation study, IEEE Access 10 (2022)376\\n60039–60059.377\\n16')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data = load_pdf_files(\"Portfolio/data\")\n",
    "extracted_data\n",
    "# len(extracted_data) : total pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa9bb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def filter_to_minimal_docs(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Given a list of Document objects, return a new list of Document objects\n",
    "    containing only 'source' in metadata and the original page_content.\n",
    "    \"\"\"\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\": src}\n",
    "            )\n",
    "        )\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594f5a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='AMDNet23: A combined deep Contour-based Convolutional Neural Network and Long \\nShort Term Memory system to diagnose Age-related Macular Degeneration \\nMd. Aiyub Ali1, Md. Shakhawat Hossain1, Md.Kawar Hossain1, Subhadra Soumi Sikder1, \\nSharun Akter Khushbu1 and Mirajul Islam1 \\n1 Department of Computer Science and Engineering, Daffodil International University, Dhaka \\n1341, Bangladesh \\nCorrespondence: Mirajul Islam; merajul15-9627@diu.edu.bd \\nAbstract \\nIn light of the expanding population, an automated framework of disease detection can assist doctors in the \\ndiagnosis of ocular diseases, yields accurate, stable, rapid outcomes, and improves the success rate of early \\ndetection. The work initially intended the enhancing the quality of fundus images by employing an adaptive \\ncontrast enhancement algori thm (CLAHE) and Gamma correction. In the preprocessing techniques, \\nCLAHE elevates the local contrast of the fundus image and gamma correction increases the intensity of \\nrelevant features. This study operates on a AMDNet23 system of deep learning that combi ned the neural \\nnetworks made up of convolutions (CNN) and short-term and long-term memory (LSTM) to automatically \\ndetect aged macular degeneration (AMD) disease from fundus ophthalmology. In this mechanism, CNN is \\nutilized for extracting features and LSTM is utilized to detect the extracted features. The dataset of this \\nresearch is collected from multiple sources and afterward applied quality assessment techniques, 2000 \\nexperimental fundus images encompass four distinct classes equitably. The proposed hybri d deep \\nAMDNet23 model demonstrates to detection of AMD ocular disease and the experimental result achieved \\nan accuracy 96.50%, specificity 99.32%, sensitivity 96.5%, and F1 -score 96.49.0%. The system achieves \\nstate-of-the-art findings on fundus imagery dat asets to diagnose AMD ocular disease and findings \\neffectively potential of our method. \\n \\nKeywords: AMDNet23, Fundus image classification, CNN-LSTM, ocular diseases, automated diagnosis, \\nconvolutional neural networks, long short-term memory,  early detection, Medical imaging, diagnosis. \\n \\nIntroduction \\nOver the past two decades, ocular diseases ( ODs) that can cause blindness have become extremely \\nwidespread. ODs encompass a wide range of c onditions that can affect various components of the ocular, \\nincluding the corneal tissue, lens, retina, optic nerves and periorbital tissues. Ocular diseases include \\nabnormalities such as cataracts, untreated nearsightedness, trachoma, macular degeneration associated with \\naging, and diabetes -associated retinopathy. These ailments play a substantial role in global retinal \\ndegeneration and visual impairment. [1]. The worldwide prevalence of near - or farsightedness vision \\ndeficiency affects over 2.2 billion in dividuals [2]. Approximately half of the total cases, amounting to at \\nleast 1 billion folks, as reported by the World Health Organization (WHO), suffer from vision impairments \\nthat could have been evaded or remain unattended. Among these individuals, aroun d 88.4 million have \\nuntreated refractive errors leading to adequate to extensive distant impaired vision, nearly ninety -four \\nmillion owned cataracts,  and eight million individuals are possessed by aged-related macular degeneration, \\ndiabetic retinopathy (3.9 million). [3] Despite significant investment, the number of individuals living with \\nvision loss might increase to 1.7 billion by 2050, from the 1.1 billion people accomplished in the year 2020. \\nAge-associated macular degeneration (AMD) predominantly strikes the older demographic, resulting in the'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='gradual deterioration of the macula, a crucial part of the retina in charge of the central region of perception. \\nThe consequences of AMD manifest as central vision abnormalities, including blurred or distorted vision, \\nwhich significantly impede various daily activities[4].  \\nAccurate and earlier identification of AMD disease explicitly a vital role in safeguarding irreversible \\ndamage to vision and initiating timely treatment and safeguarding ocular health. Machine  learning \\ntechniques have advanced to the point where early identification of aged macular degeneration eye illness \\nby an automated system has significant advantages over manual detection [5] . As aids in diagnosing eye \\ndiseases, digital pictures of the eye  and computational intelligence (CI)-based technologies assist doctors \\nin diagnosis. In the realm of diagnosing eye diseases, digital eye images and computational intelligence \\n(CI)-based technologies serve as indispensable tools, enabling doctors to enhanc e their diagnostic \\ncapabilities[6]. In medical imaging, there are also various approaches are employed including fundus \\nphotography, [7] optical coherence tomography (OCT), and imaging modalities specifically designed for \\nthe eye. These imaging technologie s allow for detailed visualization and analysis of ocular structures, \\nfacilitating the identification of characteristic features and abnormalities associated with age -related \\nmacular degeneration diseases.  \\nSeveral researchers have demonstrated a critical task in ophthalmology, facilitating the early detection and \\ndiagnosis of aged macular degeneration (AMD) ocular disease using fundus image.  \\nResearchers have focused on deep learning [8-10], The fields of  vision for computing [11,12]  and the use \\nof predictive learning of machines [13,14] method have used to develop robust classification models to \\nidentify retinal images into AMD disease categories accurately. The incorporation of deep learning \\nmethodologies [57,58] plays a pivotal role in accurately class ifying diverse ocular diseases, thereby \\nensuring the advancement of intelligent healthcare practices in the field of ophthalmology [15]. \\nTherefore, this paper seeks to demonstrate a novel system employing a AMDNet23 framework, the deep \\nmechanism of CNN and LSTM networks is combined for the automated identification of AMD from fundus \\nphotography. Within this approach, CNN performs the purpose of extracting fundus features, and LSTM \\nundertakes the crucial task of classification AMD constructed on the extracte d features. Internal memory \\ninside the LSTM network empowers it to learning knowledge gained from significant experiences with \\nextended period of condition. In the fully interconnected networks, each layer is linked comprehensively, \\nand the nodes in betwee n layers construction are unconnected, and  LSTM nodes connection within a \\ndirected graph accompined a temporal order, which serves as an input with a specific form [16] . The hybrid \\ntwo dimensional CNN and LSTM system combination improves classification of AMD ophthalmology and \\nassists clinical decision, the dataset collected from several sources and preprocessing technique for the \\nimage quality enhancemnet to classify AMD efficiently. The assets of this research have been articulated \\nin the following.  \\n \\na) Constructing a combination of CNN -LSTM based AMDNet23 framework for the automated diagnosis \\nof  AMD and aiding clinical physician in the early detection of patients. \\nb) The collection data has investigated by employing the contour -based quality assessmen t technique in \\nidentifying the structure of fundus photography, Ocular illumination levels fundus images are automatically \\neliminated. \\nc) To enhance image quality, CLAHE improves the visibility of subtle details and enhances local contrast \\nand Gamma correction adjusts the intensity levels, improving image quality and facilitating better diagnosis \\nof AMD.'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"d) AMDNet23 hybrid framework for detection of AMD utilizing fundus image ophthalmology, data \\ncomprising 2000 images equitively. \\ne) An empirical evaluation is accessible encompassing accuracy, specificity, sensitivity, F1-measure, and a \\nconfusion matrix to assess the effectiveness of the proposed method. \\n \\nThe rest of the contents of this article are arranged a manner as follows: Section II covers the related works \\nof this research. Section III Section III articulates the proposed AMDNet23 methodology, including data \\ncollection and preprocessing techniques, and a comparison of some existing models. Section IV covers the \\nexperimental findings and discussion, inc luding state-of-the-art and transfer learning comparisons. The \\nconclusion is presented in Section V. \\n \\n \\n \\nRelated work \\nIn the pursuit of identifying the ocular disease, researchers have harnessed the power of deep learning \\ntechniques. These methods leverage fundus ophthalmology to facilitate the diagnosis of ocular diseases. \\nThis reviewed literature presents cutting -edge systems that developed deep -learning techniques for \\ndetecting AMD, diabetes, and cataracts. \\nM Sahoo et. al[17] proposed an innovative ensemb le-based prediction model called weighted majority \\nvoting (WMV) for the exclusive diagnosis of Dry -AMD. This approach intelligently combines the \\npredictions from various base classifiers, utilizing assigned weights to each classifier. The WMV model \\ndemonstrates remarkable accuracy, achieving 96.15% and 96.94% accuracy rates, respectively. P \\nMuthukannan et. al. [18] introduced a computer aided approach that leverages the Flower pollination \\noptimization approach (FPOA) in accompanied with a CNN mechanism for preprocessing, specifically \\nutilizing the maximum entropy transformation on the ODIR public dataset. The model's performance was \\nthen benchmarked against other optimized models, demonstrating superior accuracy at 95.27%. In a study \\nby Serener et al. [19], their goal was to employ OCT images and deep neural networks to detect both dry \\nand wet AMD. Regarding the purpose, two architectures—AlexNet and ResNet—were used. The outcomes \\nrevealed that the eighteen -layer ResNet model correctly identified AMD with an astounding accuracy of \\n94%, whereas the AlexNet model produced an accuracy of 63%. \\nThere are several deep learning methods for detecting cataracts, because of the drawbacks of feature \\nextraction and preprocessing, these methods don't always produce adequat e results. Kumar et al. [20] \\nproposed several models to improve clinical decision -making for ophthalmologists. Paradisa et al. [21] \\nFundus images applied the Concatenate model with For feature extraction, Inception -ResNet V2 and \\nDenseNet121 are implemented , and MLP is deployed for classification and average accuracy was 91%. \\nFaizal et al.  [22] An automated cataract detection algorithm using CNN achieves high accuracy (95%) by \\nanalyzing visible wavelength and anterior segment images, enabling cost -effective early detection of \\nvarious cataract types. Pahuja et al. [23]  To enhance the model performance, data augmentation and \\nmethods to extract features have been performed. Therefore they used CNN and SVM models for the \\ndetection of cataract on a dataset compr ising normal and cataract retinal images, achieving high accuracy \\nof 87.5% for SVM and 85.42% for CNN. Hence, et al. [24] used a CNN model to diagnose cataract \\npathology with digital camera images. The model achieves high accuracy (testing: 0.9925, training: 0.9980) \\nwhile optimizing processing time. It demonstrates the potential of CNNs for cataract diagnosis. Although \\net al [25] used color fundus images to detect cataracts.\"),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"A variety of computer vision engineering approaches are used to forecast the Diabet ic retinopathy (DR)'s \\noccurrences and phases automatically. Mondal et al. [26] Their model is a collaborative deep neural system \\nfor automated diabetes-related retinopathy (DR) diagnosis and categorization using two models: modified \\nDenseNet101 and ResNeXt. Experiments were conducted on APTOS19 and DIARETDB1 datasets, with \\ndata augmentation using GAN-based techniques. Results show higher accuracy with accuracy for each of \\nthe five classes reached 86.08%, whilst for each of the two classes the score was 96.98%. Whereas, a ML-\\nFEC model with pre-trained CNN architecture was proposed for Diabetic Retinopathy (DR) detection using \\nResNet50, ResNet152, and SqueezeNet1. On testing with DR datasets, ResNet50 achieved 93.67%, \\nSqueezeNet1 achieved 91.94%, and ResNet152  achieved 94.40% accuracy, demonstrating its suitability \\nfor clinical implementation and large-scale screening programs. Using a novel CNN model, Babenko et al. \\n[27] were able to multi-class categorize retinal fundus pictures from a publically accessible dataset with an \\naccuracy of 81.33% for diabetic eye disease. Priorly  based on UNet architecture, et al. [28] achieved \\n95.65% accuracy in identifying red lesions and 94% accuracy in classifying DR levels of severity. The \\napproach was examined utilizing publically accessible datasets: IDRiD (99% specificity, 89% sensitivity) \\nand MESSIDOR (94% accuracy, 93.8% specificity, 92.3% sensitivity). \\nMethodology \\nAging macular degenerative disorder (AMD) is an advancing retinal condition that predominantly impacts \\nindividuals over the age of 50. This eye disease can significantly affect eyesight, leading to various visual \\nissues like blurred or distorted vision, where straight lines might appear wavy or twisted. Moreover, it \\ncauses a loss of central vision, the emergence of dark or empty spots at the center of vision, and alterations \\nin color perception. Thus taking proactive steps to prevent eye diseases is crucial for maintaining clear and \\nvibrant sight throughout our lives. In recent years, neural networks containing layers of convolution (CNNs) \\nhave demonstrated considerable potential in the processing of medical images. It has the remarkable \\ncapacity to recognize and extract meaningful features from images automatically.  Figure 1 outlines the \\nsteps in developing the proposed CNN-based methods for AMD eye disease detection:\"),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Figure 1:  Overall proposed-based Method \\nA. Data Collection: \\nThe case of normal class represents the absence of any specific eye disease or condition. A healthy eye \\nfunctions optimally, providing clear and unimpaired vision.  Diabetes, a systemic disease characterized by \\nelevated blood sugar levels, can lead to various ocular complications. Diabetic ocular disorders, notably \\ndiabetic retinopathy, occur when the blood vessels found in the retina undergo damage as a consequence of \\nelevated blood sugar concentration [29]. Older individuals are predominantly affected by age -associated \\nmacular degeneration (AMD) and involves the progressive deterioration of the macula, a small but crucial \\nof the core vision-related region of the retina. AMD can lead to blurred or distorted central vision, impacting \\nactivities [30]. A cataract is another common eye condition, particularly associated with aging. It involves \\nclouding the crystalline lens inside the eye, leading to inconsistent or foggy vision [31]. Fig. 2. Shows the \\nsample images of Normal, Cataract, AMD and Diabetes respectively.'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"Figure 2: Types of fundus ophthalmology \\nTo train a robust CNN model, a diverse and well -annotated dataset of AMD and non-AMD eye images is \\nessential. The dataset employed in this study containing a total of 2000 images, was put together by \\nassessing the quality of the images from six other public datasets. Those datasets are ODIR[32], DR -\\n200[33], Fundus Dataset[34] , RFMiD[35], ARIA[36], and Eye_Diseases_Classification[37]. From these \\ndatasets. The quality assessment was done using contour techniques[38]. The contour -based approach \\nfocuses on the sharpness and clarity of edges, as they play a crucial role in human as sess the quality of \\nimagery. Which included illumination level, visibility structure, color and contrast, and direct eye image. \\nFigure 3 represents a sample of the assessed image quality. \\n \\nFigure 3:  Contour-based approach \\nHere it can be seen that sharp, well-defined edges contribute to high -quality images, while blurry or \\ndistorted edges indicate poor quality. Such poor -quality images would negatively impact the machine's \\nperception. We put together a dataset consisting of four classes: Normal, Diabetes, AMD, and Cataract,\"),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='where each class contains 500 images. Table 1 indicates the quantity of accessible and selected images \\n(within the first bracket) from those six public datasets. \\nDatasets Normal Diabetes AMD Cataract \\nODIR[33] 2873(168) 1608(300) 266(266) 293(256) \\nDR-200[34] 1000(332) 1000(150)   \\nFundus Dataset[35]   46(46) 100(44) \\nRFMiD[36]  376(50) 100(100)  \\nARIA[37]   101(88)  \\nEye_Diseases_Classification[38]    1038(200) \\n \\nB.Data Pre-processing: \\nPreprocessing, which is the strong suits of the prop osed work, was focused on enhancing image quality, \\nand some of the preprocessing techniques applied to this work were not used by the previously proposed \\ncataract disease detection works. The data were preprocessed in different color spaces (as shown in Figure \\n4) for extracting the features while bettering the practicability of our models. Among the RGB(G), HSV(V), \\nand LAB(L) color spaces, the vessels were visible in the LAB(L) color space. As a result, the LAB(L) color \\nspace was chosen.  \\n \\nFigure 4: Color spaces \\nLater, some preprocessing algorithms like CLAHE and Gamma correction[39] were applied to enhance \\nimage quality by adjusting the brightness and contrast. We experimented with several parameters for these \\nalgorithms and finally got the satisfying result for CLAHE(2.0, (8,8)). For gamma values of 0.5, the image \\nis found to become darkened. Moreover, for gamma values of 2.0, the image is found quite faded. To \\novercome this problem CLAHE is used to enhance regional contrasting, making the image more visua lly \\nappealing and informative[40]. Figure 5 shows the resulting images for our experimented algorithms along \\nwith the finalized CLAHE (2.0, (8,8)) for our model.'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Figure 5: Gamma and CLAHE based quality enhancement \\nHistogram comparison between the applied Context-limited adapted equalization of histograms (CLAHE) \\npreprocessing algorithm and a normal image in Figure 6 can help illustrate the effects of CLAHE on \\nenhancing local contrast and improving image quality.'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Figure 6: Hitogram comparisons \\nBy comparing the histograms, we can observe the changes in pixel intensity distribution before and after \\napplying CLAHE. In the normal image, the histogram exhibits a relatively uniform distribution of pixel \\nintensities, with some variations depending on th e content of the image. CLAHE is implemented as a \\ncomponent in the preprocessing, it adapts the contrast enhancement locally, making it particularly effective \\nin improving the contrast of regions with varying intensities. This helps reveal hidden details a nd textures \\nthat might have been obscured in the original image. \\n \\nImages MSE PSNR SSIM \\n2376_left.jpg 2388.13 14.35 0.48 \\n84_right.jpg 2189.98 14.72 0.65 \\n980_right.jpg 927.54 18.45 0.53 \\n71_left.jpg 709.37 19.62 0.54 \\n \\nThe effectiveness of the quality of image preprocessing, Table 2 displays the readings of the metrics mean \\nsquare errors (MSE), Peak Signal -to-Noise Ratio (PSNR), and Structures Similarity Index (SSIM) [41].'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"These metrics compare the preprocessed image to the original image to determine the level of distortion or \\nsimilarity. \\nMean Squared Error (MSE ): MSE generates  the resultant mean squared disparity between the \\npreprocessed and original image's pixel values. Lesser MSE reading indicate greater similarity between the \\nimages. MSE is calculated using the formula:  \\n𝑀𝑆𝐸 = 1\\n𝑚∗𝑛 ∑ ∑[𝐼(𝑥,𝑦)−𝐾(𝑥,𝑦)]2\\n𝑛−1\\n𝑦=0\\n𝑚−1\\n𝑥=0\\n \\nwhere𝑚∗𝑛 represents the image dimensions, 𝐼(𝑥,𝑦) and 𝐾(𝑥,𝑦) indicate preprocessed image’s pixel \\nvalues and original images at coordinates(𝑥,𝑦). \\nPeak Signal-to-Noise Ratio (PSNR):  The optical appealing of preprocessed photographs is frequently \\nassessed utilizing the PSNR measure. It calculates a measure of the peak power ratio of the signal's strength \\nto noise, which assessed in decibels (dB). Increased PSNR values indicated a greater similarity between the \\nimages. PSNR is calculated using the formula: \\n𝑃𝑆𝑁𝑅 = 10𝑙𝑜𝑔10\\n𝑀𝐴𝑋2\\n𝑀𝑆𝐸  \\nwherein MAX is the highest pixel value that is permitted to be used (for example, 255 in 8-bit photographs). \\nStructural Similarity Index (SSIM): SSIM evaluates the luminosity, contrary, and structural similarities \\nbetween the preprocessed image and original image. The value of 1 denotes complete similarity, using \\nSSIM readings varying from -1 to 1. Higher SSIM values indicate better similarity between the images. \\nSSIM is calculated using a combination of mean, variance, and covariance of the image patches. \\n𝑆𝑆𝐼𝑀 = (2𝜇𝑥𝜇𝑦 +𝑐1)(2𝜎𝑥𝑦 +𝑐2)\\n(𝜇𝑥2 +𝜇𝑦2 +𝑐1)(𝜎𝑥2 +𝜎𝑦2 +𝑐2) \\nwhere c1 and c2 are constants to prohibit division by zero, σ and μ stand representing the standard deviations \\nand mean respectively. \\nBy calculating MSE, PS NR, and SSIM before and after image preprocessing, The effectiveness of the \\npreprocessing techniques in preserving image quality or reducing noise, artifacts, or other undesired effects \\ncan be determined. Lesser MSE readings, greater PSNR readings, and greater SSIM values indicate better \\nimage quality preservation. \\n \\n \\nC. Comparison of some existing models  \\nTransfer Learning: \\nIn this research study, a handful of models were trained and evaluated. Some of those models are discussed \\nbelow in the following sections: \\n \\ni.ViTB16\"),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"The ViTB16 model, also known as Vision Transformer Base with a depth of 16 layers, is a deep learning \\narchitecture specifically designed for image classification tasks [42]. 224*224 pixels were made up of the \\nsize of the input images. A grid  of patches containing fixed sizes is used to divide the input image. Each \\nindividual patch is positioned linearly to obtain a lower-dimensional representation. The patch embeddings \\nare enhanced by employing positional encoding to provide the model with spatial information. The model \\nis capable of finding relationships dependencies between different patches thanks to a self -attention \\nmechanism. It calculates attention scores between all pairs of patches and applies weighted averaging to \\naggregate information. Layer normalization is applied after the self -attention mechanism to normalize the \\noutput and improve training stability. SGD (Stochastic Gradient Descent) optimizer was employed for \\ntraining the model, and the learning rate was 0.0001. \\nii.DenseNet121, DenseNet169:  \\nDenseNet[43] is a famous deep -learning architecture known for its dense connections between layers, \\nenabling effective feature reuse and alleviating the vanishing gradient problem. DenseNet121 and \\nDenseNet169 have 121 and 169 layers, respectively, making DenseNet121 a relatively shallow variant than \\nDenseNet169. DenseNet121 has fewer parameters compared to DenseNet169, which makes it more \\nmemory-efficient and faster to train. DenseNet121 performs well on various image classification tasks but \\nmay not capture as fine -grained features as deeper models. DenseNet169 performs better than \\nDenseNet121, especially when the dataset is larger and more complex. Choosing between DenseNet121 \\nand DenseNet169 for a particular purpose like AMD classification, it is essential to consider the size and \\ncomplexity of the dataset. Since the dataset used in this study was small, DenseNet121 should have been \\nthe model to pick, but we experimented with all DenseNet variants. \\niii. InceptionResnetV2 \\nA powerful convolutional neural network conception that incorporates the Inception and ResNet \\nmodules is termed the InceptionResNetV2 system [44]. It was proposed as an extension to the \\noriginal Inception and ResNet models designed to improve image classification efficient tas ks. \\nThe InceptionResNetV2 model was initialized with pre -trained ImageNet weights, excluding the \\ntop classification layers. The pre -trained layers were frozen to prevent them from being updated \\nduring training. The Adam optimizer was utilized for bettering  the model and the training was \\ndone with an epoch size of 100. \\nD. AMDNet23: \\nAMDNet23 Combining CNNs which are convolutional neural systems and long -term short-term memory \\nnetworks (LSTM) is designated as CNN -LSTM where the strengths of CNNs in image featu re extraction \\nare combined with the temporal modeling capabilities of LSTM networks[45]. Before feeding the images \\nto the model, employed a diverse set of augmentation techniques to enhance the training data. These \\nincluded randomized horizontal and vertical flipping through a probability of fifty percent each and applied \\nrandom brightness adjustments by varying the brightness level within a range of -0.1 to +0.1. To further \\nincrease variation, used random contrast adjustments with factors ranging from 0.8 to 1.2, as well as random \\nsaturation adjustments within the bounds of 0.8 to 1.2. Moreover, introduced random hue adjustments to \\nadd subtle color variations. Lastly, to augment the dataset further, performed translation -based width and \\nheight shifting with a specific range to the input images. The augment strategy emphasizes data diversity \\nand improves the model's broad ability to diagnose unknown data[46]. The model is designed with a depth \\nof 23 layers.\"),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"Input Layer: The input to the AMDNet23 model is a collection of eye images captured from patients. The \\ninput images are represented as a tensor X with dimensions (N, W, H, C), where N corresponds to the eye \\nimage’s number, and W and H represent The width and length of the images(The model received imagery \\nthat measured 256 X 256 in size.) respectively. C denotes the number of color channels in the eye images. \\nThis tensor X is then passed into the model's input layer.     \\nConvolutional Layers (CNN): After the initial input layer, the CNN component of the mod el comprises \\nmultiple convolutional layers[47]. Every individual layer of convolution utilizes a set of adjustable filters \\nto process the input images. The resulting Features of the map from the 𝑖𝑡ℎ convolutional layer are denoted \\nas 𝐹𝑖, with 𝑖 ranging from 1 to 𝑛. The output feature map 𝐹𝑖 can be computed as follows: \\n𝐹𝑖 = 𝐶𝑜𝑛𝑣2𝐷(𝑋,𝑊𝑖)+𝑏𝑖 \\nWhere Conv2D refers to the convolution operation, 𝑊𝑖 represents the tr ainable parameters (weights) \\nspecific to the 𝑖𝑡ℎ convolutional layer, and 𝑏𝑖 represents the corresponding biases associated with that layer. \\nThe output feature maps 𝐹𝑖 have spatial dimensions (W', H') and C' channels. \\nThe model contained six  convolutional blocks. The first four convolutional blocks consisted of 2 \\nconvolutional layers and 1 batch normalization layer each. The filters were 32, 64, 128, and 256 for the first \\nfour blocks, where the kernel size was 3 X 3. The fifth and sixth convo lutional blocks consisted of 3 \\nconvolution layers and 1 batch normalization layer. All the convolution layers of the fifth and sixth blocks \\nconsisted of 512 filters. \\nPooling Layers:  After the layers based on convolution, layers of pooled [48] are frequentl y used to \\ndownsample the feature maps. Let us denote the output feature maps after pooling as 𝑃𝑖, where 𝑖 ranges \\nfrom 1 to 𝑝 (the total number of pooling layers). Each pooling layer performs a downsampling operation \\non the input feature maps. After applying all pooling layers, the resulting feature maps can be denoted as \\n𝑃𝑝 and have spatial dimensions (W'', H'') and C'' channels. The pool size for max-pooling layers was 2 X 2 \\nfor all the convolutional blocks. \\nLSTM Layer: The system conveys the development and significance of networks having long-term short-\\nterm memory (LSTM), an advancement residing in conventional Recurrent neural networks to delve into \\nthe motivation behind LSTM's creation, specifically to address the vanishing gradient problem , which \\nformerly hindered the effective training of RNNs on long sequences [49]. Behind LSTM it introduces \\nmemory cells, enabling the network to retain information over extended periods. The mechanism empowers \\nLSTMs in effectively capturing long-term dependencies within the input data. The cell state adds a long -\\nterm memory to flows the entire sequence [50]. It enables information to be retained or discarded selectively \\nutilizing the input entrance, forget gatekeeper, and output gateway, which constitute th e three main gates. \\nThe LSTM cell computations can be mathematically represented as follows, where 𝑡 denotes the current \\ntime step, 𝑥𝑡 rrepresents what was the input entered at time 𝑡, ℎ𝑡denotes the previous hidden stated, and \\n𝑐𝑡signifies the cell state: \\n \\n \\n \\n𝑖𝑡 = 𝜎(𝑊𝑖[𝑥𝑡,ℎ𝑡−1]+𝑏𝑖)...(1)\"),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='𝐶 𝑡 = 𝑡𝑎𝑛ℎ(𝑊𝑐[𝑥𝑡,ℎ𝑡−1]+𝑏𝑐)...(2) \\n𝐶𝑡 = 𝑓𝑡𝐶𝑡−1 +𝑖𝑡𝐶 𝑡...(3) \\nThe input gate (1) uses a sigmoid function to combine the previous output ℎ𝑡−1and the present time input \\n𝑥𝑡, deciding the proportion of information to be incorporated into the cell state and (2) employes to obtain \\nnew information through the tanh layer to be added into current cell state 𝐶 𝑡. The current cell state 𝐶 𝑡, and \\nlong term information 𝐶𝑡−1 are combination into 𝐶𝑡(3) whereas 𝑤𝑖 determines the sigmoid output and 𝐶 𝑡 \\ndetermines to tanh output. “Forget” gate (4) investigates how much of the previous cell state should be \\nretained and carried over to the next time step by assessing probability where 𝑊𝑓 and 𝑏𝑓 refers to the offset \\nand weight matrix and offset respectively. \\n𝑓𝑡 = 𝜎(𝑊𝑓[𝑥𝑡,ℎ𝑡−1]+𝑏𝑓)...(4) \\nThe output gate of the LSTM investigates by ℎ𝑡−1 and 𝑥𝑡inputs following (4) and (5) passed through the \\nactivation function to determine what portion of information to be appeared from the current LSTM unit at \\ntimestamp t. \\n𝑜𝑡 = 𝜎(𝑊𝑜[𝑥𝑡,ℎ𝑡−1]+𝑏𝑜)...(5) \\nℎ𝑡 = 𝑜𝑡𝑡𝑎𝑛ℎ(𝐶𝑡)...(6)  \\nIn the above equation, 𝑊𝑜refers to the matrices of the output gate and 𝑏𝑜refers LSTM bias respectively.  \\nOutput Layer: The output layer delivers the ultimate prediction regarding the existence or non -existence \\nof AMD in the input eye images. We can represent the input tensor to the output layer as 𝐻𝑜𝑢𝑡, obtained by \\nreshaping 𝐻𝑙𝑠𝑡𝑚 to have dimensions (NT, D). A function of activation throughout softmax is positioned \\nfollowing its dense layer to the output section. The dense layer takes the input 𝐻𝑜𝑢𝑡 and transforms it to \\ngenerate the output tensor Y, which has dimensions (NT, K). Here, K specifies the number of output classes \\nthe model is classifying.'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Figure 7: CNN-LSTM system \\nThe AMDNet23 model (Figure 7) for AMD ocular disease detection leverages the complementary strengths \\nof CNNs in spatial feature extraction and LSTMs in modeling sequential dependencies.  \\nLayer Type Kernel Size Kernel Input Size \\n1 Convolution2D 3 X 3 32 256 X 256 X 3 \\n2 Convolution2D 3 X 3 32 256 X 256 X 32'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='3 Maxpooling2D 2 X 2 - 256 X 256 X 32 \\n4 Convolution2D 3 X 3 64 128 X 128 X 32 \\n5 Convolution2D 3 X 3 64 128 X 128 X 64 \\n6 Maxpooling2D 2 X 2 - 128 X 128 X 64 \\n7 Convolution2D 3 X 3 128 64 X 64 X 64 \\n8 Convolution2D 3 X 3 128 64 X 64 X 128 \\n9 Maxpooling2D 2 X 2 - 64 X 64 X 128 \\n10 Convolution2D 3 X 3 256 32 X 32 X 128 \\n11 Convolution2D 3 X 3 256 32 X 32 X 256 \\n12 Maxpooling2D 2 X 2 - 32 X 32 X 256 \\n13 Convolution2D 3 X 3 512 16 X 16 X 256 \\n14 Convolution2D 3 X 3 512 16 X 16 X 512 \\n15 Convolution2D 3 X 3 512 16 X 16 X 512 \\n16 Maxpooling2D 2 X 2 - 16 X 16 X 512 \\n17 Convolution2D 3 X 3 512 8 X 8 X 512 \\n18 Convolution2D 3 X 3 512 8 X 8 X 512 \\n19 Convolution2D 3 X 3 512 8 X 8 X 512 \\n20 Maxpooling2D - - 8 X 8 X 512 \\n21 LSTM - - 16 X 512'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"22 FC - 64 524352 \\n23 Output - 4 260 \\nIn this research, An innovative and novel technique was devised to automatically detect AMD by \\nleveraging four distinct types of fundus images. This unique architecture synergizes the power of \\nNeural Networks of Convolutional and Long Short -Term Memory. The CNN module is \\nadministered for extracting intricate features from fundus imaging, and the LSTM module serves \\nas the classifier. The proposed AMDNet23 hybrid network for AMD detection consists of 23 \\nlayers: It includes 14 convolutional layers placed and 6 layers used for pooling,one fully \\ninterconnected  layer of (FC), a layer of LS TM and a single output layer with a sense of softmax \\nfunctionality. In our construction, an individual convolution block is made comprising between \\ntwo or three 2 -dimensional CNN's, A layer with a level of pooling and a layer comprising a \\ntwentieth percent dropout rate have of dropouts. Utilizing a convolutional layer with 3x3 kernels \\nand the ReLU function, the feature extraction is carried out efficiently. The input image undergoes \\ndimension reduction using A layer for maximum pooling of 2 × 2 kernels. The  resulting output \\nstructure was discovered (none, 4, 4, 512). the input size inside the layer of LSTM transforms to \\n(16, 512) whenever incorporating the reshaped approach. Combining these two neural network \\narchitectures allows the model to effectively ana lyze eye images, capturing both local spatial \\npatterns and temporal relationships, ultimately enabling accurate AMD diagnosis. The \\nsummarized architecture is presented in Table 2. \\nEvaluation Criteria \\nIn this study, as many as 13 models were experimented an d the evaluation of all those models will be \\npresented in this section of the paper. Considering the following evaluation criteria, the performance, \\nreliability, and clinical relevance of a AMD detection system can be assessed and also can be determined \\nits suitability for assisting medical professionals in accurately detecting and diagnosing AMD. \\nAccuracy: The accuracy of the AMD detection system in correctly classifying images as AMD, diabetes, \\ncataracts is a crucial evaluation criterion. It evaluates the  correctness of the system's detection computed \\noverall. \\n𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = 𝑇𝑃 +𝑇𝑁\\n𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁 \\nPrecision: The proportion of properly identified AMD situations is examined to measure precision out of \\nall predicted AMD cases.  \\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝑇𝑃\\n𝑇𝑃 +𝐹𝑃 \\nSensitivity and Specificity : Sensitivity is typically referred to by the term the true positive rate, which \\ngauges the system to identify AMD cases correctly. True negative rate, which is often referred to as \\nspecificity, assesses its capacity of identifying non-AMD conditions. Both metrics provide insights into the \\nsystem's performance in different classes and help assess its ability to avoid false positives and false \\nnegatives.\"),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦 = 𝑇𝑃\\n𝑇𝑃+𝐹𝑁 \\n𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦 = 𝑇𝑁\\n𝑇𝑁+𝐹𝑃 \\nF1 Score: The F1 score offers a comprehensive measurement that addresses the balance between precision \\nand memory and thus represents a harmonious average of precision and recall. It is advantageous in realities \\nwhereby there occurs a disparity in class or in cases when the costs of false positives and false negatives \\nfluctuate. \\n𝐹1𝑆𝑐𝑜𝑟𝑒 = 𝑇𝑃\\n𝑇𝑃+1\\n2(𝐹𝑃+𝐹𝑁)\\n \\nWhen evaluating model performance, it is crucial to consid er a combination of these evaluation criteria \\naccordance with the precise specifications of the completion and the area of expertise. Selecting appropriate \\nmetrics and interpreting the results will help determine the effectiveness and suitability of the model. Table \\n2 showcases the retained value for these evaluation metrics: \\nModel Accuracy Precision Sensitivity Specificity F1 Score \\nViTB16 95.25% 95.26% 95.25% 98.66% 95.24% \\nMobileViT_XXS 83.00% 82.58% 83.00% 98.28% 82.41% \\nInceptionResNetV2 47.50% 36.26% 47.50% 82.67% 40.50% \\n EfficientNetB7 92.75% 92.94% 92.75% 98.97% 92.62% \\n EfficientNetB6 92.75% 92.74% 92.75% 97.00% 92.69% \\nDenseNet121 82.25% 82.41% 82.25% 95.00% 81.46% \\nDenseNet169 81.25% 81.24% 81.25% 92.74% 80.31% \\nDenseNet201 84.75% 84.45% 84.75% 93.46% 84.52% \\nInceptionV3 72.25% 71.53% 72.25% 90.16% 70.71% \\nMobileNetV2 71.75% 71.68% 71.75% 88.01% 71.19% \\nVGG16 89.75% 89.82% 89.75% 94.68% 89.78%'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"VGG19 89.00% 88.96% 89.00% 95.62% 88.80% \\nResNet50 93.25% 93.37% 93.25% 96.73% 93.17% \\nAMDNet23 96.50% 96.51% 96.50% 99.32% 96.49% \\nIn conclusion, the AMDNet23 model for AMD ocular disease detection demonstrated strong performance \\nin accurately identifying AMD from eye images. Its high accuracy, precision, and recall values, along with \\nthe robust AUC-ROC score, validate its potential as a reliable tool for early detection and intervention. The \\nmodel's efficiency makes it suitable for practical deployment in healthcare settings, contributing to \\nimproved patient care and timely treatment of AMD retinal disease. \\nResult & Discussion: \\nIn this undermentioned portion, the findings of the proposed mechanism along with the comparison with \\nsome cutting-edge studies will be comprised. The collected data were divided into sets for conducting \\ntraining and testing to constr uct and examine the proposed system. This approach was initially trained to \\nleverage 80% of the data and evaluated utilizing 20% of the collected information. To ensure enhanced \\nproductivity several sets of parameters were experimented. The parameter setting that provided us with the \\nmost advantageous outline for the proposed model is given below \\n \\n Batch size 32 \\n Epochs 100  \\n Learning rate 0.001  \\n Decay rate 0.95  \\nDecay step 1 \\n \\nFigure 5 and Figure 6 show both training and test sets of data, the accuracy and loss curves. The graphical \\nrepresentations of epoch versus accuracy and epoch versus losses are valuable insights for monitoring and \\nunderstanding the progress of the proposed method.\"),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"The graph exhibits how the model's accuracy varies while an increasing amount of training epochs raises. \\nThe predictability of the model on either a set of training data or a testing set appears on the axis in the \\nvertical direction, along with the number of epochs denoted throughout the horizontal direct ion. It is \\nobserved from Figure 5 reveals the overall number of epochs rises over training, the model's accuracy \\nelevates as it learns from the training data. At first, the accuracy continues to improve with each epoch, it \\nsuggests that the model can benef it from additional training. Later the accuracy eventually plateaus. This \\nindicates that the model has converged and further training may not significantly improve accuracy.  \\nThe epoch vs loss graph demonstrates the association between the process of train ing number of epochs \\nand the loss or error of the model. The loss of performance is a disparity among the estimated of model \\noutline and the intended outline. The loss level is portrayed through the y-axis, whereas the total amount of \\nepochs is displayed through the x-axis. In Figure 6, the loss is seen initially high as the model makes random \\npredictions. The training messages, the loss decreases, reflecting the model's improved performance and \\nability to make more accurate predictions. \\nFigure X a represents The AMDNet23 model's confusion matrix, resulting in was determined on the basis \\nof the ablation investigation, Adam, and learning rate, experiences the greatest level of accuracy and is \\nconfigured in a great potential manner.\"),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='The row of values reflect s what is actually labeled \\nattached to the images, while the column-specific values \\nreveal the quantities provided the predictive model \\nestimates. The diagonal values indicate correct \\npredictions (TP). However, the model had the great \\nresults for AMD. Acco rding to the confusion matrix, \\nTwo of the AMD pictorials was mistakenly classified as \\ncataract-related and diabetes, whereas 98 among the 100 \\nAMD investigations possessed effectively diagnosed.  \\nNext, 99 among a possible 100 anticipated involving \\ncataracts taught correctly where one as AMD. Of 100 \\ndiabetes images, Seven images had been mistakenly \\nidentified, involving 3 referred to as AMD along with 4 \\nas belonging to the healthy class, exposing of 93 \\ncorrectly assigned.  In closing least, among the 100 \\nnormal images, 96 were perfectly identified, and 1 had \\nbeen misinterpreted as images related to AMD and 3 associated to diabetes. \\n \\n \\n \\n \\n \\n \\n \\n \\nState-of-the-art work comparison \\nTable 3 provides a concisely summarizes the main approaches in the existing literature for diagnosing AMD \\ndisease and our proposed method. These approaches primarily involve conventional methods and deep \\nlearning algorithms, which utilize retinal images for diagnosis. \\n \\nAuthor Year Method No. of images Accuracy \\n \\nTK Yoo et. al. [51] \\n \\n2018 \\n \\nVGG19-RF \\n \\n3000 \\n3- Class \\n95% accuracy \\n \\nHuiying Liu et. al [52] \\n \\n2019 \\n \\nDeepAMD \\n \\n4725 \\n6- Class \\n70% accuracy \\nFelix Grassman et. al. [53]  \\n2020 \\nEnsemble \\nnetworks net \\n \\n3654  \\n13- class \\n63% accuracy'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='N Chea and Y Nam [54] \\n \\n2021 \\nOptimal residual \\ndeep neural \\nnetworks  \\n \\n2335 \\n4- Class \\n85.79% accuracy \\n \\nC Domínguez et. al [55] \\n \\n2023 \\nTransformer-based \\nsystem \\n \\n4896 \\n3-Class \\n82.55% accuracy \\n \\nP Zang et. Al [56] \\n \\n2023 \\nDeep-Learning \\nbased aided \\nsystem \\n \\nNot specified \\n4-Class \\n80% accuracy \\n \\nProposed Method \\n \\n2023 \\n  \\nAMDNet23 \\n \\n2000 \\n4- Class \\n96.5% accuracy \\n \\nThe AMDNet23 model proposed in the study achieves a high accuracy rate of 96.5%, surpassing \\nother state-of-the-art works currently available. As a result, It can be presumed that this proposed \\nmethod is effective for early -stage detection and diagnosis of AMD, and this novel method also \\ndiagnoses Cataracts and diabetic retinopathy utilizing fundus ophthalmology datasets, \\ndemonstrating superior accuracy. \\nComparison of the AMDNet23 model with the transfer Learning models: \\nThe outline demonstrated and effectively potential of hybrid AMDNet23 network for precisely \\ndetecting AMD eye disease from images. The capacity of the model to precisely detect AMD \\ninstances is demonstrated by the excellent precision, accuracy, recall and F1 -score a cquired.  \\nCombination of CNNs and LSTMs allows for the extracting of both spatial and temporal features, \\ncapturing the subtle patterns and changes associated with AMD. Figure 5 exhibits the comparison \\nof performance between the model we proposed and a several pre-trained prepared.'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Conclusion \\nIn essence, this study proposed a AMDNet23 model for detecting and diagnosing AMD disease using \\nseveral image datasets. The model achieved a high accuracy rate of 96.5%, surpassing other state -of-the-\\nart works in the field. Furthermore, when compared with pre -trained models, the novel deep AMDNet23 \\nmethod also showed superior accuracy for AMD detection, and the system is efficient to diagnose cataracts \\nand diabetic retinopathy respectively. In the future, incorpora ting additional modalities or features can \\npotentially enhance the performance of AMD detection models. Combining fundus images with other \\nclinical data, which could include patient demographics, health records, or genetic information, may \\nimprove accuracy and enable a more comprehensive awareness of the disease. In broadly , the findings of \\nthis research clearly demonstrates effectively of the proposed AMDNet23 model in accurately detecting \\nand diagnosing AMD cases. This model holds promise for early detect ion and diagnosis of AMD ocular \\ndisease, which could assist clinicians and aid in timely intervention and treatment for affected individuals. \\n \\n \\nREFERENCES \\n \\n1. Ackland, P., Resnikoff, S., & Bourne, R. (2017). World blindness and visual impairment: \\ndespite many successes, the problem is growing. Community eye health, 30(100), 71.  \\n2. “Vision Impairment and Blindness.” World Health Organization, 13 Oct. 2022, \\nwww.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment.  \\n3. “Vision Atlas.” The International Agency for the Prevention of Blindness, 4 Jan. 2023, \\nwww.iapb.org/learn/vision-atlas/.  \\n4. “Common Eye Disorders and Diseases.” Centers for Disease Control and Prevention, 19 \\nDec. 2022, www.cdc.gov/visionhealth/basics/ced/index.html.'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='5. Sarki, R., Ahmed, K., Wang, H., & Zhang, Y. (2020). Automatic detection of diabetic eye \\ndisease through deep learning using fundus images: a survey. IEEE access, 8, 151133-\\n151149.  \\n6. Kumar, S. M., & Gunasundari, R. (2023). Computational intelligence in eye disease \\ndiagnosis: a comparative study. Medical & Biological Engineering & Computing, 61(3), \\n593-615.  \\n7. Iqbal, Shahzaib, et al. “Recent Trends and Advances in Fundus Image Analysis: A \\nReview.” Computers in Biology and Medicine, vol. 151, 2022, p. 106277, \\nhttps://doi.org/10.1016/j.compbiomed.2022.106277.  \\n8. Tan, J. H., Bhandary, S. V., Sivaprasad, S., Hagiwara, Y., Bagchi, A., Raghavendra, \\nU., ... & Acharya, U. R. (2018). Age-related macular degeneration detection using deep \\nconvolutional neural network. Future Generation Computer Systems, 87, 127-135.  \\n9. Sogawa, T., Tabuchi, H., Nagasato, D., Masumoto, H., Ikuno, Y., Ohsugi, H., ... & \\nMitamura, Y. (2020). Accuracy of a deep convolutional neural network in the detection of \\nmyopic macular diseases using swept-source optical coherence tomography. Plos one, \\n15(4), e0227240.  \\n10. Serener, A., & Serte, S. (2019, April). Dry and wet age-related macular degeneration \\nclassification using oct images and deep learning. In 2019 Scientific meeting on \\nelectrical-electronics & biomedical engineering and computer science (EBBT) (pp. 1-4). \\nIEEE. \\n11. Yim, J., Chopra, R., Spitz, T., Winkens, J., Obika, A., Kelly, C., ... & De Fauw, J. (2020). \\nPredicting conversion to wet age-related macular degeneration using deep learning. \\nNature Medicine, 26(6), 892-899. \\n12. Schmidt-Erfurth, U., Waldstein, S. M., Klimscha, S., Sadeghipour, A., Hu, X., Gerendas, \\nB. S., ... & Bogunović, H. (2018). Prediction of individual disease conversion in early \\nAMD using artificial intelligence. Investigative ophthalmology & visual science, 59(8), \\n3199-3208.  \\n13. Schmidt-Erfurth, U., Bogunovic, H., Sadeghipour, A., Schlegl, T., Langs, G., Gerendas, \\nB. S., ... & Waldstein, S. M. (2018). Machine learning to analyze the prognostic value of \\ncurrent imaging biomarkers in neovascular age-related macular degeneration. \\nOphthalmology Retina, 2(1), 24-30.  \\n14. Bogunović, H., Montuoro, A., Baratsits, M., Karantonis, M. G., Waldstein, S. M., \\nSchlanitz, F., & Schmidt-Erfurth, U. (2017). Machine learning of the progression of \\nintermediate age-related macular degeneration based on OCT imaging. Investigative \\nophthalmology & visual science, 58(6), BIO141-BIO150.  \\n15. Jaiswal, A. K., Tiwari, P., Kumar, S., Al-Rakhami, M. S., Alrashoud, M., & Ghoneim, A. \\n(2021). Deep learning-based smart IoT health system for blindness detection using \\nretina images. IEEE Access, 9, 70606-70615.  \\n16. Wang, Ying, et al. “Deep Back Propagation–Long Short-Term Memory Network Based \\nUpper-Limb SEMG Signal Classification for Automated Rehabilitation.” Biocybernetics'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='and Biomedical Engineering, vol. 40, no. 3, 2020, pp. 987–1001, \\nhttps://doi.org/10.1016/j.bbe.2020.05.003.  \\n17. Sahoo, M., Mitra, M., & Pal, S. (2023). Improved detection of dry age-related macular \\ndegeneration from optical coherence tomography images using adaptive window based \\nfeature extraction and weighted ensemble based classification approach. \\nPhotodiagnosis and Photodynamic Therapy, 42, 103629.  \\n18. Muthukannan, P. (2022). Optimized convolution neural network based multiple eye \\ndisease detection. Computers in Biology and Medicine, 146, 105648.  \\n19. Serener, A., & Serte, S. (2019, April). Dry and wet age-related macular degeneration \\nclassification using oct images and deep learning. In 2019 Scientific meeting on \\nelectrical-electronics & biomedical engineering and computer science (EBBT) (pp. 1-4). \\nIEEE.  \\n20. Kumar, Y. and Gupta, S., 2023. Deep transfer learning approaches to predict glaucoma, \\ncataract, choroidal neovascularization, diabetic macular edema, drusen and healthy \\neyes: an experimental review. Archives of Computational Methods in Engineering, 30(1), \\npp.521-541.  \\n21. Paradisa, Radifa Hilya, et al. \"Deep feature vectors concatenation for eye disease \\ndetection using fundus image.\" Electronics 11.1 (2022): 23.  \\n22. Faizal, Sahil, et al. \"Automated cataract disease detection on anterior segment eye \\nimages using adaptive thresholding and fine tuned inception-v3 model.\" Biomedical \\nSignal Processing and Control 82 (2023): 104550.  \\n23. Pahuja, Rahul, et al. \"A Dynamic Approach of Eye Disease Classification Using Deep \\nLearning and Machine Learning Model.\" Proceedings of Data Analytics and \\nManagement: ICDAM 2021, Volume 1. Springer Singapore, 2022.  \\n24. Chaudhary, R., & Kumar, A. (2022, June). Cataract Detection using Deep Learning \\nModel on Digital Camera Images. In 2022 IEEE International Conference on Cybernetics \\nand Computational Intelligence (CyberneticsCom) (pp. 489-493). IEEE  \\n25. Khan, Md Sajjad Mahmud, et al. \"Cataract detection using convolutional neural network \\nwith VGG-19 model.\" 2021 IEEE World AI IoT Congress (AIIoT). IEEE, 2021.  \\n26. Mondal, Sambit S., et al. \"EDLDR: An Ensemble Deep Learning Technique for Detection \\nand Classification of Diabetic Retinopathy.\" Diagnostics 13.1 (2023): 124.  \\n27. Babenko, Boris, et al. \"Detection of signs of disease in external photographs of the eyes \\nvia deep learning.\" Nature Biomedical Engineering (2022): 1-14  \\n28. Saranya, P., R. Pranati, and Sneha Shruti Patro. \"Detection and classification of red \\nlesions from retinal images for diabetic retinopathy detection using deep learning \\nmodels.\" Multimedia Tools and Applications (2023): 1-21.'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='29. Alyoubi, W. L., Abulkhair, M. F., & Shalash, W. M. (2021). Diabetic retinopathy fundus \\nimage classification and lesions localization system using deep learning. Sensors, \\n21(11), 3704.  \\n30. Acharya, U. R., Mookiah, M. R. K., Koh, J. E., Tan, J. H., Noronha, K., Bhandary, S. \\nV., ... & Laude, A. (2016).Novel risk index for the identification of age-related macular \\ndegeneration using radon transform and DWT features. Computers in biology and \\nmedicine, 73, 131-140.  \\n31. Zaki, W. M. D. W., Mutalib, H. A., Ramlan, L. A., Hussain, A., & Mustapha, A. (2022). \\nTowards a Connected Mobile Cataract Screening System: A Future Approach. Journal \\nof Imaging, 8(2).  \\n32. Larxel. “Ocular Disease Recognition.” Kaggle, 24 Sept. 2020, \\nkaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k. \\n33. Dugas, Emma, et al. “Diabetic Retinopathy Detection.” Kaggle, 2015, \\nkaggle.com/competitions/diabetic-retinopathy-detection. \\n34. Saeed, and Rimsha. “Fundus-Dataset.Zip.” Figshare, 11 Nov. 2021, \\ndoi.org/10.6084/m9.figshare.16986166.v1.  \\n35. Awsaf. “RFMID Train Dataset.” Kaggle, 26 Nov. 2020, \\nwww.kaggle.com/datasets/awsaf49/rfmid-train-dataset.  \\n36. “Eyecharity.Com Is for Sale.” HugeDomains.Com, www.eyecharity.com/aria_online. \\nAccessed 19 July 2023.  \\n37. Doddi, Guna Venkat. “Eye_diseases_classification.” Kaggle, 28 Aug. 2022, \\nwww.kaggle.com/datasets/gunavenkatdoddi/eye-diseases-classification.  \\n38. Watson, Debbie. Contouring: a guide to the analysis and display of spatial data. \\nElsevier, 2013.  \\n39. Rahman, Tawsifur, et al. \"Exploring the effect of image enhancement techniques on \\nCOVID-19 detection using chest X-ray images.\" Computers in biology and medicine 132 \\n(2021): 104319.  \\n40. Sahu, S., Singh, A. K., Ghrera, S. P., & Elhoseny, M. (2019). An approach for de-noising \\nand contrast enhancement of retinal fundus image using CLAHE. Optics & Laser \\nTechnology, 110, 87-98.  \\n41. Sara, Umme, Morium Akter, and Mohammad Shorif Uddin. \"Image quality assessment \\nthrough FSIM, SSIM, MSE and PSNR—a comparative study.\" Journal of Computer and \\nCommunications 7.3 (2019): 8-18.  \\n42. Saranya, P., R. Pranati, and Sneha Shruti Patro. \"Detection and classification of red \\nlesions from retinal images for diabetic retinopathy detection using deep learning \\nmodels.\" Multimedia Tools and Applications (2023): 1-21.'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='43. Team, Keras. “Keras Documentation: DenseNet.” Keras, \\nkeras.io/api/applications/densenet/. Accessed 10 June 2023.  \\n44. Dominic, Nicholas, et al. \"Transfer learning using inception-ResNet-v2 model to the \\naugmented neuroimages data for autism spectrum disorder classification.\" Commun. \\nMath. Biol. Neurosci. 2021 (2021): Article-ID.  \\n45. Tsiouris, Κostas Μ., et al. \"A long short-term memory deep learning network for the \\nprediction of epileptic seizures using EEG signals.\" Computers in biology and medicine \\n99 (2018): 24-37.  \\n46. Xu, M., Yoon, S., Fuentes, A., & Park, D. S. (2023). A comprehensive survey of image \\naugmentation techniques for deep learning. Pattern Recognition, 109347.  \\n47. Hasan AM, Jalab HA, Meziane F, Kahtan H, Al-Ahmad AS. Combining deep and \\nhandcrafted image features for MRI brain scan classification. IEEE Access 2019;7: \\n79959–67.  \\n48. Lundervold AS, Lundervold A. An overview of deep learning in medical imaging focusing \\non MRI. Z Med Phys 2019;29:102–27. https://doi.org/10.1016/j. Zemedi.2018.11.002.  \\n49. Hochreiter, S. (1998). The vanishing gradient problem during learning recurrent neural \\nnets and problem solutions. International Journal of Uncertainty, Fuzziness and \\nKnowledge-Based Systems, 6(02), 107-116.  \\n50. Chen, G. (2016). A gentle tutorial of recurrent neural network with error \\nbackpropagation. arXiv preprint arXiv:1610.02583.  \\n51. Yoo, T. K., Choi, J. Y., Seo, J. G., Ramasubramanian, B., Selvaperumal, S., & Kim, D. \\nW. (2019). The possibility of the combination of OCT and fundus images for improving \\nthe diagnostic accuracy of deep learning for age-related macular degeneration: a \\npreliminary experiment. Medical & biological engineering & computing, 57, 677-687.  \\n52. Liu, H., Wong, D. W., Fu, H., Xu, Y., & Liu, J. (2019). DeepAMD: detect early age-related \\nmacular degeneration by applying deep learning in a multiple instance learning \\nframework. In Computer Vision–ACCV 2018: 14th Asian Conference on Computer \\nVision, Perth, Australia, December 2–6, 2018, Revised Selected Papers, Part V 14 (pp. \\n625-640). Springer International Publishing.  \\n53. Grassmann, F., Mengelkamp, J., Brandl, C., Harsch, S., Zimmermann, M. E., Linkohr, \\nB., ... & Weber, B. H. (2018). A deep learning algorithm for prediction of age-related eye \\ndisease study severity scale for age-related macular degeneration from color fundus \\nphotography. Ophthalmology, 125(9), 1410-1420.  \\n54. Chea, N., & Nam, Y. (2021). Classification of Fundus Images Based on Deep Learning \\nfor Detecting Eye Diseases. Computers, Materials & Continua, 67(1).  \\n55. Domínguez, C., Heras, J., Mata, E., Pascual, V., Royo, D., & Zapata, M. Á. (2023). \\nBinary and multi-class automated detection of age-related macular degeneration using'),\n",
       " Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='convolutional-and transformer-based architectures. Computer Methods and Programs in \\nBiomedicine, 229, 107302.  \\n56. Zang, P., Hormel, T. T., Hwang, T. S., Bailey, S. T., Huang, D., & Jia, Y. (2023). Deep-\\nLearning–Aided Diagnosis of Diabetic Retinopathy, Age-Related Macular Degeneration, and \\nGlaucoma Based on Structural and Angiographic OCT. Ophthalmology Science, 3(1), 100245.  \\n57. Da Yan, Shengbin Wu, Mirza Tanzim Sami, Abdullateef Almudaifer, Zhe Jiang, Haiquan Chen, \\nD. Rangaprakash, Gopikrishna Deshpande, Yueen Ma, “Improving Brain Dysfunction Prediction \\nby GAN: A Functional-Connectivity Generator Approach”, IEEE International Conference on \\nBig Data (Big Data), Orlando, FL, USA, 2021. \\n58. Mirza Tanzim Sami, Da Yan, Huang Huang, Xinyu Liang, Guimu Guo, Zhe Jiang, “Drone-Based \\nTower Survey by Multi-Task Learning”, IEEE International Conference on Big Data (Big Data), \\nOrlando, FL, USA, 2021.'),\n",
       " Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='Shakhawat Hossain\\n♂phone+880 1778198423 /envel⌢peshakhawat15-14283@diu.edu.bd /linkedinshanin-hossain /githubShakhawatShanin\\nEducation\\nDaffodil International University Dhaka, Bangladesh\\nB.Sc. in Computer Science and Engineering CGPA: 3.60/4.00 (Jan. 2020 – Dec 2023)\\nRelevant Coursework:Data Structure, Data Mining and Machine Learning, Big Data and IoT, Artificial Intelligence,\\nNumerical Methods, Algorithms, Object Oriented Programming, Programming and Problem Solving\\nUndergraduate Major:Artificial Intelligence\\nUndergraduate Thesis: Graph-Based Automatic Breast Tumor Classification Through Ultrasound Imaging Using Ra-\\ndiomics Features.\\nResearch Interests\\n• Machine Learning\\n• Deep Learning\\n• Artificial Intelligence\\n• Computer Vision\\n• Health Informatics\\n• Medical Imaging\\n• Image Preprocessing\\n• Pattern Recognition\\n• Generative Artificial Intelligence\\nExperience\\nUniversity of Queensland Brisbane, Australia\\nResearch Assistant May 2024 – Present (Remote)\\n• At the AI and Digital Health Technology Lab, I developed a cutting-edge brain glioma grading system utilizing hybrid\\ngraph networks. This advanced model, trained on radiomic biomarkers extracted from 3D MRI scans, employs LIME to\\nensure accurate and interpretable grading, delivering clinically reliable outcomes.\\nHawkEyes Digital Monitoring Limited Dhaka, Bangladesh\\nAI Engineer Jan 2024 – Present\\n• Data Handling: Performed data validation and cleaning to ensure dataset reliability, and maintained consistent\\nlabeling quality across projects for developing robust AI.\\n• Computer Vision: I led computer vision projects focused on advanced image recognition, classification, segmentation,\\nand object detection. By leveraging algorithms like YOLO, UNet, custom CNN-LSTM, and OpenCV.\\n• Generative AI: Developed Lip Sync video model, AI-powered HDML employee information system, and interactive\\nchatbot designed for automated responses.\\n• OCR: Designed and implemented OCR pipelines to extract handwritten diverse text from scanned memo images.\\nProjects\\nBAT Bangladesh | Instance Segmentation, OOB Detection, Warp Perspective, Sequence Generation, Sorting\\n• Led the analysis of cigarette displays for regional campaigns using image processing, developing sequence analysis\\nalgorithms to ensure compliance with merchandising standards. This work improved the accuracy of audits and\\noptimized campaign management for BAT.\\nUnilever Bangladesh | Python, YoloV8, FastAPI, Asynchronous programming, Logging\\n• Implemented an AI-based trade merchandiser platform for Unilever Bangladesh with an accuracy rate of 98.00%.\\n• The system features a user-friendly dashboard for seamless management of trade merchandising activities, ensuring\\nefficient inventory control, meticulous task execution, and centralized digital recording, analysis, and reporting.\\nCardioCare | ML, FlaskAPI, HTML, Tailwind\\n• Developed a heart failure prediction model using machine learning, served via a Flask API with a responsive HTML and\\nTailwind CSS front-end.\\nChessCrack | OpenCV, UNet, YoloV8, Stockfish, NumPy, JS\\n• This project develops an intelligent system that automates the analysis of a physical chessboard image to predict the best\\nmove. It uses a YOLO-based model to detect and classify chess pieces, converts the board state into a FEN string, and\\nthen passes it to the Stockfish engine to determine the optimal move. The system is integrated into a web application via\\na Flask API, allowing users to upload a chessboard image and receive a move recommendation.\\nOfficeVision | FaceRecognition, Uvicorn, Llama, Langchain, Streamlit, Huggingface\\n• Implemented an advanced image recognition solution to accurately identify employees from images, enabling efficient and\\nautomated retrieval of employee details such as roles, contact information, and department.'),\n",
       " Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='Technical Skills\\nLanguages: Python, C++, HTML/CSS, MySQL\\nData Analysis and Visualization : NumPy, Pandas, Matplotlib, Seaborn\\nAssociated Frameworks: TensorFlow, PyTorch, Scikit-learn, Keras, OpenCV, Transfer Learning, Hugging Face\\nTools: Git, GitHub, Jupyter Notebook, Visual Studio Code, Latex, Colab, Roboflow\\nDevelopment Tools: FastAPI, FlaskAPI, RestAPI, MLOps, LLM, NLP\\nPublications\\n1. Md. Aiyub Ali, Md. Shakhawat Hossain , Md.Kawsar Hossain, Subhadra Soumi Sikder, Sharun Akter\\nKhushbu, Mirajul Islam, ”AMDNet23: Hybrid CNN-LSTM Deep Learning Approach with Enhanced\\nPreprocessing for Age-Related Macular Degeneration (AMD) Detection”, Intelligent Systems with\\nApplications journal, Elsevier. https://doi.org/10.1016/j.iswa.2024.200334\\n2. Md. Aiyub Ali, Md Shakhawat Hossain, Taslima Ferdaus Shuva, Muhammad Ali Abdullah Almoyad,\\nNabil Anan Orka, Md. Tanvir Rahman, Risala Tasin Khan, M. Shamim Kaiser, and Mohammad Ali Moni.\\n“RGNN3D: A Hybrid Radiomic Graph Neural Network for 3D MRI Glioma Grading” IEEE Transactions on\\nBiomedical Engineering. [In Review]\\nReferences\\nDr. Mohammad Ali Moni, PhD (Cambridge)\\nProfessor,\\nHead of the Group, AI and Digital Health Technology\\nFaculty of Health and Behavioural Science,\\nThe University of Queensland, St Lucia, QLD 4072, Australia\\nEmail: m.moni@uq.edu.au'),\n",
       " Document(metadata={'source': 'Portfolio/data/Job.pdf'}, page_content='SHAKHAWAT HOSSAIN\\nAI Engineer\\n/ne+8801778198423\\n shanin-hossain\\n/gtbShakhawatShanin\\n shaninhossain2@gmail.com\\nSUMMARY\\nI am an AI Engineer at HawkEyes Digital Monitoring Lim-\\nited, specializing in optimizing Computer Vision, NLP,\\nOCR, and AI models. I ensure top-notch quality and\\nseamlessly deploy solutions on cloud platforms and ap-\\nplications. Passionate about leveraging cutting-edge\\ntechnology to solve real-world problems, I actively learn\\nnew technologies and coding practices to push the\\nboundaries of AI innovation. I have successfully solved\\nover 300+ programming challenges on platforms such\\nas Codeforces, URI, and DIU Bluesheet.\\nSKILLS\\nLanguages: C, C++, Python, Java, MySQL\\nData Analysis: NumPy, Pandas, Matplotlib, Seaborn\\nFrameworks: TensorFlow, PyTorch, Scikit-learn, Keras,\\nOpenCV, Hugging Face\\nTools: Git, Jupyter, VS Code, LaTeX, Roboflow,\\nColab, MS PowerPoint, Word\\nWeb Tools: FastAPI, Flask, RestAPI, ReactJS, Tail-\\nwind\\nEDUCATION\\n01/2020 – 12/2023 B.Sc. in Computer Science and Engineering Daffodil International University (DIU)\\nCGPA: 3.60/4.00\\nMajor: Artificial Intelligence\\nThesis: Graph-Based Breast Tumor Classification Through Ultrasound Imaging Using Radiomics Features.\\nEXPERIENCE\\n02/2024 – Present AI Engineer HawkEyes Digital Monitoring Limited\\nPrepared datasets, trained and fine-tuned models, and optimized accuracy for real-world AI applications.\\n05/2024 – Present Research Assistant (Remote) University of Queensland\\nDeveloped a hybrid graph-based brain glioma grading system using 3D MRI datasets.\\n01/2024 – 02/2024 Junior Front-End Developer M4yours Dev\\nDesigned a responsive news portal template for publishers and bloggers.\\n01/2023 – 12/2023 Research Lab Member HIRL Lab (DIU)\\nWorked on medical image disease detection, sentiment analysis, and integrated computer vision with\\nadvanced ML algorithms.\\nPROJECTS\\nBAT Bangladesh YOLO Segmentation, Detection, Warp Perspective, Sequence Generation, LLAMA2, Langchain\\n• BAT AI-Based Cigarette Brand Detection and Competitor Analysis System:\\nBuilt an object detection system using YOLO for cigarette brand identification, sequence validation,\\nand competitor analysis.\\n• Chatbot Development:\\nDeveloped an NLP chatbot for dynamic conversations across multiple platforms, handling queries\\nand providing real-time customer support with personalized responses.'),\n",
       " Document(metadata={'source': 'Portfolio/data/Job.pdf'}, page_content='Unilever\\nBangladesh\\nPython, NL TK, FastAPI, Pytesseract, Asynchronous Programming\\n• OCR Based Billing System for Laver Bazar:\\nDeveloped an OCR application to capture invoice images and extract item names, quantities, and\\nprices. Automated data digitization and storage, enhancing inventory and financial management by\\nreducing errors.\\n• Voice Recognition System for Word Detection and Counting:\\nDeveloped a voice recognition application that processes audio input and identifies the frequency\\nof specific words or phrases.\\nGazipur Police ML, MTCNN, Keras FaceNet, OpenCV, NumPy\\n• Face Recognition System for Gazipur Metropolitan Police (GMP):\\nDesigned a real-time facial recognition system that enables secure access control, user verification,\\nand efficient recognition history management.\\nACHIVEMENT\\n2025 Best Performing Team (AI Team) HawkEyes Digital Monitoring Limited\\nPUBLICATIONS\\n• Md. Aiyub Ali, Md. Shakhawat Hossain , Md.Kawsar Hossain, Subhadra Soumi Sikder, Sharun Akter Khushbu, Mirajul\\nIslam, \"AMDNet23: Hybrid CNN-LSTM Deep Learning Approach with Enhanced Preprocessing for Age-Related Mac-\\nular Degeneration (AMD) Detection\", Intelligent Systems with Applications journal, Elsevier.\\nhttps:/ /doi.org/10.1016/j.iswa.2024.200334\\n• Md. Aiyub Ali, Md Shakhawat Hossain , Taslima Ferdaus Shuva, Muhammad Ali Abdullah Almoyad, Nabil Anan Orka,\\nMd. Tanvir Rahman, Risala Tasin Khan, M. Shamim Kaiser, and Mohammad Ali Moni. “RGNN3D: A Hybrid Radiomic\\nGraph Neural Network for 3D MRI Glioma Grading” Knowledge-Based Systems. [In Review]\\nREFERENCES\\n• Dr. Mohammad Ali Moni, PhD (Cambridge): Professor and Head, AI and Digital Health Technology, University of\\nQueensland, Australia\\nEmail: m.moni@uq.edu.au'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='RGNN3D: A Hybrid Radiomic Graph Neural Network for 3D\\nMRI Glioma Grading\\nMd. Aiyub Ali a,b, Md Shakhawat Hossaina,b, Taslima Ferdaus Shuvab, Muhammad Ali Abdullah\\nAlmoyadc, Nabil Anan Orkaa, Risala Tasin Khane, M. Shamim Kaiser e, Md. Tanvir Rahman a,d,∗,\\nMohammad Ali Monia,b,f,g,h,∗∗\\naSchool of Health and Rehabilitation Sciences, The University of Queensland, St\\nLucia, Brisbane, 4072, Queensland, Australia\\nbDepartment of Computer Science and Engineering, Daffodil International University, Daffodil Smart\\nCity, Savar, 1341, Dhaka, Bangladesh\\ncDepartment of Basic Medical Sciences, College of Applied Medical Sciences, King Khalid\\nUniversity, Guraiger, 62521, Abha, Saudi Arabia\\ndDepartment of Information and Communication Technology, Mawlana Bhashani Science and Technology\\nUniversity, Santosh, Tangail, 1902, Dhaka, Bangladesh\\neInstitute of Information Technology, Jahangirnagar University, Savar, 1342, Dhaka, Bangladesh\\nfSchool of Information Technology, Washington University of Science and Technology, Alexandria, 22314, Virginia, USA\\ngAI and Cyber Futures Institute, Charles Sturt University, Bathurst, 2795, New South Wales, Australia\\nhRural Health Research Institute, Charles Sturt University, Orange, 2800, New South Wales, Australia\\nAbstract\\nThe diagnosis of glioma, a complex and often deadly brain tumor, involves extensive medical examinations.\\nStill, accurately grading and classifying gliomas is difficult, as different areas within the same tumor can\\nexhibit varying characteristics. The integration of radiomics, a clinically relevant feature extraction method,\\nwith machine learning (ML) is becoming increasingly popular in addressing this issue, but several research\\ngaps persist. To this end, this study proposes a novel deep neural network, RGNN3D, that combines Graph\\nNeural Networks with LSTM layers to precisely grade gliomas in 3D magnetic resonance imaging (MRI)\\ndata. To train our proposed model, we meticulously extracted 112 radiomic biomarkers. Utilizing the\\nbiomarkers, RGNN3D constructs a graph, channels essential information within layers, and preserves only\\npertinent information through its integrated memory cells. The proposed framework attained an accuracy\\nof 98.58%, aligning with the performance of previous state-of-the-art architectures and surpassing prior\\nradiomic-based ML models. We further employed an explainable AI approach (LIME) to highlight the most\\nsignificant features, assisting radiologists in making more informed decisions. In short, RGNN3D offers a\\nreliable and robust computer-aided solution for potential clinical application in the automated identification\\nof gliomas.\\nKeywords: Radiomics, Biomarkers, Graph Neural Networks, Glioma Grading.\\n1. Introduction1\\nGliomas, which originate from glial cells, are the most prevalent primary intracranial tumors in adults2\\n[1]. Gliomas account for approximately 74.6% of all malignant brain tumors [2]. Patients with low-grade3\\n∗Corresponding author\\n∗∗Corresponding author\\nEmail addresses: aiyubali15-13456@diu.edu.bd (Md. Aiyub Ali), shakhawat15-14283@diu.edu.bd (Md Shakhawat\\nHossain), shuva.cse@diu.edu.bd (Taslima Ferdaus Shuva), maabdulllah@kku.edu.sa (Muhammad Ali Abdullah Almoyad),\\nn.orka@uq.edu.au (Nabil Anan Orka), risala@juniv.edu (Risala Tasin Khan), mskaiser@juniv.edu (M. Shamim Kaiser),\\ntanvirrahman@mbstu.ac.bd (Md. Tanvir Rahman ), m.moni@uq.edu.au (Mohammad Ali Moni )\\nPreprint submitted to Knowledge-Based Systems September 25, 2025'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='gliomas (LGG, grades I and II) generally have a survival rate averaging seven years [3]. In stark contrast,4\\nonly 3–5% of patients with glioblastoma (GBM, grade IV) survive beyond five years, with a median survival5\\ntime of approximately 12 months [4]. Intraoperative grading of gliomas, especially distinguishing between6\\nGBM and LGG, is crucial for making informed diagnostic decisions in clinical practice. To this end, magnetic7\\nresonance imaging (MRI) is an essential technique for screening, treatment planning, and assessing tumor8\\nresponse to therapy because it assists in analyzing the phenotypic and structural differences of gliomas9\\n[5]. In the same vein, radiomic features have shown a lot of promise for distinguishing between different10\\ngrades of glioma. Radiomics is an emerging field that extracts high-throughput quantitative features from11\\nmedical images, particularly MRI [6]. Radiomic analysis involves separating the tumor area from the rest of12\\nthe image and extracting clinically useful information about its shape, appearance, size, intensity, location,13\\nand texture [7]. For example, high heterogeneity in intensity and texture features can sometimes indicate14\\nGBM [8, 9]. Although the combination of MRI and radiomic analysis offers vital clinical information15\\nregarding gliomas, identifying patterns among the hundreds of variables and understanding how each one16\\naffects glioma grading is challenging. To this end, machine learning (ML) algorithms emerge as a viable17\\nsolution. For instance, earlier studies used various ML classifiers such as logistic regression (LR) [10],18\\nsupport vector machines (SVM) [11, 12], random forest (RF) [2, 13, 14], LASSO [15], and multi-layer19\\nperceptron [12, 16]. These models performed comparably to state-of-the-art convolutional neural networks,20\\nshowing the promise of radiomic-based automated glioma grading. Han Li et al. [17] introduced a transfer21\\nlearning-based optimizer (MSAS-DMOA) to improve adaptability in dynamic tasks. Peishu Wu et al. [18]22\\nproposed GLA-TD, a CNN-transformer model using attention and tensor decomposition for efficient medical23\\nimage analysis. These works align with our RGNN3D approach in enhancing learning and interpretability.24\\nHowever, notable research gaps still persist. First of all, despite the availability of numerous types of radiomic25\\nfeatures, prior research has focused on distinct radiomic feature types. For example, some studies used only26\\ntexture-based features [19, 20, 21], while others relied on wavelet-based features [22, 11]. To the best of27\\nour knowledge, no studies holistically explored radiomics, i.e., modeling multiple classes of radiomic features28\\nsimultaneously. Second, existing studies lack interpretability. While often accurate, black-box models do not29\\nfoster trust between clinicians and these models because the variables critical to the decision-making process30\\nremain unclear. Finally, no framework has yet outperformed current state-of-the-art classifiers regardless of31\\nearlier studies’ comparable efficacy. Given the aforementioned research gaps, we propose a novel framework,32\\nRGNN3D, which comprises graph convolutional networks (GCNs) and long short-term memory (LSTM)33\\ncells to form a hybrid classifier. The advantages of the proposed hybrid model are twofold. The primary34\\nbenefit of GCNs, or graph neural networks in general, is the ability to embed complex relational data into35\\na graph and pass only the relevant information to the subsequent layers [23]. In addition, LSTM layers use36\\nmemory cells to retain pertinent input data and avoid the vanishing gradient problem [24]. We train our37\\nproposed model on 112 clinically significant radiomic biomarkers extracted from 3D MRI scans and later38\\nemploy local interpretable model-agnostic explanations (LIME) [25]. LIME quantitatively explains which39\\nradiomic features have the most significant impact on grading LGG and GBM, ensuring utmost reliability.40\\nThe key contributions of this study are summarized as follows:41\\n• Introduced RGNN3D, a novel hybrid method that combines LSTMs and GCNs, leading to highly42\\naccurate glioma grading from complex, multimodal 3D MRI scans.43\\n• Demonstrated that GCNs captured 112 significant radiomic biomarkers, while the LSTMs serve long-44\\nrange dependencies and alleviate gradient vanishing by gated memory mechanisms.45\\n• To enhance model transparency, we incorporate LIME-based interpretability analysis, which identifies46\\nand ranks the most influential features contributing to the grading outcomes.47\\n• Identified the top 10 radiomic biomarkers that play a significant role in terms of glioma grading, which48\\nenriches the clinical trust.49\\n2'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='3 Features Integration 2 Radiomic Based Feature Extraction\\nWavelet \\nFilter\\nLoG \\nFilter\\nLBP \\nFilter\\nImage \\nIntensity\\nShape-based \\n(n=26)\\nGLDM \\n(n=14)\\nNGTDM \\n(n=5)\\nGLSZM \\n(n=16)\\nGLRLM \\n(n=16)\\nGLCM \\n(n=24)\\nShape \\nbased \\nfeatures\\nHistogram \\ndescriptors \\nfeatures\\nTexture \\nfeatures\\nTotal \\nExtracted \\nFeatures: \\n112\\nGenerated \\nnodes \\nof \\n704 \\nx \\n112 \\nEdge \\ntable \\n(Adj. \\nmatrix) \\nof\\n \\n64 \\nx \\n64 \\nProposed \\nRGNN3D\\n \\nmodel\\n4\\nHybrid \\nModel \\nDevelopment\\nCombines \\nGraph \\nand \\nSequential \\nLearning\\nEnd-to-End \\nTrainable\\nFlexible \\nto \\nDifferent \\nGraph \\nStructures\\nEvaluation \\n& \\nAnalysis\\n5\\n 1 3D MRI & ROI\\nT1\\nT2\\nT1CE\\nFLAIR\\nFirst \\norder \\nstatistics \\n(n=19)\\nRadiomic \\nBiomarker \\nTables\\nT1\\nT2\\nT1CE\\nFLAIR\\nComparison \\nwith \\nstate-of-the-art \\nmethods\\n     \\nExperimented \\non \\nfour \\nindividual \\nstages\\nT1, \\nT1CE, \\nT2, \\nFLAIR\\nAccuracy, \\nSpecificity, \\nSensitivity,\\nAUC,F1-Score\\nConfusion \\nmatrix\\nROC \\ncurve\\nModel \\ninterpretation \\nanalysis\\nFigure 1: Comprehensive workflow of the proposed radiomic-based hybrid RGNN3D Model for glioma grading.\\n2. Materials and Methods50\\nThe overall methodology adopted in this study has five key stages as shown in Fig. 1: (i) 3D MRI &51\\nROI involves acquiring 3D MRI scans and identifying regions of interest (ROI) for GBM and LGG, (ii)52\\nRadiomic Based Feature Extraction employs the PyRadiomics toolbox to extract features from the ROIs53\\nusing various filters, (iii) Features Integration incorporates shape-based features, histogram descriptors,54\\nand texture features to create a comprehensive feature set, (iv) Hybrid Model Development proposes the55\\nRGNN3D model that combines graph neural networks and LSTM-based sequential learning, ensuring end-56\\nto-end trainability and interpretability, and (v) Evaluation & Analysis assesses the performance of the57\\nproposed framework with interpretability. In the following subsections, we delve into the specifics of every58\\n3'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='a b\\nGBM\\nLGG\\nFLAIR\\nT1\\nT1CE\\nT2\\nROI\\nBraTS \\n2019\\nBraTS \\n2020\\nCombined \\nBraTS\\nGBM\\nLGG\\nTOTAL\\n552\\n152\\n704\\n259\\n76\\n335\\n76\\n293\\n369\\nFigure 2: Dataset used in this study: (a) Sample images from each category; (b) Integration of BRATS 2019 and BRATS 2020\\nstep.59\\n2.1. Dataset Description60\\nThis study used the Brain Tumor Segmentation (BraTS) datasets, where multimodal 3D MRI scans of61\\nGBM and LGG are available. The BraTS’2019 [26] contains data of 369 subjects with GBM (n = 293)62\\nand LGG (n = 76). Besides, BraTS’2020 [27] comprises 335 subjects with GBM (n = 259) and LGG63\\n(76). We merged these two datasets, and the compilation incorporated a total of 704 subjects: GBM (n64\\n= 552) and LGG (n = 152), as shown in Fig. 2 (b). The merged dataset includes neuroimaging files with65\\nmultiparametric 3D MRI scans, encompassing (a) T2 Fluid Attenuated Inversion Recovery (T2-FLAIR):66\\nsuppresses cerebrospinal fluid signals, enhancing lesion visibility, (b) Native (T1): captures the brain’s67\\nbaseline anatomy, highlighting tissue density differences, (c) Post-contrast T1-weighted (T1CE): acquires68\\nT1-weighted images with a contrast agent, highlighting areas of increased vascularization and blood-brain69\\nbarrier disruption to identify abnormal tissue, such as glioma lesions, (d) T2-weighted (T2): provides detailed70\\ninformation on edema, cysts, and tissue abnormalities, aiding in understanding glioma characteristics, and71\\n(e) ROI: allows feature extractions specifically from the tumor region or areas of interest that are relevant,72\\nprecise, and clinically meaningful. Sample MRI scans from each category are also visualized in Fig. 2 (a).73\\n2.2. Feature Extraction and Integration74\\nRadiomics, a transformative approach in medical imaging, involves the extraction and analysis of clini-75\\ncally significant information embedded within medical images, transcending what is perceptible to the human76\\neye [28]. To address this issue, we used the PyRadiomics [29] tool to identify relevant radiomic features in77\\nour study. Here, for each subject, we get 112 radiomic features corresponding to seven distinct types as78\\ndescribed in Table 1: First Order, Shape-Based, Gray Level Co-occurrence Matrix (GLCM), Gray Level79\\nRun Length Matrix (GLRLM), Gray Level Size Zone Matrix (GLSZM), Neighbouring Gray Tone Difference80\\nMatrix (NGTDM), and Gray Level Dependence Matrix (GLDM). The extracted features encompass a range81\\nof statistical metrics, shape-based attributes, and matrix-based analyses, offering insights into voxel intensity82\\ndistribution, geometric properties, spatial relationships, and textural patterns within ROI. However, each83\\ntype of these radiomic features contributes to unique and valuable information, and their integration ensures84\\na holistic understanding of the clinically significant attributes. To achieve this, for each MRI stage (T1, T2,85\\nT1CE, FLAIR), we organize these extracted features into tables (Radiomic Biomarker Table: 704 x 112)86\\nwhere each row corresponds to a specific subject, and each column represents different radiomic features.87\\n2.3. RGNN3D88\\nWe introduce a Radiomic Graph Neural Network (RGNN3D) architecture that integrates LSTM cells89\\nwithin its message-passing framework to enhance the learning of node embeddings as depicted in Fig. 3.90\\nThis approach allows the model to capture both short-term and long-term dependencies inherent in graph91\\nstructures. The design of the RGNN3D model consists of several layers, including an LSTM layer, GCN92\\nlayers with appropriate message passing among each block, and a dense output layer. The input layer receives93\\n112 features and constructs a graph. Here, the node feature vector and the graph’s adjacency matrix are94\\n4'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Table 1: Types of Radiomic Features Extracted in Our Study.\\nFeature Type Description\\nFirst Order Captures fundamental statistical metrics, offering voxel intensity distribu-\\ntion in ROIs.\\nShape-based Encompasses shape-related attributes in 3D and 2D, which understand the\\nbrain’s geometric properties.\\nGLCM Reveals voxel intensity spatial relationship patterns.\\nGLRLM Quantifies consecutive voxel lengths with identical intensity values.\\nGLSZM Offers size and spatial distribution of homogeneous intensity regions.\\nNGTDM Highlights tone differences between neighboring voxels and textural.\\nGLDM Analyzes voxel pair dependence.\\n.\\n.\\n.\\nF3\\nF2\\nF1\\nF8\\nF7\\nF6\\nF3\\nF8\\nF7\\n.\\n.\\n.\\nGCNConv3\\nf1\\nf2\\nf3\\n.\\n.\\n.\\nf91\\nf1\\nf2\\nf3\\n.\\n.\\n.\\nf91\\nf1\\nf2\\nf3\\n.\\n.\\n.\\nf112\\nL\\nG\\nL\\nL\\nGraph \\nConstruction\\nLGG\\nGBM\\nG\\nG\\nG\\nG\\nAdjacency \\nMatrix\\nNode \\nFeatures\\n Graph Convolutional with LSTM Network\\nOutput\\nFilter \\n2\\n[64, \\n2]\\nL\\nG\\nG\\n.\\n.\\n.\\nL\\nFilter \\n32 \\n[64,64]\\nLSTM \\n64\\n[64,64]\\nFilter \\n32 \\n[64,32]\\nFilter \\n16 \\n[64,16]\\nAggregate\\nUpdate\\nMessage \\nPassing\\nLSTM \\n64\\nAggregate\\nUpdate\\nMessage \\nPassing\\nAggregate\\nUpdate\\nMessage \\nPassing\\nDense \\nLayer\\nSoftmax\\nNeighbor \\nInformation \\nFlow\\nFigure 3: Proposed RGNN3D model architecture.\\ninitially fed into the Feed Forward Network (FFN) [30] blocks, followed by passing these to the GCN layers95\\nafter processing. This architecture includes three GCN layers, each employing the ReLU activation function96\\nwithin a message-passing framework. The embeddings updated by the LSTM cell and GCN layers are97\\nthen channeled back into the FFN blocks, producing the final node embeddings as logit values. Finally, a98\\nSoftmax activation function is applied to generate a probability distribution for the potential node labels.99\\nThe inclusion of LSTM enables gradient flow across time steps due to its gating mechanisms (input, forget,100\\nand output gates), which retain essential information and reduce vanishing gradient effects. This makes it101\\nsuitable for preserving long-range dependencies and refining spatial-temporal feature representations learned102\\nfrom the graph structure.103\\n5'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='2.4. Graph Representation and Initial Node Embeddings104\\nWe generate a graph with 704 rows and 112 columns using the radiomic biomarker table, representing105\\nimage features and the target class. Each patient is considered a single node in the graph and the corre-106\\nsponding radiomic feature vector serves as its initial embedding. In this regard, G = (V, E) is a graph, where107\\nV is the set of patient nodes and E represents the set of edges. The node feature matrix X ∈ R704×112, that108\\nencodes the radiomic features for all patients.109\\nTo avoid an artificial dependency from different patients, we employ an identity adjacency matrix A =110\\nI704. Because we decided to connect by self-loops. In this matter, each node maintains independence during111\\nmessage passing, where there is no possibillity of information leakage from other patients. This clinical112\\npractice especially relevant in the healthcare domain, where patient to patient connections are not always113\\nmeaningful. We ensure a simple graph structure but clinically efficient. This approach confirms that node114\\nembeddings are rely on radiomic features of each patient’s record.115\\nTheoretically, initial representation of a node v ∈ V is given by:116\\nh(0)\\nv = xv ∈ Rd,\\nwhere d = 112 is the dimensionality of the feature vector. After that, these embeddings pass through a series117\\nof graph convolution and recurrent (LSTM gates) operations. That enables the model to capture higher118\\nlevel abstractions from radiomic features.119\\nFor better computational efficiency and scalability, we generate a mini batch (each batch is 64 training120\\nsizes). In this regard, each batch feeds to the model during iteration. For a batch size B, we construct the121\\ncorresponding subgraph with a feature matrix Xb ∈ RB×112 and adjacency matrix Ab = IB. This approach122\\nensures the message passing within a batch. It is also prevents corss batch dependencies during training.123\\nWhile we use an identity adjacency matrix in this study, the framework is flexible and robust. That can124\\nextend to more complex graphs such as kNN based graphs or fully connected graphs but they did not offer125\\nreliable performance. By using a simple identity graph with LSTM cells, the approach is truly robust to the126\\ndataset. It is also adaptable to broader clinical applications. Overall, it supports the clinical generalisability.127\\n2.4.1. Graph Convolutions with LSTM Network128\\nOne of the core innovations of the RGNN3D model lies in its use of LSTM cells for aggregating and129\\nupdating node embeddings inside GCN blocks. The underlying message-passing process involves aggregat-130\\ning node features from its neighbors and updating the node’s embedding using LSTM cells, followed by131\\nnormalization and the final nonlinear embeddings. These steps are described as follows:132\\n(a) Aggregation of Neighboring Node Features:For a node u at iteration i, the aggregated message m(i)\\nu133\\nfrom its neighbors N(u) is computed as:134\\nm(i)\\nu =\\nX\\nv∈N(u)\\nW(i)\\nNeih(i−1)\\nv (1)\\nwhere W(i)\\nNei is a learnable weight matrix.135\\n(b) LSTM-based Node Updating:The updated embedding h(i)\\nu for node u is obtained using an LSTM cell136\\n[31] that takes the node’s previous embedding h(i−1)\\nu and the aggregated message m(i)\\nu as inputs:137\\nh(i)\\nu = LSTM(h(i−1)\\nu , m(i)\\nu ) (2)\\n6'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='The internal operations of the LSTM cell are defined by the following equations:138\\ni(i)\\nu = σ(W(i)\\ni h(i−1)\\nu + U(i)\\ni m(i)\\nu + b(i)\\ni ) (3)\\nf(i)\\nu = σ(W(i)\\nf h(i−1)\\nu + U(i)\\nf m(i)\\nu + b(i)\\nf ) (4)\\no(i)\\nu = σ(W(i)\\no h(i−1)\\nu + U(i)\\no m(i)\\nu + b(i)\\no ) (5)\\nc(i)\\nu = f(i)\\nu ⊙ c(i−1)\\nu + i(i)\\nu ⊙ tanh(W(i)\\nc h(i−1)\\nu +\\nU(i)\\nc m(i)\\nu + b(i)\\nc )\\n(6)\\nh(i)\\nu = o(i)\\nu ⊙ tanh(c(i)\\nu ) (7)\\nHere, i(i)\\nu , f(i)\\nu , o(i)\\nu are the input, forget, and output gates, respectively, and c(i)\\nu is the cell state. The139\\nparameters W(i)\\ni , U(i)\\ni , b(i)\\ni , etc., are the learnable weights and biases of the LSTM cell at iteration i.140\\n(c) Normalization of Node Embeddings:After several iterations of message passing, the node embeddings141\\nare normalized to improve their representational power:142\\nhnorm\\nu = h(i)\\nu\\n∥h(i)\\nu ∥\\n(8)\\nwhere ∥h(i)\\nu ∥ denotes the Euclidean norm of the vector h(i)\\nu .143\\n(d) Final Node Embeddings via GCN Layer:The normalized embeddings are then passed through a Graph144\\nConvolutional Network (GCN) [32] layer to obtain the final nonlinear node embeddings:145\\nH(i+1) = σ(AH(i)WGCN) (9)\\nwhere A is the adjacency matrix, where ones are on the diagonal and zeros elsewhere. This approach146\\nensures that each node is only connected to itself, focusing on the node-specific features without147\\nconsidering inter-node dependencies. Here, H(i) is the matrix of node embeddings at iteration i,148\\nWGCN is the weight matrix of the GCN layer, and σ is a nonlinear activation function.149\\n2.4.2. Output Layer and Grading150\\nThe final embeddings H(i+1) from the GCN layer are passed through a dense output layer to predict the151\\nnode labels. The dense layer applies a linear transformation followed by a nonlinear activation function of152\\nSoftmax to generate a probability distribution for each node’s label. The output for a node u is computed153\\nas follows:154\\nyu = σ(Wouthnorm\\nu + bout) (10)\\nHere, Wout is the weight matrix of the output layer, bout is the bias term, and σ denotes the Softmax155\\nactivation function, which maps the output to each probability value representing the grading of glioma.156\\n2.5. Implementation157\\nThe implementation of the RGNN3D model commenced with the combined BraTS dataset comprising158\\n704 records, which was further divided into training and testing sections in an 80:20 ratio. The model was159\\noptimized using the Adam optimizer, with a batch size of 64, and trained over 50 epochs. The Glorot160\\nUniform initializer was employed for the kernel initialization. Sparse Categorical Cross-Entropy was utilized161\\nfor the loss function. The implementation was executed on a system equipped with an RTX 3060 GPU and162\\n32GB of RAM, running the NVIDIA driver version 535.104.05 with CUDA version 12.2.163\\n7'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='3. Results and Discussion164\\nFirst, we start with the performance analysis across all four 3D MRI stages (T1, T1CE, T2, and FLAIR)165\\nusing standard classification performance metrics [33] i.e., precision, recall, F1-score, and accuracy. From166\\nthe confusion matrices as shown in Fig. 4 we observe: (i) for classifying LGG, the proposed model acquired167\\nidentical performance across all stages with 28 accurate cases and only two misclassifications, (ii) for GBM,168\\nwe get no false negative result for both T1 and T2 stages. However, there were only 2 and 6 misclassifications169\\nout of 141 cases with T1CE and FLAIR stages, respectively. from the confusion matrices, it is clear that GBM170\\nis more consistently detected compared to LGG, which demonstrated strong power for aggressive gliomas.\\n105\\n6\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nFLAIR\\n0\\n100\\n75\\n50\\n25\\nPREDICTED\\nACTUAL\\n111\\n0\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nT2\\n0\\n100\\n75\\n50\\n25\\nPREDICTED\\nACTUAL\\n109\\n2\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nT1CE\\n0\\n100\\n75\\n50\\n25\\nPREDICTED\\nACTUAL\\n111\\n0\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nT1\\n0\\n100\\n75\\n50\\n25\\nPREDICTED\\nACTUAL\\nFigure 4: Confusion Matrices of RGNN3D Model for MRI Stages T1, T2, T1CE, and FLAIR\\n171\\nUsing these matrices, we calculate different scores and discover a similar trend for each evaluation criterion172\\nas depicted in Fig. 5. The proposed RGNN3D system achieved outstanding accuracy scores of 98.58% for173\\nT1 and T2, 97.16% for T1CE, and 94.33% for FLAIR. The F1-scores also reinforce the system’s robustness,174\\nmaintaining values of 99.10% for T1 and T2, 98.20% for T1CE, and 96.33% for FLAIR. Furthermore, the175\\nprecision values reached 100% for F1 and F2, which are able to completely avoid false-positive decisions for176\\nglioma grading. Even for T1CE and FLAIR modalities, precision scores remain high at 98.20% and 94.59%177\\nrespectively. Moreover, our system shows strong recall: 98.23% (T1), 98.23% (T2), 98.20% (T1CE), and178\\n98.13% (FLAIR). Overall, we notice the best and identical performance of the system with both T1 and T2179\\n(accuracy: 98.58%, precision: 100%, f1-score: 99.10%, and recall: 98.23%) for each category.180\\nBesides, the accuracy and loss curves (outlined in Fig. 6) illustrate the performance of the proposed181\\nmodel over 50 epochs. Here, we notice that the training accuracy reaches 100.00%, and we get the validation182\\naccuracy to be 98.58%. We noted that the training loss decreases smoothly for 4 different MRI stages, but183\\n8'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='ACCURACY\\nF1-SCORE\\nPRECISION\\nRECALL\\n98.58%\\n97.16%\\n98.58%\\n94.33%\\n99.10%\\n98.20%\\n99.10%\\n96.33%\\n100.00%\\n98.20%\\n100.00%\\n94.59%\\n98.23%\\n98.20%\\n98.23%\\n98.13%\\nFLAIR\\nT2\\nT1CE\\nT1\\nFigure 5: Comparison of Scores of RGNN3D Model for 4 MRI Stages\\nTraining Loss Training Accuracy\\nValidation \\nLoss\\nValidation \\nAccuracy\\nEpochs\\nEpochs\\nEpochs\\nEpochs\\nLoss\\nAccuracy\\n \\nLoss\\nAccuracy\\nFLAIR\\nT1CE\\nT2\\nT1\\nFLAIR\\nT1CE\\nT2\\nT1\\nFLAIR\\nT1CE\\nT2\\nT1\\nFLAIR\\nT1CE\\nT2\\nT1\\n0.7\\n0.6\\n0.5\\n0.4\\n0.3\\n0.2\\n0.1\\n0.0\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n10\\n20\\n30\\n40\\n50\\n0.0\\n0.2\\n0.4 \\n0. \\n6\\n0. \\n8\\n1.0\\n0.0\\n0.2\\n0.4 \\n0. \\n6\\n0. \\n8\\n1.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\n0.0\\nFigure 6: Accuracy and Loss Curves of RGNN3D Model for 4 MRI Stages\\nthe validation losss shows differently. Here, for T1 and T2, the validation loss follows training loss closely,184\\nwhich indicates stable learning in our proposed RGNN3D method. In T1CE, the validation loss shows185\\n9'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='98.59%\\n1\\n0.90\\n0.92\\n0.94\\n0.96\\n0.98\\n1.00\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n94.37%\\n100.00%\\n98.57%\\n100.00%\\n100.00%\\n98.57%\\n92.66%\\n98.57%\\n97.14%\\n10-FOLD CROSS VALIDATION of T1\\nBest \\nVal \\nAccuracy\\nper \\nFold\\nOverall \\nBest\\nAccuracy \\n(97.87%)\\nOverall \\nStd \\nDev\\n(±2.31%)\\n98.59%\\n1\\n0.90\\n0.92\\n0.94\\n0.96\\n0.98\\n1.00\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n92.96%\\n98.57%\\n98.57%\\n97.14%\\n100.00%\\n98.57%\\n97.14%\\n95.71%\\n98.14%\\n10-FOLD \\nCROSS \\nVALIDATION \\nof \\nT2\\nBest \\nVal \\nAccuracy\\nper \\nFold\\nOverall \\nBest\\nAccuracy \\n(97.44%)\\nOverall \\nStd \\nDev\\n(±1.87%)\\nFigure 7: Cross-Validation Performance of Glioma Grading with T1 and T2 MRI Modalities\\na bit higher, but it has stability. For the FLAIR stage, it significantly rises. This is why, the FLAIR186\\nmodality achieved the lowest accuracy compared to other modalities. Overall, the training loss curve shows187\\na consistent decrease compared to the validation loss trend, where we only observe consistency with T1 and188\\nT2. This phenomenon also supports its effectiveness with our architecture.189\\nWe extend our investigation into contemporary glioma grading systems that are not explicitly focused on190\\nradiomics features for 10-fold cross-validation. Among all the configurations tested, T1 and T2 were chosen191\\nsince they attained the highest overall accuracies. Figure 7 shows T1’s (upper part) validation accuracies192\\n(x-axis: fold numbers 1-10, y-axis: accuracy %), ranging from 92.66% (fold 7) to 100% (folds 3, 5, 8).193\\nThe overall average accuracy across all folds was 97.87% with a standard deviation of ± 2.31%, indicating194\\nstrong performance with some variation across folds. Figure 2 (lower part) shows T2’s accuracies, which195\\nranged from 92.96% (fold 2) to 100% (fold 3), with an average accuracy of 97.44% ± 1.87%, reflecting196\\nslightly lower but more consistent results compared to T1. Furthermore, the ROC curves concerning MRI197\\nmodality are displayed in Fig. 8 to confirm the proposed RGNN3D model’s strength in identifying glioma198\\ngrades. Remarkably, both GBM and LGG perform exceptionally prominent; their ROC curves for T1 and199\\nT2 approach the optimal top-left corner, which is consistent with their high accuracy and the prior described200\\nF1-scores. Conversely, FLAIR demonstrates a marginally lower AUC, confirming its comparatively poorer201\\nperformance. Taken together, our proposed architecture demonstrates better performance with T1 and T2202\\nstages compared to T1CE and FLAIR while grading glioma tumors.203\\nSubsequently, to illustrate the interpretability of our proposed architecture, we implement LIME [34].204\\n10'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='ROC Curve of T1\\nROC \\ncurve \\n(AUC=0.96)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\n ROC Curve of T2\\nROC \\ncurve \\n(AUC=0.97)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\nROC \\nCurve \\nof \\nT1CE\\nROC \\ncurve \\n(AUC=0.97)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\nROC \\ncurve \\n(AUC=0.96)\\nROC \\nCurve \\nof \\nFLAIR\\nROC \\ncurve \\n(AUC=0.97)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\nROC \\ncurve \\n(AUC=0.94)\\nFigure 8: Performance Analysis of RGNN3D Model for 4 MRI Stages in ROC Curves\\nAs shown in Fig. 9, the prioritization of specific radiomic features in LIME-based interpretations is driven205\\nby contribution to model performance where all radiomic features are clinically relevant. Still, the top three206\\nmost essential features make a significant contribution to the grading of glioma in these specific data points.207\\nFor GBM, the features “Mesh Volume”, “Maximum 2D diameter”, and “Maximum 3D diameter” are crucial208\\nfactors among the 112 features, underscoring their significant roles in distinguishing glioma. These features209\\nprovide critical information about the tumor’s size, shape, and spatial dimensions. For LGG, the features210\\n“Minor Axis Length”, “Maximum 2D diameter”, and “Gray Level Non-Uniformity” are the top features,211\\nemphasizing their interpretability and actionable insights in clinical decision-making. The shape and texture212\\nfeatures captured by these metrics are vital in differentiating between glioma grades and understanding the213\\nheterogeneity within the tumor. While the remaining features also contribute valuable information, they214\\nare comparatively less significant for several reasons, such as redundancy, specificity, statistical significance,215\\nclinical validation, and model performance.216\\nAt this point, we concentrate on the previous studies utilizing radiomic features for glioma tumor grading.217\\nFrom the current literature, we observe that most of the related studies focused on statistical ML models218\\nsuch as LR [10], SVM [11, 12], and RF [2, 13, 14]. Consequently, we aim to evaluate the performance of219\\nstatistical ML models on our dataset. Consequently, we implement five ML models (K-NN, LR, DT, RF,220\\nand SVM) with the acquired radiomic features using our dataset. Here, we use T1 stage MRI images as we221\\ngot good results during our prior analysis. Table 2 presents a comparative summary of the above-mentioned222\\n11'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Mesh \\nVolume\\nMaximum \\n2D \\ndiameter \\n(1.00)\\nKurtosis \\n(-0.60)\\n10th \\npercentile \\n(-0.34)\\nLow \\nGray \\nLevel \\nZone \\nEmphasis \\n(-0.40)\\nLarge \\nDependence \\nLow \\nGray \\nLevel \\nEmphasis \\n(-0.21)\\nMaximum \\n3D \\ndiameter \\n(0.64)\\nSmall \\nArea \\nHigh \\nGray \\nLevel \\nEmphasis \\n(-0.12)\\nElongation \\n(-1.28)\\nSmall \\nDependence \\nHigh \\nGray \\nLevel \\nEmphasis \\n(-0.11)\\nMesh \\nVolume \\n(0.63)\\n0.07\\n0.04\\n0.04\\n0.04\\n0.05\\n0.05\\n0.05\\n0.05\\n0.06\\n0.27\\nMaximum \\n2D \\ndiameter \\nSmall \\nDependence \\nHigh \\nGray \\nLevel \\nEmphasis\\nElongation\\nSmall \\nArea \\nHigh \\nGray \\nLevel \\nEmphasis\\nMaximum \\n3D \\ndiameter\\nLarge \\nDependence \\nLow \\nGray \\nLevel \\nEmphasis\\nLow \\nGray \\nLevel \\nZone \\nEmphasis\\nKurtosis\\n10th \\npercentile\\nGBM\\n1.00\\nLGG\\n0.00\\nFEATURES \\nCONTRIBUTION \\nON \\nGBM\\nGRADING \\nABILITIES \\nOF \\nGBM\\nGBM \\nGRADING\\nMesh \\nVolume\\nMinor \\nAxis \\nLength \\n(2.55)\\nGray \\nLevel \\nNon-Uniformity \\n(3.08)\\nJoint \\nEntropy \\n(0.51)\\n90th \\npercentile \\n(0.23)\\nMaximum \\n2D \\ndiameter \\n(2.12)\\nSum \\nof \\nSquares \\n(-0.10)\\nJoint \\nAverage \\n(0.96)\\nZone \\nEntropy \\n(1.52)\\nHigh \\nGray \\nLevel \\nZone \\nEmphasis \\n(0.02)\\nMesh \\nVolume \\n(-0.76)\\n0.12\\n0.09\\n0.08\\n0.09\\n0.10\\n0.10\\n0.11\\n0.11\\n0.11\\n0.15\\nMinor \\nAxis \\nLength\\nHigh \\nGray \\nLevel \\nZone \\nEmphasis\\nZone \\nEntropy\\nJoint \\nAverage\\nSum \\nof \\nSquares\\nMaximum \\n2D \\ndiameter\\n90th \\npercentile\\nGray \\nLevel \\nNon-Uniformity\\nJoint \\nEntropy\\nGBM\\n0.00\\nLGG\\n1.00\\nFEATURES \\nCONTRIBUTION \\nON \\nLGG\\nGRADING \\nABILITIES \\nOF \\nLGG\\nLGG \\nGRADING\\nb\\n a\\nFigure 9: Analysis of Interpretability and Contribution of Radiomic Biomarkers for Glioma Grading using LIME: a) Explain-\\nability on grading GBM, b) Explainability on grading LGG\\nTable 2: Comparative Performance Analysis of Conventional ML Models with RGNN3D for Glioma Grading.\\nModel Precision Recall Specificity F1-Score AUC Accuracy\\nK-NN 96.30% 86.67% 99.10% 91.23% 92.88% 95.72%\\nLR 96.00% 80.00% 99.10% 87.27% 89.55% 92.69%\\nDT 90.32% 93.33% 97.30% 91.80% 95.32% 95.55%\\nRF 100.00% 93.33% 100.00% 96.55% 96.67% 96.79%\\nSVM 89.66% 86.67% 97.30% 88.14% 96.67% 92.87%\\nRGNN3D 100.00% 98.23% 100.00% 98.10% 97.00% 98.58%\\nML models and RGNN3D. After a thorough investigation, we noticed that RGNN3D outperformed all other223\\nML models across nearly all metrics, demonstrating superior performance in glioma grading. These results224\\nindicate the efficacy of our proposed model in accurately grading glioma tumors compared to other ML225\\nmodels.226\\nAs evidenced in Table 3, the proposed system achieves a testing accuracy of 98.58% on the BraTS 2019-20227\\ndataset, ensuring head-to-head competency. For image-based data-learning tasks, deep convolutional neural228\\nnetworks remain the gold standard. Still, our proposed neural network, trained on only radiomic features,229\\nshowcases comparable performance, narrowly missing out on the highest accuracy by less than 0.5%. In230\\nfact, RGNN3D outperforms the only other graph-based model in the table by around 5% in accuracy.231\\nThe comprehensive comparative analysis establishes RGNN3D as a reliable, accurate, and interpretable232\\nalternative to the existing state-of-the-art.233\\n3.1. Ablation Study234\\nTo validate the design of our RGNN3D model, we performed an ablation study that focuses on optimizers,235\\nactivation functions, and GCN block configurations (model depth). As shown in Table 4, the Adam optimizer236\\n12'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Table 3: Comparative Analysis of State-of-the-Art Glioma Grading Methods with the Proposed RGNN3D Architecture.\\nArchitecture Dataset Result\\nMM-XGB, 2023 [35] BraTS 2020 93.00%\\nSASG-GCN, 2023 [36] TCGA-LGG 93.62%\\nSGD with ADASYN, 2023 [37]BraTS 2020 96.00%\\nCNN Based, 2023 [38] BraTS 2017-19 97.85%\\nMMD-VAE, 2022 [39] BraTS 2019 98.46%\\nTEWMA-CNN, 2022 [40] BraTS 2015,21 98.76%\\nTD-CNN-LSTM, 2022 [41] BraTS 2019-21 98.90%\\nProposed RGNN3D, 2025BraTS 2019-20 98.58%\\nnot only achieved the best accuracy, but it also ensured the fastest operation (33s x 50). It proves the237\\neffectiveness of performance and efficiency. While Admax and Nadam achieved competitive accuracies of238\\n95.03% and 97.87% respectively, they took longer computational times (52–83s for every epoch). In contrast,239\\nSGD optimizer is both slower and less accurate (39.83%) compared to others. For activation functions, ReLU\\nTable 4: Ablation Study: Impact of Optimizers, Activation Functions, and GCN Block Configurations on RGNN3D Perfor-\\nmance.\\nNo Variant Training Time ×Epoch Test Accuracy Findings\\nOptimizers\\n1 Adam 33 s ×50 98.58% Best accuracy\\n2 Adamax 83 s ×50 95.03% Good accuracy\\n3 Nadam 52 s ×50 97.87% Good accuracy\\n4 RMSprop 59 s ×50 97.16% Good accuracy\\n5 SGD 62 s ×50 39.83% Poor accuracy\\nActivation Functions\\n6 Sigmoid 64 s ×50 95.03% Good accuracy\\n7 Elu 67 s ×50 97.87% Good accuracy\\n8 ReLU 33 s ×50 98.58% Best accuracy\\n9 Tanh 60 s ×50 97.16% Good accuracy\\n10 Leaky ReLU 62 s ×200 97.16% Good accuracy\\nModel Blocks\\n11 1 GCN block 42 s ×50 93.17% Poor accuracy\\n12 2 GCN blocks 50 s ×50 97.87% Good accuracy\\n13 3 GCN blocks 62 s ×50 97.16% Good accuracy\\n14 RGNN3D (3 GCN + LSTM) 33 s ×50 98.58% Best accuracy\\nLSTM Cell Sizes\\n15 LSTM (8 units) 59 s ×50 97.87% Good accuracy\\n16 LSTM (16 units) 30 s ×50 97.16% Good accuracy\\n17 LSTM (32 units) 33 s ×50 98.58% Best accuracy\\n18 LSTM (64 units) 63 s ×50 98.58% Best accuracy but time consuming\\n240\\nactivation achieves 98.58% test accuracy with the lowest training time (33s in every single epoch). Although241\\nElu, Tanh, and Leaky ReLU achieved good accuracy, Sigmoid has shown poor performance.242\\nRegarding model architecture, a single GCN block is not good (93.17%) in terms of accuracy but it243\\nconsumes reasonable training time (42 s per epoch). Afterward, we added another GCN block to our model.244\\nBy doing it, performance was improved considerably (97.87%), while it took more training time, around 50245\\ns in every epoch. Then, we integrated another GCN block into our method, but the performance fell slightly246\\nand the time also increased. To decrease the computation power and increase robustness, we integrated247\\nthe LSTM layer into our model. The proposed RGNN3D model (3 GCN + LSTM) not only achieved248\\nthe highest accuracy (98.58%) but also trained efficiently (33s × 50 epochs), which offered the benefit of249\\ncombining spatial and temporal learning. It is clear that LSTM enhances temporal learning with the highest250\\nperformance, and it also maintains computational power efficiently.251\\nTo experiment on the impact of recurrent layers, we conducted an ablation study on different LSTM252\\ncell sizes (8, 16, 32 and 64 units). Smaller LSTM configurations with 8 and 16 units achieved slightly lower253\\n13'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='accuracies of 97.87% and 97.16% respectively, while they are computationally efficient. In contrast, the unit254\\nsize 64 also achieves the best accuracy, but it takes the highest time as computational power. It means a255\\nlarger LSTM cell does not offer performance benefits. We noted that using LSTM with a 32 cell size gives us256\\nthe best accuracy (98.58%), aslo the least time consuming (33 s per epoch). That makes the most balanced257\\nconfiguration for our proposed (RGNN3D) model. So, these findings support our architectural choices and258\\ndemonstrate the strength of RGNN3D for glioma grading.259\\n3.2. Limitations260\\nThis study has a few limitations. The model was trained on a limited dataset (BraTS 2019-20), which261\\nmay affect its generalizability across diverse patient groups. Performance on FLAIR images was lower,262\\nsuggesting room for improvement in preprocessing. While LIME helps explain predictions, deeper clinical263\\ninterpretability remains a challenge. Lastly, the model has not yet been tested in real clinical environments,264\\nwhich is essential for confirming its practical utility.265\\n4. Conclusion266\\nThis study innovated a novel hybrid architecture that could be used to grade glioma tumors reliably and267\\naccurately utilizing 3D MRI data. Hence, one of the key strengths of the RGNN3D model is its ability to268\\neffectively integrate and utilize medical radiomic features, which are critical in the accurate characterization269\\nof tumor heterogeneity and progression. We believe the RGNN3D architecture would serve as an intuitive270\\ndecision support system for medical experts by significantly improving diagnostic precision. Our evaluation271\\nacross four MRI stages (T1, T1CE, T2, and FLAIR) reveals that both T1 and T2 stages can be utilized to get272\\nthe highest performance in terms of grading accuracy, demonstrating their robustness in capturing critical273\\ntumor characteristics. Future research should focus on integrating multi-parametric MRI data to leverage274\\nthe strengths of each modality. Additionally, improving preprocessing and feature extraction techniques275\\nfor FLAIR images may assist in mitigating current limitations and enhancing their utility in grading. The276\\nmodel’s robustness could be further enhanced with a larger dataset that includes diverse patient glioma277\\ngrading reports, ensuring broader applicability and accuracy. We also believe that our proposed system278\\ncould be applicable in clinical settings and digital healthcare, especially in rural or isolated places with279\\nlimited access to specialist physicians.280\\nReferences281\\n[1] D. Ricard, A. Idbaih, F. Ducray, M. Lahutte, K. Hoang-Xuan, J.-Y. Delattre, Primary brain tumours in adults, The282\\nLancet 379 (9830) (2012) 1984–1996.283\\n[2] R. Kumar, A. Gupta, H. S. Arora, G. N. Pandian, B. Raman, Cghf: A computational decision support system for glioma284\\nclassification using hybrid radiomics-and stationary wavelet-based features, IEEE Access 8 (2020) 79440–79458.285\\n[3] E. B. Claus, K. M. Walsh, J. K. Wiencke, A. M. Molinaro, J. L. Wiemels, J. M. Schildkraut, M. L. Bondy, M. Berger,286\\nR. Jenkins, M. Wrensch, Survival and low-grade glioma: the emergence of genetic information, Neurosurgical focus 38 (1)287\\n(2015) E6.288\\n[4] Q. T. Ostrom, L. Bauchet, F. G. Davis, I. Deltour, J. L. Fisher, C. E. Langer, M. Pekmezci, J. A. Schwartzbaum, M. C.289\\nTurner, K. M. Walsh, et al., The epidemiology of glioma in adults: a “state of the science” review, Neuro-oncology 16 (7)290\\n(2014) 896–913.291\\n[5] H. Hyare, S. Thust, J. Rees, Advanced mri techniques in the monitoring of treatment of gliomas, Current treatment292\\noptions in neurology 19 (2017) 1–15.293\\n[6] Z. Liu, S. Wang, D. Dong, J. Wei, C. Fang, X. Zhou, K. Sun, L. Li, B. Li, M. Wang, et al., The applications of radiomics294\\nin precision diagnosis and treatment of oncology: opportunities and challenges, Theranostics 9 (5) (2019) 1303.295\\n[7] M. E. Mayerhoefer, A. Materka, G. Langs, I. H¨ aggstr¨ om, P. Szczypi´ nski, P. Gibbs, G. Cook, Introduction to radiomics,296\\nJournal of Nuclear Medicine 61 (4) (2020) 488–495.297\\n[8] J.-b. Qin, Z. Liu, H. Zhang, C. Shen, X.-c. Wang, Y. Tan, S. Wang, X.-f. Wu, J. Tian, Grading of gliomas by using radiomic298\\nfeatures on multiple magnetic resonance imaging (mri) sequences, Medical science monitor: international medical journal299\\nof experimental and clinical research 23 (2017) 2168.300\\n[9] G. Cui, J. J. Jeong, Y. Lei, T. Wang, T. Liu, W. J. Curran, H. Mao, X. Yang, Machine-learning-based classification of301\\nglioblastoma using mri-based radiomic features, in: Medical imaging 2019: computer-aided diagnosis, Vol. 10950, SPIE,302\\n2019, pp. 1063–1068.303\\n14'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[10] H.-H. Cho, H. Park, Classification of low-grade and high-grade glioma using multi-modal image radiomics features, in:304\\n2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), IEEE,305\\n2017, pp. 3081–3084.306\\n[11] Q. Chen, L. Wang, L. Wang, Z. Deng, J. Zhang, Y. Zhu, Glioma grade prediction using wavelet scattering-based radiomics,307\\nIEEE Access 8 (2020) 106564–106575.308\\n[12] P. Sun, D. Wang, V. C. Mok, L. Shi, Comparison of feature selection methods and machine learning classifiers for radiomics309\\nanalysis in glioma grading, Ieee Access 7 (2019) 102010–102020.310\\n[13] J. Cheng, J. Liu, H. Yue, H. Bai, Y. Pan, J. Wang, Prediction of glioma grade using intratumoral and peritumoral radiomic311\\nfeatures from multiparametric mri images, IEEE/ACM Transactions on Computational Biology and Bioinformatics 19 (2)312\\n(2020) 1084–1095.313\\n[14] H.-h. Cho, S.-h. Lee, J. Kim, H. Park, Classification of the glioma grading using radiomics analysis, PeerJ 6 (2018) e5982.314\\n[15] S. Priya, Y. Liu, C. Ward, N. H. Le, N. Soni, R. Pillenahalli Maheshwarappa, V. Monga, H. Zhang, M. Sonka, G. Bathla,315\\nMachine learning based differentiation of glioblastoma from brain metastasis using mri derived radiomics, Scientific reports316\\n11 (1) (2021) 10478.317\\n[16] U. Baid, S. U. Rane, S. Talbar, S. Gupta, M. H. Thakur, A. Moiyadi, A. Mahajan, Overall survival prediction in318\\nglioblastoma with radiomic features using machine learning, Frontiers in computational neuroscience 14 (2020) 61.319\\n[17] H. Li, Z. Wang, C. Lan, P. Wu, N. Zeng, A novel dynamic multiobjective optimization algorithm with non-inductive320\\ntransfer learning based on multi-strategy adaptive selection, IEEE transactions on neural networks and learning systems321\\n(2023).322\\n[18] P. Wu, H. Li, L. Hu, J. Ge, N. Zeng, A local-global attention fusion framework with tensor decomposition for medical323\\ndiagnosis, IEEE/CAA Journal of Automatica Sinica 11 (6) (2024) 1536–1538.324\\n[19] T. Xie, X. Chen, J. Fang, H. Kang, W. Xue, H. Tong, P. Cao, S. Wang, Y. Yang, W. Zhang, Textural features of dy-325\\nnamic contrast-enhanced mri derived model-free and model-based parameter maps in glioma grading, Journal of Magnetic326\\nResonance Imaging 47 (4) (2018) 1099–1111.327\\n[20] S. Bisdas, C. Tisca, C. Sudre, E. Sanverdi, D. Roettger, J. M. Cardoso, Non-invasive in vivo prediction of tumour grade and328\\nidh mutation status in gliomas using dynamic susceptibility contrast (dsc) perfusion-and diffusion-weighted mri. (2018).329\\n[21] Q. Tian, L.-F. Yan, X. Zhang, X. Zhang, Y.-C. Hu, Y. Han, Z.-C. Liu, H.-Y. Nan, Q. Sun, Y.-Z. Sun, et al., Radiomics330\\nstrategy for glioma grading using texture features from multiparametric mri, Journal of Magnetic Resonance Imaging331\\n48 (6) (2018) 1518–1528.332\\n[22] C. Su, J. Jiang, S. Zhang, J. Shi, K. Xu, N. Shen, J. Zhang, L. Li, L. Zhao, J. Zhang, et al., Radiomics based on333\\nmulticontrast mri can precisely differentiate among glioma subtypes and predict tumour-proliferative behaviour, European334\\nradiology 29 (2019) 1986–1996.335\\n[23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, S. Y. Philip, A comprehensive survey on graph neural networks, IEEE336\\ntransactions on neural networks and learning systems 32 (1) (2020) 4–24.337\\n[24] A. Graves, A. Graves, Long short-term memory, Supervised sequence labelling with recurrent neural networks (2012)338\\n37–45.339\\n[25] M. T. Ribeiro, S. Singh, C. Guestrin, ” why should i trust you?” explaining the predictions of any classifier, in: Proceedings340\\nof the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016, pp. 1135–1144.341\\n[26] Multimodal Brain Tumor Segmentation Challenge 2019: Data — CBICA — Perelman School of Medicine at the University342\\nof Pennsylvania — med.upenn.edu, www.med.upenn.edu/cbica/brats2019/data.html, [Accessed 23-06-2024].343\\n[27] Multimodal Brain Tumor Segmentation Challenge 2020: Data — CBICA — Perelman School of Medicine at the University344\\nof Pennsylvania — med.upenn.edu, www.med.upenn.edu/cbica/brats2020/data.html, [Accessed 23-06-2024].345\\n[28] J. E. Van Timmeren, D. Cester, S. Tanadini-Lang, H. Alkadhi, B. Baessler, Radiomics in medical imaging—“how-to”346\\nguide and critical reflection, Insights into imaging 11 (1) (2020) 91.347\\n[29] J. J. Van Griethuysen, A. Fedorov, C. Parmar, A. Hosny, N. Aucoin, V. Narayan, R. G. Beets-Tan, J.-C. Fillion-Robin,348\\nS. Pieper, H. J. Aerts, Computational radiomics system to decode the radiographic phenotype, Cancer research 77 (21)349\\n(2017) e104–e107.350\\n[30] T. T. Truong, D. Dinh-Cong, J. Lee, T. Nguyen-Thoi, An effective deep feedforward neural networks (dfnn) method for351\\ndamage identification of truss structures using noisy incomplete modal data, Journal of Building Engineering 30 (2020)352\\n101244.353\\n[31] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation 9 (8) (1997) 1735–1780.354\\n[32] T. N. Kipf, M. Welling, Semi-supervised classification with graph convolutional networks, International Conference on355\\nLearning Representations (ICLR), 2017.356\\n[33] M. A. Ali, M. S. Hossain, M. K. Hossain, S. S. Sikder, S. A. Khushbu, M. Islam, Amdnet23: Hybrid cnn-lstm deep357\\nlearning approach with enhanced preprocessing for age-related macular degeneration (amd) detection, Intelligent Systems358\\nwith Applications 21 (2024) 200334.359\\n[34] A. Palkar, C. C. Dias, K. Chadaga, N. Sampathila, Empowering glioma prognosis with transparent machine learning and360\\ninterpretative insights using explainable ai, IEEE Access 12 (2024) 31697–31718.361\\n[35] F. Ullah, M. Nadeem, M. Abrar, F. Amin, A. Salam, A. Alabrah, H. AlSalman, Evolutionary model for brain cancer-362\\ngrading and classification, IEEE Access (2023).363\\n[36] L. Liu, J. Chang, P. Zhang, H. Qiao, S. Xiong, Sasg-gcn: self-attention similarity guided graph convolutional network for364\\nmulti-type lower-grade glioma classification, IEEE Journal of Biomedical and Health Informatics (2023).365\\n[37] M. Renugadevi, K. Narasimhan, C. Ravikumar, R. Anbazhagan, G. Pau, K. Ramkumar, M. Abbas, N. Raju, K. Satish,366\\nS. Prabu, Machine learning empowered brain tumor segmentation and grading model for lifetime prediction, IEEE Access367\\n(2023).368\\n15'),\n",
       " Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[38] H. A. Hafeez, M. A. Elmagzoub, N. A. B. Abdullah, M. S. Al Reshan, G. Gilanie, S. Alyami, M. U. Hassan, A. Shaikh,369\\nA cnn-model to classify low-grade and high-grade glioma from mri images, IEEE Access 11 (2023) 46283–46296.370\\n[39] J. Cheng, M. Gao, J. Liu, H. Yue, H. Kuang, J. Liu, J. Wang, Multimodal disentangled variational autoencoder with game371\\ntheoretic interpretability for glioma grading, IEEE Journal of Biomedical and Health Informatics 26 (2) (2021) 673–684.372\\n[40] S. Divya, L. Padma Suresh, A. John, Enhanced deep-joint segmentation with deep learning networks of glioma tumor for373\\nmulti-grade classification using mr images, Pattern Analysis and Applications 25 (4) (2022) 891–911.374\\n[41] S. Montaha, S. Azam, A. R. H. Rafid, M. Z. Hasan, A. Karim, A. Islam, Timedistributed-cnn-lstm: A hybrid approach375\\ncombining cnn and lstm to classify brain tumor on 3d mri scans performing ablation study, IEEE Access 10 (2022)376\\n60039–60059.377\\n16')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimal_docs = filter_to_minimal_docs(extracted_data)\n",
    "minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cbf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 268\n",
      "[Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='AMDNet23: A combined deep Contour-based Convolutional Neural Network and Long \\nShort Term Memory system to diagnose Age-related Macular Degeneration \\nMd. Aiyub Ali1, Md. Shakhawat Hossain1, Md.Kawar Hossain1, Subhadra Soumi Sikder1, \\nSharun Akter Khushbu1 and Mirajul Islam1 \\n1 Department of Computer Science and Engineering, Daffodil International University, Dhaka \\n1341, Bangladesh \\nCorrespondence: Mirajul Islam; merajul15-9627@diu.edu.bd \\nAbstract'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Abstract \\nIn light of the expanding population, an automated framework of disease detection can assist doctors in the \\ndiagnosis of ocular diseases, yields accurate, stable, rapid outcomes, and improves the success rate of early \\ndetection. The work initially intended the enhancing the quality of fundus images by employing an adaptive \\ncontrast enhancement algori thm (CLAHE) and Gamma correction. In the preprocessing techniques,'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='CLAHE elevates the local contrast of the fundus image and gamma correction increases the intensity of \\nrelevant features. This study operates on a AMDNet23 system of deep learning that combi ned the neural \\nnetworks made up of convolutions (CNN) and short-term and long-term memory (LSTM) to automatically \\ndetect aged macular degeneration (AMD) disease from fundus ophthalmology. In this mechanism, CNN is'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='utilized for extracting features and LSTM is utilized to detect the extracted features. The dataset of this \\nresearch is collected from multiple sources and afterward applied quality assessment techniques, 2000 \\nexperimental fundus images encompass four distinct classes equitably. The proposed hybri d deep \\nAMDNet23 model demonstrates to detection of AMD ocular disease and the experimental result achieved'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='an accuracy 96.50%, specificity 99.32%, sensitivity 96.5%, and F1 -score 96.49.0%. The system achieves \\nstate-of-the-art findings on fundus imagery dat asets to diagnose AMD ocular disease and findings \\neffectively potential of our method. \\n \\nKeywords: AMDNet23, Fundus image classification, CNN-LSTM, ocular diseases, automated diagnosis, \\nconvolutional neural networks, long short-term memory,  early detection, Medical imaging, diagnosis. \\n \\nIntroduction'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Introduction \\nOver the past two decades, ocular diseases ( ODs) that can cause blindness have become extremely \\nwidespread. ODs encompass a wide range of c onditions that can affect various components of the ocular, \\nincluding the corneal tissue, lens, retina, optic nerves and periorbital tissues. Ocular diseases include \\nabnormalities such as cataracts, untreated nearsightedness, trachoma, macular degeneration associated with'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='aging, and diabetes -associated retinopathy. These ailments play a substantial role in global retinal \\ndegeneration and visual impairment. [1]. The worldwide prevalence of near - or farsightedness vision \\ndeficiency affects over 2.2 billion in dividuals [2]. Approximately half of the total cases, amounting to at \\nleast 1 billion folks, as reported by the World Health Organization (WHO), suffer from vision impairments'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='that could have been evaded or remain unattended. Among these individuals, aroun d 88.4 million have \\nuntreated refractive errors leading to adequate to extensive distant impaired vision, nearly ninety -four \\nmillion owned cataracts,  and eight million individuals are possessed by aged-related macular degeneration, \\ndiabetic retinopathy (3.9 million). [3] Despite significant investment, the number of individuals living with'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='vision loss might increase to 1.7 billion by 2050, from the 1.1 billion people accomplished in the year 2020. \\nAge-associated macular degeneration (AMD) predominantly strikes the older demographic, resulting in the'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='gradual deterioration of the macula, a crucial part of the retina in charge of the central region of perception. \\nThe consequences of AMD manifest as central vision abnormalities, including blurred or distorted vision, \\nwhich significantly impede various daily activities[4].  \\nAccurate and earlier identification of AMD disease explicitly a vital role in safeguarding irreversible \\ndamage to vision and initiating timely treatment and safeguarding ocular health. Machine  learning'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='techniques have advanced to the point where early identification of aged macular degeneration eye illness \\nby an automated system has significant advantages over manual detection [5] . As aids in diagnosing eye \\ndiseases, digital pictures of the eye  and computational intelligence (CI)-based technologies assist doctors \\nin diagnosis. In the realm of diagnosing eye diseases, digital eye images and computational intelligence'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='(CI)-based technologies serve as indispensable tools, enabling doctors to enhanc e their diagnostic \\ncapabilities[6]. In medical imaging, there are also various approaches are employed including fundus \\nphotography, [7] optical coherence tomography (OCT), and imaging modalities specifically designed for \\nthe eye. These imaging technologie s allow for detailed visualization and analysis of ocular structures,'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='facilitating the identification of characteristic features and abnormalities associated with age -related \\nmacular degeneration diseases.  \\nSeveral researchers have demonstrated a critical task in ophthalmology, facilitating the early detection and \\ndiagnosis of aged macular degeneration (AMD) ocular disease using fundus image.  \\nResearchers have focused on deep learning [8-10], The fields of  vision for computing [11,12]  and the use'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='of predictive learning of machines [13,14] method have used to develop robust classification models to \\nidentify retinal images into AMD disease categories accurately. The incorporation of deep learning \\nmethodologies [57,58] plays a pivotal role in accurately class ifying diverse ocular diseases, thereby \\nensuring the advancement of intelligent healthcare practices in the field of ophthalmology [15].'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Therefore, this paper seeks to demonstrate a novel system employing a AMDNet23 framework, the deep \\nmechanism of CNN and LSTM networks is combined for the automated identification of AMD from fundus \\nphotography. Within this approach, CNN performs the purpose of extracting fundus features, and LSTM \\nundertakes the crucial task of classification AMD constructed on the extracte d features. Internal memory'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='inside the LSTM network empowers it to learning knowledge gained from significant experiences with \\nextended period of condition. In the fully interconnected networks, each layer is linked comprehensively, \\nand the nodes in betwee n layers construction are unconnected, and  LSTM nodes connection within a \\ndirected graph accompined a temporal order, which serves as an input with a specific form [16] . The hybrid'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='two dimensional CNN and LSTM system combination improves classification of AMD ophthalmology and \\nassists clinical decision, the dataset collected from several sources and preprocessing technique for the \\nimage quality enhancemnet to classify AMD efficiently. The assets of this research have been articulated \\nin the following.  \\n \\na) Constructing a combination of CNN -LSTM based AMDNet23 framework for the automated diagnosis'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='of  AMD and aiding clinical physician in the early detection of patients. \\nb) The collection data has investigated by employing the contour -based quality assessmen t technique in \\nidentifying the structure of fundus photography, Ocular illumination levels fundus images are automatically \\neliminated. \\nc) To enhance image quality, CLAHE improves the visibility of subtle details and enhances local contrast'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='and Gamma correction adjusts the intensity levels, improving image quality and facilitating better diagnosis \\nof AMD.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='d) AMDNet23 hybrid framework for detection of AMD utilizing fundus image ophthalmology, data \\ncomprising 2000 images equitively. \\ne) An empirical evaluation is accessible encompassing accuracy, specificity, sensitivity, F1-measure, and a \\nconfusion matrix to assess the effectiveness of the proposed method. \\n \\nThe rest of the contents of this article are arranged a manner as follows: Section II covers the related works'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='of this research. Section III Section III articulates the proposed AMDNet23 methodology, including data \\ncollection and preprocessing techniques, and a comparison of some existing models. Section IV covers the \\nexperimental findings and discussion, inc luding state-of-the-art and transfer learning comparisons. The \\nconclusion is presented in Section V. \\n \\n \\n \\nRelated work \\nIn the pursuit of identifying the ocular disease, researchers have harnessed the power of deep learning'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='techniques. These methods leverage fundus ophthalmology to facilitate the diagnosis of ocular diseases. \\nThis reviewed literature presents cutting -edge systems that developed deep -learning techniques for \\ndetecting AMD, diabetes, and cataracts. \\nM Sahoo et. al[17] proposed an innovative ensemb le-based prediction model called weighted majority \\nvoting (WMV) for the exclusive diagnosis of Dry -AMD. This approach intelligently combines the'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='predictions from various base classifiers, utilizing assigned weights to each classifier. The WMV model \\ndemonstrates remarkable accuracy, achieving 96.15% and 96.94% accuracy rates, respectively. P \\nMuthukannan et. al. [18] introduced a computer aided approach that leverages the Flower pollination \\noptimization approach (FPOA) in accompanied with a CNN mechanism for preprocessing, specifically'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"utilizing the maximum entropy transformation on the ODIR public dataset. The model's performance was \\nthen benchmarked against other optimized models, demonstrating superior accuracy at 95.27%. In a study \\nby Serener et al. [19], their goal was to employ OCT images and deep neural networks to detect both dry \\nand wet AMD. Regarding the purpose, two architectures—AlexNet and ResNet—were used. The outcomes\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"revealed that the eighteen -layer ResNet model correctly identified AMD with an astounding accuracy of \\n94%, whereas the AlexNet model produced an accuracy of 63%. \\nThere are several deep learning methods for detecting cataracts, because of the drawbacks of feature \\nextraction and preprocessing, these methods don't always produce adequat e results. Kumar et al. [20] \\nproposed several models to improve clinical decision -making for ophthalmologists. Paradisa et al. [21]\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Fundus images applied the Concatenate model with For feature extraction, Inception -ResNet V2 and \\nDenseNet121 are implemented , and MLP is deployed for classification and average accuracy was 91%. \\nFaizal et al.  [22] An automated cataract detection algorithm using CNN achieves high accuracy (95%) by \\nanalyzing visible wavelength and anterior segment images, enabling cost -effective early detection of'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='various cataract types. Pahuja et al. [23]  To enhance the model performance, data augmentation and \\nmethods to extract features have been performed. Therefore they used CNN and SVM models for the \\ndetection of cataract on a dataset compr ising normal and cataract retinal images, achieving high accuracy \\nof 87.5% for SVM and 85.42% for CNN. Hence, et al. [24] used a CNN model to diagnose cataract'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='pathology with digital camera images. The model achieves high accuracy (testing: 0.9925, training: 0.9980) \\nwhile optimizing processing time. It demonstrates the potential of CNNs for cataract diagnosis. Although \\net al [25] used color fundus images to detect cataracts.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"A variety of computer vision engineering approaches are used to forecast the Diabet ic retinopathy (DR)'s \\noccurrences and phases automatically. Mondal et al. [26] Their model is a collaborative deep neural system \\nfor automated diabetes-related retinopathy (DR) diagnosis and categorization using two models: modified \\nDenseNet101 and ResNeXt. Experiments were conducted on APTOS19 and DIARETDB1 datasets, with\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='data augmentation using GAN-based techniques. Results show higher accuracy with accuracy for each of \\nthe five classes reached 86.08%, whilst for each of the two classes the score was 96.98%. Whereas, a ML-\\nFEC model with pre-trained CNN architecture was proposed for Diabetic Retinopathy (DR) detection using \\nResNet50, ResNet152, and SqueezeNet1. On testing with DR datasets, ResNet50 achieved 93.67%,'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='SqueezeNet1 achieved 91.94%, and ResNet152  achieved 94.40% accuracy, demonstrating its suitability \\nfor clinical implementation and large-scale screening programs. Using a novel CNN model, Babenko et al. \\n[27] were able to multi-class categorize retinal fundus pictures from a publically accessible dataset with an \\naccuracy of 81.33% for diabetic eye disease. Priorly  based on UNet architecture, et al. [28] achieved'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='95.65% accuracy in identifying red lesions and 94% accuracy in classifying DR levels of severity. The \\napproach was examined utilizing publically accessible datasets: IDRiD (99% specificity, 89% sensitivity) \\nand MESSIDOR (94% accuracy, 93.8% specificity, 92.3% sensitivity). \\nMethodology \\nAging macular degenerative disorder (AMD) is an advancing retinal condition that predominantly impacts'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='individuals over the age of 50. This eye disease can significantly affect eyesight, leading to various visual \\nissues like blurred or distorted vision, where straight lines might appear wavy or twisted. Moreover, it \\ncauses a loss of central vision, the emergence of dark or empty spots at the center of vision, and alterations \\nin color perception. Thus taking proactive steps to prevent eye diseases is crucial for maintaining clear and'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='vibrant sight throughout our lives. In recent years, neural networks containing layers of convolution (CNNs) \\nhave demonstrated considerable potential in the processing of medical images. It has the remarkable \\ncapacity to recognize and extract meaningful features from images automatically.  Figure 1 outlines the \\nsteps in developing the proposed CNN-based methods for AMD eye disease detection:'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Figure 1:  Overall proposed-based Method \\nA. Data Collection: \\nThe case of normal class represents the absence of any specific eye disease or condition. A healthy eye \\nfunctions optimally, providing clear and unimpaired vision.  Diabetes, a systemic disease characterized by \\nelevated blood sugar levels, can lead to various ocular complications. Diabetic ocular disorders, notably \\ndiabetic retinopathy, occur when the blood vessels found in the retina undergo damage as a consequence of'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='elevated blood sugar concentration [29]. Older individuals are predominantly affected by age -associated \\nmacular degeneration (AMD) and involves the progressive deterioration of the macula, a small but crucial \\nof the core vision-related region of the retina. AMD can lead to blurred or distorted central vision, impacting \\nactivities [30]. A cataract is another common eye condition, particularly associated with aging. It involves'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='clouding the crystalline lens inside the eye, leading to inconsistent or foggy vision [31]. Fig. 2. Shows the \\nsample images of Normal, Cataract, AMD and Diabetes respectively.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Figure 2: Types of fundus ophthalmology \\nTo train a robust CNN model, a diverse and well -annotated dataset of AMD and non-AMD eye images is \\nessential. The dataset employed in this study containing a total of 2000 images, was put together by \\nassessing the quality of the images from six other public datasets. Those datasets are ODIR[32], DR -\\n200[33], Fundus Dataset[34] , RFMiD[35], ARIA[36], and Eye_Diseases_Classification[37]. From these'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='datasets. The quality assessment was done using contour techniques[38]. The contour -based approach \\nfocuses on the sharpness and clarity of edges, as they play a crucial role in human as sess the quality of \\nimagery. Which included illumination level, visibility structure, color and contrast, and direct eye image. \\nFigure 3 represents a sample of the assessed image quality. \\n \\nFigure 3:  Contour-based approach'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"Here it can be seen that sharp, well-defined edges contribute to high -quality images, while blurry or \\ndistorted edges indicate poor quality. Such poor -quality images would negatively impact the machine's \\nperception. We put together a dataset consisting of four classes: Normal, Diabetes, AMD, and Cataract,\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='where each class contains 500 images. Table 1 indicates the quantity of accessible and selected images \\n(within the first bracket) from those six public datasets. \\nDatasets Normal Diabetes AMD Cataract \\nODIR[33] 2873(168) 1608(300) 266(266) 293(256) \\nDR-200[34] 1000(332) 1000(150)   \\nFundus Dataset[35]   46(46) 100(44) \\nRFMiD[36]  376(50) 100(100)  \\nARIA[37]   101(88)  \\nEye_Diseases_Classification[38]    1038(200) \\n \\nB.Data Pre-processing:'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Preprocessing, which is the strong suits of the prop osed work, was focused on enhancing image quality, \\nand some of the preprocessing techniques applied to this work were not used by the previously proposed \\ncataract disease detection works. The data were preprocessed in different color spaces (as shown in Figure \\n4) for extracting the features while bettering the practicability of our models. Among the RGB(G), HSV(V),'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='and LAB(L) color spaces, the vessels were visible in the LAB(L) color space. As a result, the LAB(L) color \\nspace was chosen.  \\n \\nFigure 4: Color spaces \\nLater, some preprocessing algorithms like CLAHE and Gamma correction[39] were applied to enhance \\nimage quality by adjusting the brightness and contrast. We experimented with several parameters for these \\nalgorithms and finally got the satisfying result for CLAHE(2.0, (8,8)). For gamma values of 0.5, the image'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='is found to become darkened. Moreover, for gamma values of 2.0, the image is found quite faded. To \\novercome this problem CLAHE is used to enhance regional contrasting, making the image more visua lly \\nappealing and informative[40]. Figure 5 shows the resulting images for our experimented algorithms along \\nwith the finalized CLAHE (2.0, (8,8)) for our model.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Figure 5: Gamma and CLAHE based quality enhancement \\nHistogram comparison between the applied Context-limited adapted equalization of histograms (CLAHE) \\npreprocessing algorithm and a normal image in Figure 6 can help illustrate the effects of CLAHE on \\nenhancing local contrast and improving image quality.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Figure 6: Hitogram comparisons \\nBy comparing the histograms, we can observe the changes in pixel intensity distribution before and after \\napplying CLAHE. In the normal image, the histogram exhibits a relatively uniform distribution of pixel \\nintensities, with some variations depending on th e content of the image. CLAHE is implemented as a \\ncomponent in the preprocessing, it adapts the contrast enhancement locally, making it particularly effective'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='in improving the contrast of regions with varying intensities. This helps reveal hidden details a nd textures \\nthat might have been obscured in the original image. \\n \\nImages MSE PSNR SSIM \\n2376_left.jpg 2388.13 14.35 0.48 \\n84_right.jpg 2189.98 14.72 0.65 \\n980_right.jpg 927.54 18.45 0.53 \\n71_left.jpg 709.37 19.62 0.54 \\n \\nThe effectiveness of the quality of image preprocessing, Table 2 displays the readings of the metrics mean'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='square errors (MSE), Peak Signal -to-Noise Ratio (PSNR), and Structures Similarity Index (SSIM) [41].'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"These metrics compare the preprocessed image to the original image to determine the level of distortion or \\nsimilarity. \\nMean Squared Error (MSE ): MSE generates  the resultant mean squared disparity between the \\npreprocessed and original image's pixel values. Lesser MSE reading indicate greater similarity between the \\nimages. MSE is calculated using the formula:  \\n𝑀𝑆𝐸 = 1\\n𝑚∗𝑛 ∑ ∑[𝐼(𝑥,𝑦)−𝐾(𝑥,𝑦)]2\\n𝑛−1\\n𝑦=0\\n𝑚−1\\n𝑥=0\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"𝑛−1\\n𝑦=0\\n𝑚−1\\n𝑥=0\\n \\nwhere𝑚∗𝑛 represents the image dimensions, 𝐼(𝑥,𝑦) and 𝐾(𝑥,𝑦) indicate preprocessed image’s pixel \\nvalues and original images at coordinates(𝑥,𝑦). \\nPeak Signal-to-Noise Ratio (PSNR):  The optical appealing of preprocessed photographs is frequently \\nassessed utilizing the PSNR measure. It calculates a measure of the peak power ratio of the signal's strength \\nto noise, which assessed in decibels (dB). Increased PSNR values indicated a greater similarity between the\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='images. PSNR is calculated using the formula: \\n𝑃𝑆𝑁𝑅 = 10𝑙𝑜𝑔10\\n𝑀𝐴𝑋2\\n𝑀𝑆𝐸  \\nwherein MAX is the highest pixel value that is permitted to be used (for example, 255 in 8-bit photographs). \\nStructural Similarity Index (SSIM): SSIM evaluates the luminosity, contrary, and structural similarities \\nbetween the preprocessed image and original image. The value of 1 denotes complete similarity, using \\nSSIM readings varying from -1 to 1. Higher SSIM values indicate better similarity between the images.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='SSIM is calculated using a combination of mean, variance, and covariance of the image patches. \\n𝑆𝑆𝐼𝑀 = (2𝜇𝑥𝜇𝑦 +𝑐1)(2𝜎𝑥𝑦 +𝑐2)\\n(𝜇𝑥2 +𝜇𝑦2 +𝑐1)(𝜎𝑥2 +𝜎𝑦2 +𝑐2) \\nwhere c1 and c2 are constants to prohibit division by zero, σ and μ stand representing the standard deviations \\nand mean respectively. \\nBy calculating MSE, PS NR, and SSIM before and after image preprocessing, The effectiveness of the'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='preprocessing techniques in preserving image quality or reducing noise, artifacts, or other undesired effects \\ncan be determined. Lesser MSE readings, greater PSNR readings, and greater SSIM values indicate better \\nimage quality preservation. \\n \\n \\nC. Comparison of some existing models  \\nTransfer Learning: \\nIn this research study, a handful of models were trained and evaluated. Some of those models are discussed \\nbelow in the following sections: \\n \\ni.ViTB16'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='The ViTB16 model, also known as Vision Transformer Base with a depth of 16 layers, is a deep learning \\narchitecture specifically designed for image classification tasks [42]. 224*224 pixels were made up of the \\nsize of the input images. A grid  of patches containing fixed sizes is used to divide the input image. Each \\nindividual patch is positioned linearly to obtain a lower-dimensional representation. The patch embeddings'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='are enhanced by employing positional encoding to provide the model with spatial information. The model \\nis capable of finding relationships dependencies between different patches thanks to a self -attention \\nmechanism. It calculates attention scores between all pairs of patches and applies weighted averaging to \\naggregate information. Layer normalization is applied after the self -attention mechanism to normalize the'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='output and improve training stability. SGD (Stochastic Gradient Descent) optimizer was employed for \\ntraining the model, and the learning rate was 0.0001. \\nii.DenseNet121, DenseNet169:  \\nDenseNet[43] is a famous deep -learning architecture known for its dense connections between layers, \\nenabling effective feature reuse and alleviating the vanishing gradient problem. DenseNet121 and \\nDenseNet169 have 121 and 169 layers, respectively, making DenseNet121 a relatively shallow variant than'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='DenseNet169. DenseNet121 has fewer parameters compared to DenseNet169, which makes it more \\nmemory-efficient and faster to train. DenseNet121 performs well on various image classification tasks but \\nmay not capture as fine -grained features as deeper models. DenseNet169 performs better than \\nDenseNet121, especially when the dataset is larger and more complex. Choosing between DenseNet121 \\nand DenseNet169 for a particular purpose like AMD classification, it is essential to consider the size and'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='complexity of the dataset. Since the dataset used in this study was small, DenseNet121 should have been \\nthe model to pick, but we experimented with all DenseNet variants. \\niii. InceptionResnetV2 \\nA powerful convolutional neural network conception that incorporates the Inception and ResNet \\nmodules is termed the InceptionResNetV2 system [44]. It was proposed as an extension to the \\noriginal Inception and ResNet models designed to improve image classification efficient tas ks.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='The InceptionResNetV2 model was initialized with pre -trained ImageNet weights, excluding the \\ntop classification layers. The pre -trained layers were frozen to prevent them from being updated \\nduring training. The Adam optimizer was utilized for bettering  the model and the training was \\ndone with an epoch size of 100. \\nD. AMDNet23: \\nAMDNet23 Combining CNNs which are convolutional neural systems and long -term short-term memory'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='networks (LSTM) is designated as CNN -LSTM where the strengths of CNNs in image featu re extraction \\nare combined with the temporal modeling capabilities of LSTM networks[45]. Before feeding the images \\nto the model, employed a diverse set of augmentation techniques to enhance the training data. These \\nincluded randomized horizontal and vertical flipping through a probability of fifty percent each and applied'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='random brightness adjustments by varying the brightness level within a range of -0.1 to +0.1. To further \\nincrease variation, used random contrast adjustments with factors ranging from 0.8 to 1.2, as well as random \\nsaturation adjustments within the bounds of 0.8 to 1.2. Moreover, introduced random hue adjustments to \\nadd subtle color variations. Lastly, to augment the dataset further, performed translation -based width and'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"height shifting with a specific range to the input images. The augment strategy emphasizes data diversity \\nand improves the model's broad ability to diagnose unknown data[46]. The model is designed with a depth \\nof 23 layers.\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"Input Layer: The input to the AMDNet23 model is a collection of eye images captured from patients. The \\ninput images are represented as a tensor X with dimensions (N, W, H, C), where N corresponds to the eye \\nimage’s number, and W and H represent The width and length of the images(The model received imagery \\nthat measured 256 X 256 in size.) respectively. C denotes the number of color channels in the eye images. \\nThis tensor X is then passed into the model's input layer.\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Convolutional Layers (CNN): After the initial input layer, the CNN component of the mod el comprises \\nmultiple convolutional layers[47]. Every individual layer of convolution utilizes a set of adjustable filters \\nto process the input images. The resulting Features of the map from the 𝑖𝑡ℎ convolutional layer are denoted \\nas 𝐹𝑖, with 𝑖 ranging from 1 to 𝑛. The output feature map 𝐹𝑖 can be computed as follows: \\n𝐹𝑖 = 𝐶𝑜𝑛𝑣2𝐷(𝑋,𝑊𝑖)+𝑏𝑖'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"Where Conv2D refers to the convolution operation, 𝑊𝑖 represents the tr ainable parameters (weights) \\nspecific to the 𝑖𝑡ℎ convolutional layer, and 𝑏𝑖 represents the corresponding biases associated with that layer. \\nThe output feature maps 𝐹𝑖 have spatial dimensions (W', H') and C' channels. \\nThe model contained six  convolutional blocks. The first four convolutional blocks consisted of 2\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='convolutional layers and 1 batch normalization layer each. The filters were 32, 64, 128, and 256 for the first \\nfour blocks, where the kernel size was 3 X 3. The fifth and sixth convo lutional blocks consisted of 3 \\nconvolution layers and 1 batch normalization layer. All the convolution layers of the fifth and sixth blocks \\nconsisted of 512 filters. \\nPooling Layers:  After the layers based on convolution, layers of pooled [48] are frequentl y used to'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"downsample the feature maps. Let us denote the output feature maps after pooling as 𝑃𝑖, where 𝑖 ranges \\nfrom 1 to 𝑝 (the total number of pooling layers). Each pooling layer performs a downsampling operation \\non the input feature maps. After applying all pooling layers, the resulting feature maps can be denoted as \\n𝑃𝑝 and have spatial dimensions (W'', H'') and C'' channels. The pool size for max-pooling layers was 2 X 2 \\nfor all the convolutional blocks.\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"LSTM Layer: The system conveys the development and significance of networks having long-term short-\\nterm memory (LSTM), an advancement residing in conventional Recurrent neural networks to delve into \\nthe motivation behind LSTM's creation, specifically to address the vanishing gradient problem , which \\nformerly hindered the effective training of RNNs on long sequences [49]. Behind LSTM it introduces\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='memory cells, enabling the network to retain information over extended periods. The mechanism empowers \\nLSTMs in effectively capturing long-term dependencies within the input data. The cell state adds a long -\\nterm memory to flows the entire sequence [50]. It enables information to be retained or discarded selectively \\nutilizing the input entrance, forget gatekeeper, and output gateway, which constitute th e three main gates.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='The LSTM cell computations can be mathematically represented as follows, where 𝑡 denotes the current \\ntime step, 𝑥𝑡 rrepresents what was the input entered at time 𝑡, ℎ𝑡denotes the previous hidden stated, and \\n𝑐𝑡signifies the cell state: \\n \\n \\n \\n𝑖𝑡 = 𝜎(𝑊𝑖[𝑥𝑡,ℎ𝑡−1]+𝑏𝑖)...(1)'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='𝐶 𝑡 = 𝑡𝑎𝑛ℎ(𝑊𝑐[𝑥𝑡,ℎ𝑡−1]+𝑏𝑐)...(2) \\n𝐶𝑡 = 𝑓𝑡𝐶𝑡−1 +𝑖𝑡𝐶 𝑡...(3) \\nThe input gate (1) uses a sigmoid function to combine the previous output ℎ𝑡−1and the present time input \\n𝑥𝑡, deciding the proportion of information to be incorporated into the cell state and (2) employes to obtain \\nnew information through the tanh layer to be added into current cell state 𝐶 𝑡. The current cell state 𝐶 𝑡, and \\nlong term information 𝐶𝑡−1 are combination into 𝐶𝑡(3) whereas 𝑤𝑖 determines the sigmoid output and 𝐶 𝑡'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='determines to tanh output. “Forget” gate (4) investigates how much of the previous cell state should be \\nretained and carried over to the next time step by assessing probability where 𝑊𝑓 and 𝑏𝑓 refers to the offset \\nand weight matrix and offset respectively. \\n𝑓𝑡 = 𝜎(𝑊𝑓[𝑥𝑡,ℎ𝑡−1]+𝑏𝑓)...(4) \\nThe output gate of the LSTM investigates by ℎ𝑡−1 and 𝑥𝑡inputs following (4) and (5) passed through the'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='activation function to determine what portion of information to be appeared from the current LSTM unit at \\ntimestamp t. \\n𝑜𝑡 = 𝜎(𝑊𝑜[𝑥𝑡,ℎ𝑡−1]+𝑏𝑜)...(5) \\nℎ𝑡 = 𝑜𝑡𝑡𝑎𝑛ℎ(𝐶𝑡)...(6)  \\nIn the above equation, 𝑊𝑜refers to the matrices of the output gate and 𝑏𝑜refers LSTM bias respectively.  \\nOutput Layer: The output layer delivers the ultimate prediction regarding the existence or non -existence \\nof AMD in the input eye images. We can represent the input tensor to the output layer as 𝐻𝑜𝑢𝑡, obtained by'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='reshaping 𝐻𝑙𝑠𝑡𝑚 to have dimensions (NT, D). A function of activation throughout softmax is positioned \\nfollowing its dense layer to the output section. The dense layer takes the input 𝐻𝑜𝑢𝑡 and transforms it to \\ngenerate the output tensor Y, which has dimensions (NT, K). Here, K specifies the number of output classes \\nthe model is classifying.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Figure 7: CNN-LSTM system \\nThe AMDNet23 model (Figure 7) for AMD ocular disease detection leverages the complementary strengths \\nof CNNs in spatial feature extraction and LSTMs in modeling sequential dependencies.  \\nLayer Type Kernel Size Kernel Input Size \\n1 Convolution2D 3 X 3 32 256 X 256 X 3 \\n2 Convolution2D 3 X 3 32 256 X 256 X 32'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='3 Maxpooling2D 2 X 2 - 256 X 256 X 32 \\n4 Convolution2D 3 X 3 64 128 X 128 X 32 \\n5 Convolution2D 3 X 3 64 128 X 128 X 64 \\n6 Maxpooling2D 2 X 2 - 128 X 128 X 64 \\n7 Convolution2D 3 X 3 128 64 X 64 X 64 \\n8 Convolution2D 3 X 3 128 64 X 64 X 128 \\n9 Maxpooling2D 2 X 2 - 64 X 64 X 128 \\n10 Convolution2D 3 X 3 256 32 X 32 X 128 \\n11 Convolution2D 3 X 3 256 32 X 32 X 256 \\n12 Maxpooling2D 2 X 2 - 32 X 32 X 256 \\n13 Convolution2D 3 X 3 512 16 X 16 X 256 \\n14 Convolution2D 3 X 3 512 16 X 16 X 512'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='15 Convolution2D 3 X 3 512 16 X 16 X 512 \\n16 Maxpooling2D 2 X 2 - 16 X 16 X 512 \\n17 Convolution2D 3 X 3 512 8 X 8 X 512 \\n18 Convolution2D 3 X 3 512 8 X 8 X 512 \\n19 Convolution2D 3 X 3 512 8 X 8 X 512 \\n20 Maxpooling2D - - 8 X 8 X 512 \\n21 LSTM - - 16 X 512'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='22 FC - 64 524352 \\n23 Output - 4 260 \\nIn this research, An innovative and novel technique was devised to automatically detect AMD by \\nleveraging four distinct types of fundus images. This unique architecture synergizes the power of \\nNeural Networks of Convolutional and Long Short -Term Memory. The CNN module is \\nadministered for extracting intricate features from fundus imaging, and the LSTM module serves \\nas the classifier. The proposed AMDNet23 hybrid network for AMD detection consists of 23'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"layers: It includes 14 convolutional layers placed and 6 layers used for pooling,one fully \\ninterconnected  layer of (FC), a layer of LS TM and a single output layer with a sense of softmax \\nfunctionality. In our construction, an individual convolution block is made comprising between \\ntwo or three 2 -dimensional CNN's, A layer with a level of pooling and a layer comprising a \\ntwentieth percent dropout rate have of dropouts. Utilizing a convolutional layer with 3x3 kernels\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='and the ReLU function, the feature extraction is carried out efficiently. The input image undergoes \\ndimension reduction using A layer for maximum pooling of 2 × 2 kernels. The  resulting output \\nstructure was discovered (none, 4, 4, 512). the input size inside the layer of LSTM transforms to \\n(16, 512) whenever incorporating the reshaped approach. Combining these two neural network \\narchitectures allows the model to effectively ana lyze eye images, capturing both local spatial'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='patterns and temporal relationships, ultimately enabling accurate AMD diagnosis. The \\nsummarized architecture is presented in Table 2. \\nEvaluation Criteria \\nIn this study, as many as 13 models were experimented an d the evaluation of all those models will be \\npresented in this section of the paper. Considering the following evaluation criteria, the performance, \\nreliability, and clinical relevance of a AMD detection system can be assessed and also can be determined'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"its suitability for assisting medical professionals in accurately detecting and diagnosing AMD. \\nAccuracy: The accuracy of the AMD detection system in correctly classifying images as AMD, diabetes, \\ncataracts is a crucial evaluation criterion. It evaluates the  correctness of the system's detection computed \\noverall. \\n𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = 𝑇𝑃 +𝑇𝑁\\n𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁 \\nPrecision: The proportion of properly identified AMD situations is examined to measure precision out of \\nall predicted AMD cases.  \\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝑇𝑃\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝑇𝑃\\n𝑇𝑃 +𝐹𝑃 \\nSensitivity and Specificity : Sensitivity is typically referred to by the term the true positive rate, which \\ngauges the system to identify AMD cases correctly. True negative rate, which is often referred to as \\nspecificity, assesses its capacity of identifying non-AMD conditions. Both metrics provide insights into the \\nsystem's performance in different classes and help assess its ability to avoid false positives and false \\nnegatives.\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦 = 𝑇𝑃\\n𝑇𝑃+𝐹𝑁 \\n𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦 = 𝑇𝑁\\n𝑇𝑁+𝐹𝑃 \\nF1 Score: The F1 score offers a comprehensive measurement that addresses the balance between precision \\nand memory and thus represents a harmonious average of precision and recall. It is advantageous in realities \\nwhereby there occurs a disparity in class or in cases when the costs of false positives and false negatives \\nfluctuate. \\n𝐹1𝑆𝑐𝑜𝑟𝑒 = 𝑇𝑃\\n𝑇𝑃+1\\n2(𝐹𝑃+𝐹𝑁)'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='𝑇𝑃+1\\n2(𝐹𝑃+𝐹𝑁)\\n \\nWhen evaluating model performance, it is crucial to consid er a combination of these evaluation criteria \\naccordance with the precise specifications of the completion and the area of expertise. Selecting appropriate \\nmetrics and interpreting the results will help determine the effectiveness and suitability of the model. Table \\n2 showcases the retained value for these evaluation metrics: \\nModel Accuracy Precision Sensitivity Specificity F1 Score'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='ViTB16 95.25% 95.26% 95.25% 98.66% 95.24% \\nMobileViT_XXS 83.00% 82.58% 83.00% 98.28% 82.41% \\nInceptionResNetV2 47.50% 36.26% 47.50% 82.67% 40.50% \\n EfficientNetB7 92.75% 92.94% 92.75% 98.97% 92.62% \\n EfficientNetB6 92.75% 92.74% 92.75% 97.00% 92.69% \\nDenseNet121 82.25% 82.41% 82.25% 95.00% 81.46% \\nDenseNet169 81.25% 81.24% 81.25% 92.74% 80.31% \\nDenseNet201 84.75% 84.45% 84.75% 93.46% 84.52% \\nInceptionV3 72.25% 71.53% 72.25% 90.16% 70.71% \\nMobileNetV2 71.75% 71.68% 71.75% 88.01% 71.19%'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='VGG16 89.75% 89.82% 89.75% 94.68% 89.78%'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='VGG19 89.00% 88.96% 89.00% 95.62% 88.80% \\nResNet50 93.25% 93.37% 93.25% 96.73% 93.17% \\nAMDNet23 96.50% 96.51% 96.50% 99.32% 96.49% \\nIn conclusion, the AMDNet23 model for AMD ocular disease detection demonstrated strong performance \\nin accurately identifying AMD from eye images. Its high accuracy, precision, and recall values, along with \\nthe robust AUC-ROC score, validate its potential as a reliable tool for early detection and intervention. The'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"model's efficiency makes it suitable for practical deployment in healthcare settings, contributing to \\nimproved patient care and timely treatment of AMD retinal disease. \\nResult & Discussion: \\nIn this undermentioned portion, the findings of the proposed mechanism along with the comparison with \\nsome cutting-edge studies will be comprised. The collected data were divided into sets for conducting\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='training and testing to constr uct and examine the proposed system. This approach was initially trained to \\nleverage 80% of the data and evaluated utilizing 20% of the collected information. To ensure enhanced \\nproductivity several sets of parameters were experimented. The parameter setting that provided us with the \\nmost advantageous outline for the proposed model is given below \\n \\n Batch size 32 \\n Epochs 100  \\n Learning rate 0.001  \\n Decay rate 0.95  \\nDecay step 1'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Decay step 1 \\n \\nFigure 5 and Figure 6 show both training and test sets of data, the accuracy and loss curves. The graphical \\nrepresentations of epoch versus accuracy and epoch versus losses are valuable insights for monitoring and \\nunderstanding the progress of the proposed method.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"The graph exhibits how the model's accuracy varies while an increasing amount of training epochs raises. \\nThe predictability of the model on either a set of training data or a testing set appears on the axis in the \\nvertical direction, along with the number of epochs denoted throughout the horizontal direct ion. It is \\nobserved from Figure 5 reveals the overall number of epochs rises over training, the model's accuracy\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='elevates as it learns from the training data. At first, the accuracy continues to improve with each epoch, it \\nsuggests that the model can benef it from additional training. Later the accuracy eventually plateaus. This \\nindicates that the model has converged and further training may not significantly improve accuracy.  \\nThe epoch vs loss graph demonstrates the association between the process of train ing number of epochs'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"and the loss or error of the model. The loss of performance is a disparity among the estimated of model \\noutline and the intended outline. The loss level is portrayed through the y-axis, whereas the total amount of \\nepochs is displayed through the x-axis. In Figure 6, the loss is seen initially high as the model makes random \\npredictions. The training messages, the loss decreases, reflecting the model's improved performance and \\nability to make more accurate predictions.\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"Figure X a represents The AMDNet23 model's confusion matrix, resulting in was determined on the basis \\nof the ablation investigation, Adam, and learning rate, experiences the greatest level of accuracy and is \\nconfigured in a great potential manner.\"), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='The row of values reflect s what is actually labeled \\nattached to the images, while the column-specific values \\nreveal the quantities provided the predictive model \\nestimates. The diagonal values indicate correct \\npredictions (TP). However, the model had the great \\nresults for AMD. Acco rding to the confusion matrix, \\nTwo of the AMD pictorials was mistakenly classified as \\ncataract-related and diabetes, whereas 98 among the 100 \\nAMD investigations possessed effectively diagnosed.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Next, 99 among a possible 100 anticipated involving \\ncataracts taught correctly where one as AMD. Of 100 \\ndiabetes images, Seven images had been mistakenly \\nidentified, involving 3 referred to as AMD along with 4 \\nas belonging to the healthy class, exposing of 93 \\ncorrectly assigned.  In closing least, among the 100 \\nnormal images, 96 were perfectly identified, and 1 had \\nbeen misinterpreted as images related to AMD and 3 associated to diabetes.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='State-of-the-art work comparison \\nTable 3 provides a concisely summarizes the main approaches in the existing literature for diagnosing AMD \\ndisease and our proposed method. These approaches primarily involve conventional methods and deep \\nlearning algorithms, which utilize retinal images for diagnosis. \\n \\nAuthor Year Method No. of images Accuracy \\n \\nTK Yoo et. al. [51] \\n \\n2018 \\n \\nVGG19-RF \\n \\n3000 \\n3- Class \\n95% accuracy \\n \\nHuiying Liu et. al [52] \\n \\n2019 \\n \\nDeepAMD \\n \\n4725'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='DeepAMD \\n \\n4725 \\n6- Class \\n70% accuracy \\nFelix Grassman et. al. [53]  \\n2020 \\nEnsemble \\nnetworks net \\n \\n3654  \\n13- class \\n63% accuracy'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='N Chea and Y Nam [54] \\n \\n2021 \\nOptimal residual \\ndeep neural \\nnetworks  \\n \\n2335 \\n4- Class \\n85.79% accuracy \\n \\nC Domínguez et. al [55] \\n \\n2023 \\nTransformer-based \\nsystem \\n \\n4896 \\n3-Class \\n82.55% accuracy \\n \\nP Zang et. Al [56] \\n \\n2023 \\nDeep-Learning \\nbased aided \\nsystem \\n \\nNot specified \\n4-Class \\n80% accuracy \\n \\nProposed Method \\n \\n2023 \\n  \\nAMDNet23 \\n \\n2000 \\n4- Class \\n96.5% accuracy \\n \\nThe AMDNet23 model proposed in the study achieves a high accuracy rate of 96.5%, surpassing'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='other state-of-the-art works currently available. As a result, It can be presumed that this proposed \\nmethod is effective for early -stage detection and diagnosis of AMD, and this novel method also \\ndiagnoses Cataracts and diabetic retinopathy utilizing fundus ophthalmology datasets, \\ndemonstrating superior accuracy. \\nComparison of the AMDNet23 model with the transfer Learning models: \\nThe outline demonstrated and effectively potential of hybrid AMDNet23 network for precisely'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='detecting AMD eye disease from images. The capacity of the model to precisely detect AMD \\ninstances is demonstrated by the excellent precision, accuracy, recall and F1 -score a cquired.  \\nCombination of CNNs and LSTMs allows for the extracting of both spatial and temporal features, \\ncapturing the subtle patterns and changes associated with AMD. Figure 5 exhibits the comparison \\nof performance between the model we proposed and a several pre-trained prepared.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Conclusion \\nIn essence, this study proposed a AMDNet23 model for detecting and diagnosing AMD disease using \\nseveral image datasets. The model achieved a high accuracy rate of 96.5%, surpassing other state -of-the-\\nart works in the field. Furthermore, when compared with pre -trained models, the novel deep AMDNet23 \\nmethod also showed superior accuracy for AMD detection, and the system is efficient to diagnose cataracts'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='and diabetic retinopathy respectively. In the future, incorpora ting additional modalities or features can \\npotentially enhance the performance of AMD detection models. Combining fundus images with other \\nclinical data, which could include patient demographics, health records, or genetic information, may \\nimprove accuracy and enable a more comprehensive awareness of the disease. In broadly , the findings of'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='this research clearly demonstrates effectively of the proposed AMDNet23 model in accurately detecting \\nand diagnosing AMD cases. This model holds promise for early detect ion and diagnosis of AMD ocular \\ndisease, which could assist clinicians and aid in timely intervention and treatment for affected individuals. \\n \\n \\nREFERENCES \\n \\n1. Ackland, P., Resnikoff, S., & Bourne, R. (2017). World blindness and visual impairment:'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='despite many successes, the problem is growing. Community eye health, 30(100), 71.  \\n2. “Vision Impairment and Blindness.” World Health Organization, 13 Oct. 2022, \\nwww.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment.  \\n3. “Vision Atlas.” The International Agency for the Prevention of Blindness, 4 Jan. 2023, \\nwww.iapb.org/learn/vision-atlas/.  \\n4. “Common Eye Disorders and Diseases.” Centers for Disease Control and Prevention, 19'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Dec. 2022, www.cdc.gov/visionhealth/basics/ced/index.html.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='5. Sarki, R., Ahmed, K., Wang, H., & Zhang, Y. (2020). Automatic detection of diabetic eye \\ndisease through deep learning using fundus images: a survey. IEEE access, 8, 151133-\\n151149.  \\n6. Kumar, S. M., & Gunasundari, R. (2023). Computational intelligence in eye disease \\ndiagnosis: a comparative study. Medical & Biological Engineering & Computing, 61(3), \\n593-615.  \\n7. Iqbal, Shahzaib, et al. “Recent Trends and Advances in Fundus Image Analysis: A'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Review.” Computers in Biology and Medicine, vol. 151, 2022, p. 106277, \\nhttps://doi.org/10.1016/j.compbiomed.2022.106277.  \\n8. Tan, J. H., Bhandary, S. V., Sivaprasad, S., Hagiwara, Y., Bagchi, A., Raghavendra, \\nU., ... & Acharya, U. R. (2018). Age-related macular degeneration detection using deep \\nconvolutional neural network. Future Generation Computer Systems, 87, 127-135.  \\n9. Sogawa, T., Tabuchi, H., Nagasato, D., Masumoto, H., Ikuno, Y., Ohsugi, H., ... &'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Mitamura, Y. (2020). Accuracy of a deep convolutional neural network in the detection of \\nmyopic macular diseases using swept-source optical coherence tomography. Plos one, \\n15(4), e0227240.  \\n10. Serener, A., & Serte, S. (2019, April). Dry and wet age-related macular degeneration \\nclassification using oct images and deep learning. In 2019 Scientific meeting on \\nelectrical-electronics & biomedical engineering and computer science (EBBT) (pp. 1-4). \\nIEEE.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='IEEE. \\n11. Yim, J., Chopra, R., Spitz, T., Winkens, J., Obika, A., Kelly, C., ... & De Fauw, J. (2020). \\nPredicting conversion to wet age-related macular degeneration using deep learning. \\nNature Medicine, 26(6), 892-899. \\n12. Schmidt-Erfurth, U., Waldstein, S. M., Klimscha, S., Sadeghipour, A., Hu, X., Gerendas, \\nB. S., ... & Bogunović, H. (2018). Prediction of individual disease conversion in early \\nAMD using artificial intelligence. Investigative ophthalmology & visual science, 59(8),'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='3199-3208.  \\n13. Schmidt-Erfurth, U., Bogunovic, H., Sadeghipour, A., Schlegl, T., Langs, G., Gerendas, \\nB. S., ... & Waldstein, S. M. (2018). Machine learning to analyze the prognostic value of \\ncurrent imaging biomarkers in neovascular age-related macular degeneration. \\nOphthalmology Retina, 2(1), 24-30.  \\n14. Bogunović, H., Montuoro, A., Baratsits, M., Karantonis, M. G., Waldstein, S. M., \\nSchlanitz, F., & Schmidt-Erfurth, U. (2017). Machine learning of the progression of'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='intermediate age-related macular degeneration based on OCT imaging. Investigative \\nophthalmology & visual science, 58(6), BIO141-BIO150.  \\n15. Jaiswal, A. K., Tiwari, P., Kumar, S., Al-Rakhami, M. S., Alrashoud, M., & Ghoneim, A. \\n(2021). Deep learning-based smart IoT health system for blindness detection using \\nretina images. IEEE Access, 9, 70606-70615.  \\n16. Wang, Ying, et al. “Deep Back Propagation–Long Short-Term Memory Network Based'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Upper-Limb SEMG Signal Classification for Automated Rehabilitation.” Biocybernetics'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='and Biomedical Engineering, vol. 40, no. 3, 2020, pp. 987–1001, \\nhttps://doi.org/10.1016/j.bbe.2020.05.003.  \\n17. Sahoo, M., Mitra, M., & Pal, S. (2023). Improved detection of dry age-related macular \\ndegeneration from optical coherence tomography images using adaptive window based \\nfeature extraction and weighted ensemble based classification approach. \\nPhotodiagnosis and Photodynamic Therapy, 42, 103629.  \\n18. Muthukannan, P. (2022). Optimized convolution neural network based multiple eye'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='disease detection. Computers in Biology and Medicine, 146, 105648.  \\n19. Serener, A., & Serte, S. (2019, April). Dry and wet age-related macular degeneration \\nclassification using oct images and deep learning. In 2019 Scientific meeting on \\nelectrical-electronics & biomedical engineering and computer science (EBBT) (pp. 1-4). \\nIEEE.  \\n20. Kumar, Y. and Gupta, S., 2023. Deep transfer learning approaches to predict glaucoma,'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='cataract, choroidal neovascularization, diabetic macular edema, drusen and healthy \\neyes: an experimental review. Archives of Computational Methods in Engineering, 30(1), \\npp.521-541.  \\n21. Paradisa, Radifa Hilya, et al. \"Deep feature vectors concatenation for eye disease \\ndetection using fundus image.\" Electronics 11.1 (2022): 23.  \\n22. Faizal, Sahil, et al. \"Automated cataract disease detection on anterior segment eye'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='images using adaptive thresholding and fine tuned inception-v3 model.\" Biomedical \\nSignal Processing and Control 82 (2023): 104550.  \\n23. Pahuja, Rahul, et al. \"A Dynamic Approach of Eye Disease Classification Using Deep \\nLearning and Machine Learning Model.\" Proceedings of Data Analytics and \\nManagement: ICDAM 2021, Volume 1. Springer Singapore, 2022.  \\n24. Chaudhary, R., & Kumar, A. (2022, June). Cataract Detection using Deep Learning'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Model on Digital Camera Images. In 2022 IEEE International Conference on Cybernetics \\nand Computational Intelligence (CyberneticsCom) (pp. 489-493). IEEE  \\n25. Khan, Md Sajjad Mahmud, et al. \"Cataract detection using convolutional neural network \\nwith VGG-19 model.\" 2021 IEEE World AI IoT Congress (AIIoT). IEEE, 2021.  \\n26. Mondal, Sambit S., et al. \"EDLDR: An Ensemble Deep Learning Technique for Detection \\nand Classification of Diabetic Retinopathy.\" Diagnostics 13.1 (2023): 124.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='27. Babenko, Boris, et al. \"Detection of signs of disease in external photographs of the eyes \\nvia deep learning.\" Nature Biomedical Engineering (2022): 1-14  \\n28. Saranya, P., R. Pranati, and Sneha Shruti Patro. \"Detection and classification of red \\nlesions from retinal images for diabetic retinopathy detection using deep learning \\nmodels.\" Multimedia Tools and Applications (2023): 1-21.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='29. Alyoubi, W. L., Abulkhair, M. F., & Shalash, W. M. (2021). Diabetic retinopathy fundus \\nimage classification and lesions localization system using deep learning. Sensors, \\n21(11), 3704.  \\n30. Acharya, U. R., Mookiah, M. R. K., Koh, J. E., Tan, J. H., Noronha, K., Bhandary, S. \\nV., ... & Laude, A. (2016).Novel risk index for the identification of age-related macular \\ndegeneration using radon transform and DWT features. Computers in biology and \\nmedicine, 73, 131-140.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='31. Zaki, W. M. D. W., Mutalib, H. A., Ramlan, L. A., Hussain, A., & Mustapha, A. (2022). \\nTowards a Connected Mobile Cataract Screening System: A Future Approach. Journal \\nof Imaging, 8(2).  \\n32. Larxel. “Ocular Disease Recognition.” Kaggle, 24 Sept. 2020, \\nkaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k. \\n33. Dugas, Emma, et al. “Diabetic Retinopathy Detection.” Kaggle, 2015, \\nkaggle.com/competitions/diabetic-retinopathy-detection.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='34. Saeed, and Rimsha. “Fundus-Dataset.Zip.” Figshare, 11 Nov. 2021, \\ndoi.org/10.6084/m9.figshare.16986166.v1.  \\n35. Awsaf. “RFMID Train Dataset.” Kaggle, 26 Nov. 2020, \\nwww.kaggle.com/datasets/awsaf49/rfmid-train-dataset.  \\n36. “Eyecharity.Com Is for Sale.” HugeDomains.Com, www.eyecharity.com/aria_online. \\nAccessed 19 July 2023.  \\n37. Doddi, Guna Venkat. “Eye_diseases_classification.” Kaggle, 28 Aug. 2022, \\nwww.kaggle.com/datasets/gunavenkatdoddi/eye-diseases-classification.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='38. Watson, Debbie. Contouring: a guide to the analysis and display of spatial data. \\nElsevier, 2013.  \\n39. Rahman, Tawsifur, et al. \"Exploring the effect of image enhancement techniques on \\nCOVID-19 detection using chest X-ray images.\" Computers in biology and medicine 132 \\n(2021): 104319.  \\n40. Sahu, S., Singh, A. K., Ghrera, S. P., & Elhoseny, M. (2019). An approach for de-noising \\nand contrast enhancement of retinal fundus image using CLAHE. Optics & Laser \\nTechnology, 110, 87-98.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='41. Sara, Umme, Morium Akter, and Mohammad Shorif Uddin. \"Image quality assessment \\nthrough FSIM, SSIM, MSE and PSNR—a comparative study.\" Journal of Computer and \\nCommunications 7.3 (2019): 8-18.  \\n42. Saranya, P., R. Pranati, and Sneha Shruti Patro. \"Detection and classification of red \\nlesions from retinal images for diabetic retinopathy detection using deep learning \\nmodels.\" Multimedia Tools and Applications (2023): 1-21.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='43. Team, Keras. “Keras Documentation: DenseNet.” Keras, \\nkeras.io/api/applications/densenet/. Accessed 10 June 2023.  \\n44. Dominic, Nicholas, et al. \"Transfer learning using inception-ResNet-v2 model to the \\naugmented neuroimages data for autism spectrum disorder classification.\" Commun. \\nMath. Biol. Neurosci. 2021 (2021): Article-ID.  \\n45. Tsiouris, Κostas Μ., et al. \"A long short-term memory deep learning network for the'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='prediction of epileptic seizures using EEG signals.\" Computers in biology and medicine \\n99 (2018): 24-37.  \\n46. Xu, M., Yoon, S., Fuentes, A., & Park, D. S. (2023). A comprehensive survey of image \\naugmentation techniques for deep learning. Pattern Recognition, 109347.  \\n47. Hasan AM, Jalab HA, Meziane F, Kahtan H, Al-Ahmad AS. Combining deep and \\nhandcrafted image features for MRI brain scan classification. IEEE Access 2019;7: \\n79959–67.'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='79959–67.  \\n48. Lundervold AS, Lundervold A. An overview of deep learning in medical imaging focusing \\non MRI. Z Med Phys 2019;29:102–27. https://doi.org/10.1016/j. Zemedi.2018.11.002.  \\n49. Hochreiter, S. (1998). The vanishing gradient problem during learning recurrent neural \\nnets and problem solutions. International Journal of Uncertainty, Fuzziness and \\nKnowledge-Based Systems, 6(02), 107-116.  \\n50. Chen, G. (2016). A gentle tutorial of recurrent neural network with error'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='backpropagation. arXiv preprint arXiv:1610.02583.  \\n51. Yoo, T. K., Choi, J. Y., Seo, J. G., Ramasubramanian, B., Selvaperumal, S., & Kim, D. \\nW. (2019). The possibility of the combination of OCT and fundus images for improving \\nthe diagnostic accuracy of deep learning for age-related macular degeneration: a \\npreliminary experiment. Medical & biological engineering & computing, 57, 677-687.  \\n52. Liu, H., Wong, D. W., Fu, H., Xu, Y., & Liu, J. (2019). DeepAMD: detect early age-related'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='macular degeneration by applying deep learning in a multiple instance learning \\nframework. In Computer Vision–ACCV 2018: 14th Asian Conference on Computer \\nVision, Perth, Australia, December 2–6, 2018, Revised Selected Papers, Part V 14 (pp. \\n625-640). Springer International Publishing.  \\n53. Grassmann, F., Mengelkamp, J., Brandl, C., Harsch, S., Zimmermann, M. E., Linkohr, \\nB., ... & Weber, B. H. (2018). A deep learning algorithm for prediction of age-related eye'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='disease study severity scale for age-related macular degeneration from color fundus \\nphotography. Ophthalmology, 125(9), 1410-1420.  \\n54. Chea, N., & Nam, Y. (2021). Classification of Fundus Images Based on Deep Learning \\nfor Detecting Eye Diseases. Computers, Materials & Continua, 67(1).  \\n55. Domínguez, C., Heras, J., Mata, E., Pascual, V., Royo, D., & Zapata, M. Á. (2023). \\nBinary and multi-class automated detection of age-related macular degeneration using'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='convolutional-and transformer-based architectures. Computer Methods and Programs in \\nBiomedicine, 229, 107302.  \\n56. Zang, P., Hormel, T. T., Hwang, T. S., Bailey, S. T., Huang, D., & Jia, Y. (2023). Deep-\\nLearning–Aided Diagnosis of Diabetic Retinopathy, Age-Related Macular Degeneration, and \\nGlaucoma Based on Structural and Angiographic OCT. Ophthalmology Science, 3(1), 100245.  \\n57. Da Yan, Shengbin Wu, Mirza Tanzim Sami, Abdullateef Almudaifer, Zhe Jiang, Haiquan Chen,'), Document(metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='D. Rangaprakash, Gopikrishna Deshpande, Yueen Ma, “Improving Brain Dysfunction Prediction \\nby GAN: A Functional-Connectivity Generator Approach”, IEEE International Conference on \\nBig Data (Big Data), Orlando, FL, USA, 2021. \\n58. Mirza Tanzim Sami, Da Yan, Huang Huang, Xinyu Liang, Guimu Guo, Zhe Jiang, “Drone-Based \\nTower Survey by Multi-Task Learning”, IEEE International Conference on Big Data (Big Data), \\nOrlando, FL, USA, 2021.'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='Shakhawat Hossain\\n♂phone+880 1778198423 /envel⌢peshakhawat15-14283@diu.edu.bd /linkedinshanin-hossain /githubShakhawatShanin\\nEducation\\nDaffodil International University Dhaka, Bangladesh\\nB.Sc. in Computer Science and Engineering CGPA: 3.60/4.00 (Jan. 2020 – Dec 2023)\\nRelevant Coursework:Data Structure, Data Mining and Machine Learning, Big Data and IoT, Artificial Intelligence,\\nNumerical Methods, Algorithms, Object Oriented Programming, Programming and Problem Solving'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='Undergraduate Major:Artificial Intelligence\\nUndergraduate Thesis: Graph-Based Automatic Breast Tumor Classification Through Ultrasound Imaging Using Ra-\\ndiomics Features.\\nResearch Interests\\n• Machine Learning\\n• Deep Learning\\n• Artificial Intelligence\\n• Computer Vision\\n• Health Informatics\\n• Medical Imaging\\n• Image Preprocessing\\n• Pattern Recognition\\n• Generative Artificial Intelligence\\nExperience\\nUniversity of Queensland Brisbane, Australia\\nResearch Assistant May 2024 – Present (Remote)'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='• At the AI and Digital Health Technology Lab, I developed a cutting-edge brain glioma grading system utilizing hybrid\\ngraph networks. This advanced model, trained on radiomic biomarkers extracted from 3D MRI scans, employs LIME to\\nensure accurate and interpretable grading, delivering clinically reliable outcomes.\\nHawkEyes Digital Monitoring Limited Dhaka, Bangladesh\\nAI Engineer Jan 2024 – Present'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='• Data Handling: Performed data validation and cleaning to ensure dataset reliability, and maintained consistent\\nlabeling quality across projects for developing robust AI.\\n• Computer Vision: I led computer vision projects focused on advanced image recognition, classification, segmentation,\\nand object detection. By leveraging algorithms like YOLO, UNet, custom CNN-LSTM, and OpenCV.\\n• Generative AI: Developed Lip Sync video model, AI-powered HDML employee information system, and interactive'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='chatbot designed for automated responses.\\n• OCR: Designed and implemented OCR pipelines to extract handwritten diverse text from scanned memo images.\\nProjects\\nBAT Bangladesh | Instance Segmentation, OOB Detection, Warp Perspective, Sequence Generation, Sorting\\n• Led the analysis of cigarette displays for regional campaigns using image processing, developing sequence analysis\\nalgorithms to ensure compliance with merchandising standards. This work improved the accuracy of audits and'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='optimized campaign management for BAT.\\nUnilever Bangladesh | Python, YoloV8, FastAPI, Asynchronous programming, Logging\\n• Implemented an AI-based trade merchandiser platform for Unilever Bangladesh with an accuracy rate of 98.00%.\\n• The system features a user-friendly dashboard for seamless management of trade merchandising activities, ensuring\\nefficient inventory control, meticulous task execution, and centralized digital recording, analysis, and reporting.'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='CardioCare | ML, FlaskAPI, HTML, Tailwind\\n• Developed a heart failure prediction model using machine learning, served via a Flask API with a responsive HTML and\\nTailwind CSS front-end.\\nChessCrack | OpenCV, UNet, YoloV8, Stockfish, NumPy, JS\\n• This project develops an intelligent system that automates the analysis of a physical chessboard image to predict the best\\nmove. It uses a YOLO-based model to detect and classify chess pieces, converts the board state into a FEN string, and'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='then passes it to the Stockfish engine to determine the optimal move. The system is integrated into a web application via\\na Flask API, allowing users to upload a chessboard image and receive a move recommendation.\\nOfficeVision | FaceRecognition, Uvicorn, Llama, Langchain, Streamlit, Huggingface\\n• Implemented an advanced image recognition solution to accurately identify employees from images, enabling efficient and'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='automated retrieval of employee details such as roles, contact information, and department.'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='Technical Skills\\nLanguages: Python, C++, HTML/CSS, MySQL\\nData Analysis and Visualization : NumPy, Pandas, Matplotlib, Seaborn\\nAssociated Frameworks: TensorFlow, PyTorch, Scikit-learn, Keras, OpenCV, Transfer Learning, Hugging Face\\nTools: Git, GitHub, Jupyter Notebook, Visual Studio Code, Latex, Colab, Roboflow\\nDevelopment Tools: FastAPI, FlaskAPI, RestAPI, MLOps, LLM, NLP\\nPublications\\n1. Md. Aiyub Ali, Md. Shakhawat Hossain , Md.Kawsar Hossain, Subhadra Soumi Sikder, Sharun Akter'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='Khushbu, Mirajul Islam, ”AMDNet23: Hybrid CNN-LSTM Deep Learning Approach with Enhanced\\nPreprocessing for Age-Related Macular Degeneration (AMD) Detection”, Intelligent Systems with\\nApplications journal, Elsevier. https://doi.org/10.1016/j.iswa.2024.200334\\n2. Md. Aiyub Ali, Md Shakhawat Hossain, Taslima Ferdaus Shuva, Muhammad Ali Abdullah Almoyad,\\nNabil Anan Orka, Md. Tanvir Rahman, Risala Tasin Khan, M. Shamim Kaiser, and Mohammad Ali Moni.'), Document(metadata={'source': 'Portfolio/data/PhD.pdf'}, page_content='“RGNN3D: A Hybrid Radiomic Graph Neural Network for 3D MRI Glioma Grading” IEEE Transactions on\\nBiomedical Engineering. [In Review]\\nReferences\\nDr. Mohammad Ali Moni, PhD (Cambridge)\\nProfessor,\\nHead of the Group, AI and Digital Health Technology\\nFaculty of Health and Behavioural Science,\\nThe University of Queensland, St Lucia, QLD 4072, Australia\\nEmail: m.moni@uq.edu.au'), Document(metadata={'source': 'Portfolio/data/Job.pdf'}, page_content='SHAKHAWAT HOSSAIN\\nAI Engineer\\n/ne+8801778198423\\n shanin-hossain\\n/gtbShakhawatShanin\\n shaninhossain2@gmail.com\\nSUMMARY\\nI am an AI Engineer at HawkEyes Digital Monitoring Lim-\\nited, specializing in optimizing Computer Vision, NLP,\\nOCR, and AI models. I ensure top-notch quality and\\nseamlessly deploy solutions on cloud platforms and ap-\\nplications. Passionate about leveraging cutting-edge\\ntechnology to solve real-world problems, I actively learn\\nnew technologies and coding practices to push the'), Document(metadata={'source': 'Portfolio/data/Job.pdf'}, page_content='boundaries of AI innovation. I have successfully solved\\nover 300+ programming challenges on platforms such\\nas Codeforces, URI, and DIU Bluesheet.\\nSKILLS\\nLanguages: C, C++, Python, Java, MySQL\\nData Analysis: NumPy, Pandas, Matplotlib, Seaborn\\nFrameworks: TensorFlow, PyTorch, Scikit-learn, Keras,\\nOpenCV, Hugging Face\\nTools: Git, Jupyter, VS Code, LaTeX, Roboflow,\\nColab, MS PowerPoint, Word\\nWeb Tools: FastAPI, Flask, RestAPI, ReactJS, Tail-\\nwind\\nEDUCATION'), Document(metadata={'source': 'Portfolio/data/Job.pdf'}, page_content='wind\\nEDUCATION\\n01/2020 – 12/2023 B.Sc. in Computer Science and Engineering Daffodil International University (DIU)\\nCGPA: 3.60/4.00\\nMajor: Artificial Intelligence\\nThesis: Graph-Based Breast Tumor Classification Through Ultrasound Imaging Using Radiomics Features.\\nEXPERIENCE\\n02/2024 – Present AI Engineer HawkEyes Digital Monitoring Limited\\nPrepared datasets, trained and fine-tuned models, and optimized accuracy for real-world AI applications.'), Document(metadata={'source': 'Portfolio/data/Job.pdf'}, page_content='05/2024 – Present Research Assistant (Remote) University of Queensland\\nDeveloped a hybrid graph-based brain glioma grading system using 3D MRI datasets.\\n01/2024 – 02/2024 Junior Front-End Developer M4yours Dev\\nDesigned a responsive news portal template for publishers and bloggers.\\n01/2023 – 12/2023 Research Lab Member HIRL Lab (DIU)\\nWorked on medical image disease detection, sentiment analysis, and integrated computer vision with\\nadvanced ML algorithms.\\nPROJECTS'), Document(metadata={'source': 'Portfolio/data/Job.pdf'}, page_content='PROJECTS\\nBAT Bangladesh YOLO Segmentation, Detection, Warp Perspective, Sequence Generation, LLAMA2, Langchain\\n• BAT AI-Based Cigarette Brand Detection and Competitor Analysis System:\\nBuilt an object detection system using YOLO for cigarette brand identification, sequence validation,\\nand competitor analysis.\\n• Chatbot Development:\\nDeveloped an NLP chatbot for dynamic conversations across multiple platforms, handling queries\\nand providing real-time customer support with personalized responses.'), Document(metadata={'source': 'Portfolio/data/Job.pdf'}, page_content='Unilever\\nBangladesh\\nPython, NL TK, FastAPI, Pytesseract, Asynchronous Programming\\n• OCR Based Billing System for Laver Bazar:\\nDeveloped an OCR application to capture invoice images and extract item names, quantities, and\\nprices. Automated data digitization and storage, enhancing inventory and financial management by\\nreducing errors.\\n• Voice Recognition System for Word Detection and Counting:\\nDeveloped a voice recognition application that processes audio input and identifies the frequency'), Document(metadata={'source': 'Portfolio/data/Job.pdf'}, page_content='of specific words or phrases.\\nGazipur Police ML, MTCNN, Keras FaceNet, OpenCV, NumPy\\n• Face Recognition System for Gazipur Metropolitan Police (GMP):\\nDesigned a real-time facial recognition system that enables secure access control, user verification,\\nand efficient recognition history management.\\nACHIVEMENT\\n2025 Best Performing Team (AI Team) HawkEyes Digital Monitoring Limited\\nPUBLICATIONS'), Document(metadata={'source': 'Portfolio/data/Job.pdf'}, page_content='PUBLICATIONS\\n• Md. Aiyub Ali, Md. Shakhawat Hossain , Md.Kawsar Hossain, Subhadra Soumi Sikder, Sharun Akter Khushbu, Mirajul\\nIslam, \"AMDNet23: Hybrid CNN-LSTM Deep Learning Approach with Enhanced Preprocessing for Age-Related Mac-\\nular Degeneration (AMD) Detection\", Intelligent Systems with Applications journal, Elsevier.\\nhttps:/ /doi.org/10.1016/j.iswa.2024.200334\\n• Md. Aiyub Ali, Md Shakhawat Hossain , Taslima Ferdaus Shuva, Muhammad Ali Abdullah Almoyad, Nabil Anan Orka,'), Document(metadata={'source': 'Portfolio/data/Job.pdf'}, page_content='Md. Tanvir Rahman, Risala Tasin Khan, M. Shamim Kaiser, and Mohammad Ali Moni. “RGNN3D: A Hybrid Radiomic\\nGraph Neural Network for 3D MRI Glioma Grading” Knowledge-Based Systems. [In Review]\\nREFERENCES\\n• Dr. Mohammad Ali Moni, PhD (Cambridge): Professor and Head, AI and Digital Health Technology, University of\\nQueensland, Australia\\nEmail: m.moni@uq.edu.au'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='RGNN3D: A Hybrid Radiomic Graph Neural Network for 3D\\nMRI Glioma Grading\\nMd. Aiyub Ali a,b, Md Shakhawat Hossaina,b, Taslima Ferdaus Shuvab, Muhammad Ali Abdullah\\nAlmoyadc, Nabil Anan Orkaa, Risala Tasin Khane, M. Shamim Kaiser e, Md. Tanvir Rahman a,d,∗,\\nMohammad Ali Monia,b,f,g,h,∗∗\\naSchool of Health and Rehabilitation Sciences, The University of Queensland, St\\nLucia, Brisbane, 4072, Queensland, Australia'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='bDepartment of Computer Science and Engineering, Daffodil International University, Daffodil Smart\\nCity, Savar, 1341, Dhaka, Bangladesh\\ncDepartment of Basic Medical Sciences, College of Applied Medical Sciences, King Khalid\\nUniversity, Guraiger, 62521, Abha, Saudi Arabia\\ndDepartment of Information and Communication Technology, Mawlana Bhashani Science and Technology\\nUniversity, Santosh, Tangail, 1902, Dhaka, Bangladesh'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='eInstitute of Information Technology, Jahangirnagar University, Savar, 1342, Dhaka, Bangladesh\\nfSchool of Information Technology, Washington University of Science and Technology, Alexandria, 22314, Virginia, USA\\ngAI and Cyber Futures Institute, Charles Sturt University, Bathurst, 2795, New South Wales, Australia\\nhRural Health Research Institute, Charles Sturt University, Orange, 2800, New South Wales, Australia\\nAbstract'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Abstract\\nThe diagnosis of glioma, a complex and often deadly brain tumor, involves extensive medical examinations.\\nStill, accurately grading and classifying gliomas is difficult, as different areas within the same tumor can\\nexhibit varying characteristics. The integration of radiomics, a clinically relevant feature extraction method,\\nwith machine learning (ML) is becoming increasingly popular in addressing this issue, but several research'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='gaps persist. To this end, this study proposes a novel deep neural network, RGNN3D, that combines Graph\\nNeural Networks with LSTM layers to precisely grade gliomas in 3D magnetic resonance imaging (MRI)\\ndata. To train our proposed model, we meticulously extracted 112 radiomic biomarkers. Utilizing the\\nbiomarkers, RGNN3D constructs a graph, channels essential information within layers, and preserves only'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='pertinent information through its integrated memory cells. The proposed framework attained an accuracy\\nof 98.58%, aligning with the performance of previous state-of-the-art architectures and surpassing prior\\nradiomic-based ML models. We further employed an explainable AI approach (LIME) to highlight the most\\nsignificant features, assisting radiologists in making more informed decisions. In short, RGNN3D offers a'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='reliable and robust computer-aided solution for potential clinical application in the automated identification\\nof gliomas.\\nKeywords: Radiomics, Biomarkers, Graph Neural Networks, Glioma Grading.\\n1. Introduction1\\nGliomas, which originate from glial cells, are the most prevalent primary intracranial tumors in adults2\\n[1]. Gliomas account for approximately 74.6% of all malignant brain tumors [2]. Patients with low-grade3\\n∗Corresponding author\\n∗∗Corresponding author'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Email addresses: aiyubali15-13456@diu.edu.bd (Md. Aiyub Ali), shakhawat15-14283@diu.edu.bd (Md Shakhawat\\nHossain), shuva.cse@diu.edu.bd (Taslima Ferdaus Shuva), maabdulllah@kku.edu.sa (Muhammad Ali Abdullah Almoyad),\\nn.orka@uq.edu.au (Nabil Anan Orka), risala@juniv.edu (Risala Tasin Khan), mskaiser@juniv.edu (M. Shamim Kaiser),\\ntanvirrahman@mbstu.ac.bd (Md. Tanvir Rahman ), m.moni@uq.edu.au (Mohammad Ali Moni )\\nPreprint submitted to Knowledge-Based Systems September 25, 2025'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='gliomas (LGG, grades I and II) generally have a survival rate averaging seven years [3]. In stark contrast,4\\nonly 3–5% of patients with glioblastoma (GBM, grade IV) survive beyond five years, with a median survival5\\ntime of approximately 12 months [4]. Intraoperative grading of gliomas, especially distinguishing between6\\nGBM and LGG, is crucial for making informed diagnostic decisions in clinical practice. To this end, magnetic7'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='resonance imaging (MRI) is an essential technique for screening, treatment planning, and assessing tumor8\\nresponse to therapy because it assists in analyzing the phenotypic and structural differences of gliomas9\\n[5]. In the same vein, radiomic features have shown a lot of promise for distinguishing between different10\\ngrades of glioma. Radiomics is an emerging field that extracts high-throughput quantitative features from11'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='medical images, particularly MRI [6]. Radiomic analysis involves separating the tumor area from the rest of12\\nthe image and extracting clinically useful information about its shape, appearance, size, intensity, location,13\\nand texture [7]. For example, high heterogeneity in intensity and texture features can sometimes indicate14\\nGBM [8, 9]. Although the combination of MRI and radiomic analysis offers vital clinical information15'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='regarding gliomas, identifying patterns among the hundreds of variables and understanding how each one16\\naffects glioma grading is challenging. To this end, machine learning (ML) algorithms emerge as a viable17\\nsolution. For instance, earlier studies used various ML classifiers such as logistic regression (LR) [10],18\\nsupport vector machines (SVM) [11, 12], random forest (RF) [2, 13, 14], LASSO [15], and multi-layer19'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='perceptron [12, 16]. These models performed comparably to state-of-the-art convolutional neural networks,20\\nshowing the promise of radiomic-based automated glioma grading. Han Li et al. [17] introduced a transfer21\\nlearning-based optimizer (MSAS-DMOA) to improve adaptability in dynamic tasks. Peishu Wu et al. [18]22\\nproposed GLA-TD, a CNN-transformer model using attention and tensor decomposition for efficient medical23'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='image analysis. These works align with our RGNN3D approach in enhancing learning and interpretability.24\\nHowever, notable research gaps still persist. First of all, despite the availability of numerous types of radiomic25\\nfeatures, prior research has focused on distinct radiomic feature types. For example, some studies used only26\\ntexture-based features [19, 20, 21], while others relied on wavelet-based features [22, 11]. To the best of27'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='our knowledge, no studies holistically explored radiomics, i.e., modeling multiple classes of radiomic features28\\nsimultaneously. Second, existing studies lack interpretability. While often accurate, black-box models do not29\\nfoster trust between clinicians and these models because the variables critical to the decision-making process30\\nremain unclear. Finally, no framework has yet outperformed current state-of-the-art classifiers regardless of31'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='earlier studies’ comparable efficacy. Given the aforementioned research gaps, we propose a novel framework,32\\nRGNN3D, which comprises graph convolutional networks (GCNs) and long short-term memory (LSTM)33\\ncells to form a hybrid classifier. The advantages of the proposed hybrid model are twofold. The primary34\\nbenefit of GCNs, or graph neural networks in general, is the ability to embed complex relational data into35'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='a graph and pass only the relevant information to the subsequent layers [23]. In addition, LSTM layers use36\\nmemory cells to retain pertinent input data and avoid the vanishing gradient problem [24]. We train our37\\nproposed model on 112 clinically significant radiomic biomarkers extracted from 3D MRI scans and later38\\nemploy local interpretable model-agnostic explanations (LIME) [25]. LIME quantitatively explains which39'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='radiomic features have the most significant impact on grading LGG and GBM, ensuring utmost reliability.40\\nThe key contributions of this study are summarized as follows:41\\n• Introduced RGNN3D, a novel hybrid method that combines LSTMs and GCNs, leading to highly42\\naccurate glioma grading from complex, multimodal 3D MRI scans.43\\n• Demonstrated that GCNs captured 112 significant radiomic biomarkers, while the LSTMs serve long-44'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='range dependencies and alleviate gradient vanishing by gated memory mechanisms.45\\n• To enhance model transparency, we incorporate LIME-based interpretability analysis, which identifies46\\nand ranks the most influential features contributing to the grading outcomes.47\\n• Identified the top 10 radiomic biomarkers that play a significant role in terms of glioma grading, which48\\nenriches the clinical trust.49\\n2'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='3 Features Integration 2 Radiomic Based Feature Extraction\\nWavelet \\nFilter\\nLoG \\nFilter\\nLBP \\nFilter\\nImage \\nIntensity\\nShape-based \\n(n=26)\\nGLDM \\n(n=14)\\nNGTDM \\n(n=5)\\nGLSZM \\n(n=16)\\nGLRLM \\n(n=16)\\nGLCM \\n(n=24)\\nShape \\nbased \\nfeatures\\nHistogram \\ndescriptors \\nfeatures\\nTexture \\nfeatures\\nTotal \\nExtracted \\nFeatures: \\n112\\nGenerated \\nnodes \\nof \\n704 \\nx \\n112 \\nEdge \\ntable \\n(Adj. \\nmatrix) \\nof\\n \\n64 \\nx \\n64 \\nProposed \\nRGNN3D\\n \\nmodel\\n4\\nHybrid \\nModel \\nDevelopment\\nCombines \\nGraph \\nand \\nSequential \\nLearning\\nEnd-to-End'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='End-to-End \\nTrainable\\nFlexible \\nto \\nDifferent \\nGraph \\nStructures\\nEvaluation \\n& \\nAnalysis\\n5\\n 1 3D MRI & ROI\\nT1\\nT2\\nT1CE\\nFLAIR\\nFirst \\norder \\nstatistics \\n(n=19)\\nRadiomic \\nBiomarker \\nTables\\nT1\\nT2\\nT1CE\\nFLAIR\\nComparison \\nwith \\nstate-of-the-art \\nmethods\\n     \\nExperimented \\non \\nfour \\nindividual \\nstages\\nT1, \\nT1CE, \\nT2, \\nFLAIR\\nAccuracy, \\nSpecificity, \\nSensitivity,\\nAUC,F1-Score\\nConfusion \\nmatrix\\nROC \\ncurve\\nModel \\ninterpretation \\nanalysis'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='analysis\\nFigure 1: Comprehensive workflow of the proposed radiomic-based hybrid RGNN3D Model for glioma grading.\\n2. Materials and Methods50\\nThe overall methodology adopted in this study has five key stages as shown in Fig. 1: (i) 3D MRI &51\\nROI involves acquiring 3D MRI scans and identifying regions of interest (ROI) for GBM and LGG, (ii)52\\nRadiomic Based Feature Extraction employs the PyRadiomics toolbox to extract features from the ROIs53'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='using various filters, (iii) Features Integration incorporates shape-based features, histogram descriptors,54\\nand texture features to create a comprehensive feature set, (iv) Hybrid Model Development proposes the55\\nRGNN3D model that combines graph neural networks and LSTM-based sequential learning, ensuring end-56\\nto-end trainability and interpretability, and (v) Evaluation & Analysis assesses the performance of the57'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='proposed framework with interpretability. In the following subsections, we delve into the specifics of every58\\n3'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='a b\\nGBM\\nLGG\\nFLAIR\\nT1\\nT1CE\\nT2\\nROI\\nBraTS \\n2019\\nBraTS \\n2020\\nCombined \\nBraTS\\nGBM\\nLGG\\nTOTAL\\n552\\n152\\n704\\n259\\n76\\n335\\n76\\n293\\n369\\nFigure 2: Dataset used in this study: (a) Sample images from each category; (b) Integration of BRATS 2019 and BRATS 2020\\nstep.59\\n2.1. Dataset Description60\\nThis study used the Brain Tumor Segmentation (BraTS) datasets, where multimodal 3D MRI scans of61\\nGBM and LGG are available. The BraTS’2019 [26] contains data of 369 subjects with GBM (n = 293)62'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='and LGG (n = 76). Besides, BraTS’2020 [27] comprises 335 subjects with GBM (n = 259) and LGG63\\n(76). We merged these two datasets, and the compilation incorporated a total of 704 subjects: GBM (n64\\n= 552) and LGG (n = 152), as shown in Fig. 2 (b). The merged dataset includes neuroimaging files with65\\nmultiparametric 3D MRI scans, encompassing (a) T2 Fluid Attenuated Inversion Recovery (T2-FLAIR):66'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='suppresses cerebrospinal fluid signals, enhancing lesion visibility, (b) Native (T1): captures the brain’s67\\nbaseline anatomy, highlighting tissue density differences, (c) Post-contrast T1-weighted (T1CE): acquires68\\nT1-weighted images with a contrast agent, highlighting areas of increased vascularization and blood-brain69\\nbarrier disruption to identify abnormal tissue, such as glioma lesions, (d) T2-weighted (T2): provides detailed70'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='information on edema, cysts, and tissue abnormalities, aiding in understanding glioma characteristics, and71\\n(e) ROI: allows feature extractions specifically from the tumor region or areas of interest that are relevant,72\\nprecise, and clinically meaningful. Sample MRI scans from each category are also visualized in Fig. 2 (a).73\\n2.2. Feature Extraction and Integration74\\nRadiomics, a transformative approach in medical imaging, involves the extraction and analysis of clini-75'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='cally significant information embedded within medical images, transcending what is perceptible to the human76\\neye [28]. To address this issue, we used the PyRadiomics [29] tool to identify relevant radiomic features in77\\nour study. Here, for each subject, we get 112 radiomic features corresponding to seven distinct types as78\\ndescribed in Table 1: First Order, Shape-Based, Gray Level Co-occurrence Matrix (GLCM), Gray Level79'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Run Length Matrix (GLRLM), Gray Level Size Zone Matrix (GLSZM), Neighbouring Gray Tone Difference80\\nMatrix (NGTDM), and Gray Level Dependence Matrix (GLDM). The extracted features encompass a range81\\nof statistical metrics, shape-based attributes, and matrix-based analyses, offering insights into voxel intensity82\\ndistribution, geometric properties, spatial relationships, and textural patterns within ROI. However, each83'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='type of these radiomic features contributes to unique and valuable information, and their integration ensures84\\na holistic understanding of the clinically significant attributes. To achieve this, for each MRI stage (T1, T2,85\\nT1CE, FLAIR), we organize these extracted features into tables (Radiomic Biomarker Table: 704 x 112)86\\nwhere each row corresponds to a specific subject, and each column represents different radiomic features.87\\n2.3. RGNN3D88'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='2.3. RGNN3D88\\nWe introduce a Radiomic Graph Neural Network (RGNN3D) architecture that integrates LSTM cells89\\nwithin its message-passing framework to enhance the learning of node embeddings as depicted in Fig. 3.90\\nThis approach allows the model to capture both short-term and long-term dependencies inherent in graph91\\nstructures. The design of the RGNN3D model consists of several layers, including an LSTM layer, GCN92'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='layers with appropriate message passing among each block, and a dense output layer. The input layer receives93\\n112 features and constructs a graph. Here, the node feature vector and the graph’s adjacency matrix are94\\n4'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Table 1: Types of Radiomic Features Extracted in Our Study.\\nFeature Type Description\\nFirst Order Captures fundamental statistical metrics, offering voxel intensity distribu-\\ntion in ROIs.\\nShape-based Encompasses shape-related attributes in 3D and 2D, which understand the\\nbrain’s geometric properties.\\nGLCM Reveals voxel intensity spatial relationship patterns.\\nGLRLM Quantifies consecutive voxel lengths with identical intensity values.'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='GLSZM Offers size and spatial distribution of homogeneous intensity regions.\\nNGTDM Highlights tone differences between neighboring voxels and textural.\\nGLDM Analyzes voxel pair dependence.\\n.\\n.\\n.\\nF3\\nF2\\nF1\\nF8\\nF7\\nF6\\nF3\\nF8\\nF7\\n.\\n.\\n.\\nGCNConv3\\nf1\\nf2\\nf3\\n.\\n.\\n.\\nf91\\nf1\\nf2\\nf3\\n.\\n.\\n.\\nf91\\nf1\\nf2\\nf3\\n.\\n.\\n.\\nf112\\nL\\nG\\nL\\nL\\nGraph \\nConstruction\\nLGG\\nGBM\\nG\\nG\\nG\\nG\\nAdjacency \\nMatrix\\nNode \\nFeatures\\n Graph Convolutional with LSTM Network\\nOutput\\nFilter \\n2\\n[64, \\n2]\\nL\\nG\\nG\\n.\\n.\\n.\\nL\\nFilter \\n32 \\n[64,64]\\nLSTM \\n64\\n[64,64]\\nFilter \\n32'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[64,64]\\nFilter \\n32 \\n[64,32]\\nFilter \\n16 \\n[64,16]\\nAggregate\\nUpdate\\nMessage \\nPassing\\nLSTM \\n64\\nAggregate\\nUpdate\\nMessage \\nPassing\\nAggregate\\nUpdate\\nMessage \\nPassing\\nDense \\nLayer\\nSoftmax\\nNeighbor \\nInformation \\nFlow\\nFigure 3: Proposed RGNN3D model architecture.\\ninitially fed into the Feed Forward Network (FFN) [30] blocks, followed by passing these to the GCN layers95\\nafter processing. This architecture includes three GCN layers, each employing the ReLU activation function96'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='within a message-passing framework. The embeddings updated by the LSTM cell and GCN layers are97\\nthen channeled back into the FFN blocks, producing the final node embeddings as logit values. Finally, a98\\nSoftmax activation function is applied to generate a probability distribution for the potential node labels.99\\nThe inclusion of LSTM enables gradient flow across time steps due to its gating mechanisms (input, forget,100'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='and output gates), which retain essential information and reduce vanishing gradient effects. This makes it101\\nsuitable for preserving long-range dependencies and refining spatial-temporal feature representations learned102\\nfrom the graph structure.103\\n5'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='2.4. Graph Representation and Initial Node Embeddings104\\nWe generate a graph with 704 rows and 112 columns using the radiomic biomarker table, representing105\\nimage features and the target class. Each patient is considered a single node in the graph and the corre-106\\nsponding radiomic feature vector serves as its initial embedding. In this regard, G = (V, E) is a graph, where107\\nV is the set of patient nodes and E represents the set of edges. The node feature matrix X ∈ R704×112, that108'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='encodes the radiomic features for all patients.109\\nTo avoid an artificial dependency from different patients, we employ an identity adjacency matrix A =110\\nI704. Because we decided to connect by self-loops. In this matter, each node maintains independence during111\\nmessage passing, where there is no possibillity of information leakage from other patients. This clinical112\\npractice especially relevant in the healthcare domain, where patient to patient connections are not always113'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='meaningful. We ensure a simple graph structure but clinically efficient. This approach confirms that node114\\nembeddings are rely on radiomic features of each patient’s record.115\\nTheoretically, initial representation of a node v ∈ V is given by:116\\nh(0)\\nv = xv ∈ Rd,\\nwhere d = 112 is the dimensionality of the feature vector. After that, these embeddings pass through a series117\\nof graph convolution and recurrent (LSTM gates) operations. That enables the model to capture higher118'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='level abstractions from radiomic features.119\\nFor better computational efficiency and scalability, we generate a mini batch (each batch is 64 training120\\nsizes). In this regard, each batch feeds to the model during iteration. For a batch size B, we construct the121\\ncorresponding subgraph with a feature matrix Xb ∈ RB×112 and adjacency matrix Ab = IB. This approach122\\nensures the message passing within a batch. It is also prevents corss batch dependencies during training.123'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='While we use an identity adjacency matrix in this study, the framework is flexible and robust. That can124\\nextend to more complex graphs such as kNN based graphs or fully connected graphs but they did not offer125\\nreliable performance. By using a simple identity graph with LSTM cells, the approach is truly robust to the126\\ndataset. It is also adaptable to broader clinical applications. Overall, it supports the clinical generalisability.127\\n2.4.1. Graph Convolutions with LSTM Network128'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='One of the core innovations of the RGNN3D model lies in its use of LSTM cells for aggregating and129\\nupdating node embeddings inside GCN blocks. The underlying message-passing process involves aggregat-130\\ning node features from its neighbors and updating the node’s embedding using LSTM cells, followed by131\\nnormalization and the final nonlinear embeddings. These steps are described as follows:132'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='(a) Aggregation of Neighboring Node Features:For a node u at iteration i, the aggregated message m(i)\\nu133\\nfrom its neighbors N(u) is computed as:134\\nm(i)\\nu =\\nX\\nv∈N(u)\\nW(i)\\nNeih(i−1)\\nv (1)\\nwhere W(i)\\nNei is a learnable weight matrix.135\\n(b) LSTM-based Node Updating:The updated embedding h(i)\\nu for node u is obtained using an LSTM cell136\\n[31] that takes the node’s previous embedding h(i−1)\\nu and the aggregated message m(i)\\nu as inputs:137\\nh(i)\\nu = LSTM(h(i−1)\\nu , m(i)\\nu ) (2)\\n6'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='The internal operations of the LSTM cell are defined by the following equations:138\\ni(i)\\nu = σ(W(i)\\ni h(i−1)\\nu + U(i)\\ni m(i)\\nu + b(i)\\ni ) (3)\\nf(i)\\nu = σ(W(i)\\nf h(i−1)\\nu + U(i)\\nf m(i)\\nu + b(i)\\nf ) (4)\\no(i)\\nu = σ(W(i)\\no h(i−1)\\nu + U(i)\\no m(i)\\nu + b(i)\\no ) (5)\\nc(i)\\nu = f(i)\\nu ⊙ c(i−1)\\nu + i(i)\\nu ⊙ tanh(W(i)\\nc h(i−1)\\nu +\\nU(i)\\nc m(i)\\nu + b(i)\\nc )\\n(6)\\nh(i)\\nu = o(i)\\nu ⊙ tanh(c(i)\\nu ) (7)\\nHere, i(i)\\nu , f(i)\\nu , o(i)\\nu are the input, forget, and output gates, respectively, and c(i)'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='u is the cell state. The139\\nparameters W(i)\\ni , U(i)\\ni , b(i)\\ni , etc., are the learnable weights and biases of the LSTM cell at iteration i.140\\n(c) Normalization of Node Embeddings:After several iterations of message passing, the node embeddings141\\nare normalized to improve their representational power:142\\nhnorm\\nu = h(i)\\nu\\n∥h(i)\\nu ∥\\n(8)\\nwhere ∥h(i)\\nu ∥ denotes the Euclidean norm of the vector h(i)\\nu .143'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='u .143\\n(d) Final Node Embeddings via GCN Layer:The normalized embeddings are then passed through a Graph144\\nConvolutional Network (GCN) [32] layer to obtain the final nonlinear node embeddings:145\\nH(i+1) = σ(AH(i)WGCN) (9)\\nwhere A is the adjacency matrix, where ones are on the diagonal and zeros elsewhere. This approach146\\nensures that each node is only connected to itself, focusing on the node-specific features without147'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='considering inter-node dependencies. Here, H(i) is the matrix of node embeddings at iteration i,148\\nWGCN is the weight matrix of the GCN layer, and σ is a nonlinear activation function.149\\n2.4.2. Output Layer and Grading150\\nThe final embeddings H(i+1) from the GCN layer are passed through a dense output layer to predict the151\\nnode labels. The dense layer applies a linear transformation followed by a nonlinear activation function of152'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Softmax to generate a probability distribution for each node’s label. The output for a node u is computed153\\nas follows:154\\nyu = σ(Wouthnorm\\nu + bout) (10)\\nHere, Wout is the weight matrix of the output layer, bout is the bias term, and σ denotes the Softmax155\\nactivation function, which maps the output to each probability value representing the grading of glioma.156\\n2.5. Implementation157\\nThe implementation of the RGNN3D model commenced with the combined BraTS dataset comprising158'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='704 records, which was further divided into training and testing sections in an 80:20 ratio. The model was159\\noptimized using the Adam optimizer, with a batch size of 64, and trained over 50 epochs. The Glorot160\\nUniform initializer was employed for the kernel initialization. Sparse Categorical Cross-Entropy was utilized161\\nfor the loss function. The implementation was executed on a system equipped with an RTX 3060 GPU and162'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='32GB of RAM, running the NVIDIA driver version 535.104.05 with CUDA version 12.2.163\\n7'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='3. Results and Discussion164\\nFirst, we start with the performance analysis across all four 3D MRI stages (T1, T1CE, T2, and FLAIR)165\\nusing standard classification performance metrics [33] i.e., precision, recall, F1-score, and accuracy. From166\\nthe confusion matrices as shown in Fig. 4 we observe: (i) for classifying LGG, the proposed model acquired167\\nidentical performance across all stages with 28 accurate cases and only two misclassifications, (ii) for GBM,168'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='we get no false negative result for both T1 and T2 stages. However, there were only 2 and 6 misclassifications169\\nout of 141 cases with T1CE and FLAIR stages, respectively. from the confusion matrices, it is clear that GBM170\\nis more consistently detected compared to LGG, which demonstrated strong power for aggressive gliomas.\\n105\\n6\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nFLAIR\\n0\\n100\\n75\\n50\\n25\\nPREDICTED\\nACTUAL\\n111\\n0\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nT2\\n0\\n100\\n75\\n50\\n25\\nPREDICTED'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='75\\n50\\n25\\nPREDICTED\\nACTUAL\\n109\\n2\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nT1CE\\n0\\n100\\n75\\n50\\n25\\nPREDICTED\\nACTUAL\\n111\\n0\\n28\\n2\\nGBM\\nLGG\\nGBM\\nLGG\\nConfusion \\nMatrix \\nof \\nT1\\n0\\n100\\n75\\n50\\n25\\nPREDICTED\\nACTUAL\\nFigure 4: Confusion Matrices of RGNN3D Model for MRI Stages T1, T2, T1CE, and FLAIR\\n171\\nUsing these matrices, we calculate different scores and discover a similar trend for each evaluation criterion172'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='as depicted in Fig. 5. The proposed RGNN3D system achieved outstanding accuracy scores of 98.58% for173\\nT1 and T2, 97.16% for T1CE, and 94.33% for FLAIR. The F1-scores also reinforce the system’s robustness,174\\nmaintaining values of 99.10% for T1 and T2, 98.20% for T1CE, and 96.33% for FLAIR. Furthermore, the175\\nprecision values reached 100% for F1 and F2, which are able to completely avoid false-positive decisions for176'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='glioma grading. Even for T1CE and FLAIR modalities, precision scores remain high at 98.20% and 94.59%177\\nrespectively. Moreover, our system shows strong recall: 98.23% (T1), 98.23% (T2), 98.20% (T1CE), and178\\n98.13% (FLAIR). Overall, we notice the best and identical performance of the system with both T1 and T2179\\n(accuracy: 98.58%, precision: 100%, f1-score: 99.10%, and recall: 98.23%) for each category.180'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Besides, the accuracy and loss curves (outlined in Fig. 6) illustrate the performance of the proposed181\\nmodel over 50 epochs. Here, we notice that the training accuracy reaches 100.00%, and we get the validation182\\naccuracy to be 98.58%. We noted that the training loss decreases smoothly for 4 different MRI stages, but183\\n8'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='ACCURACY\\nF1-SCORE\\nPRECISION\\nRECALL\\n98.58%\\n97.16%\\n98.58%\\n94.33%\\n99.10%\\n98.20%\\n99.10%\\n96.33%\\n100.00%\\n98.20%\\n100.00%\\n94.59%\\n98.23%\\n98.20%\\n98.23%\\n98.13%\\nFLAIR\\nT2\\nT1CE\\nT1\\nFigure 5: Comparison of Scores of RGNN3D Model for 4 MRI Stages\\nTraining Loss Training Accuracy\\nValidation \\nLoss\\nValidation \\nAccuracy\\nEpochs\\nEpochs\\nEpochs\\nEpochs\\nLoss\\nAccuracy\\n \\nLoss\\nAccuracy\\nFLAIR\\nT1CE\\nT2\\nT1\\nFLAIR\\nT1CE\\nT2\\nT1\\nFLAIR\\nT1CE\\nT2\\nT1\\nFLAIR\\nT1CE\\nT2\\nT1\\n0.7\\n0.6\\n0.5\\n0.4\\n0.3\\n0.2\\n0.1\\n0.0\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n10\\n20'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='20\\n30\\n40\\n50\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n10\\n20\\n30\\n40\\n50\\n0.0\\n0.2\\n0.4 \\n0. \\n6\\n0. \\n8\\n1.0\\n0.0\\n0.2\\n0.4 \\n0. \\n6\\n0. \\n8\\n1.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\n0.0\\nFigure 6: Accuracy and Loss Curves of RGNN3D Model for 4 MRI Stages\\nthe validation losss shows differently. Here, for T1 and T2, the validation loss follows training loss closely,184\\nwhich indicates stable learning in our proposed RGNN3D method. In T1CE, the validation loss shows185\\n9'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='98.59%\\n1\\n0.90\\n0.92\\n0.94\\n0.96\\n0.98\\n1.00\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n94.37%\\n100.00%\\n98.57%\\n100.00%\\n100.00%\\n98.57%\\n92.66%\\n98.57%\\n97.14%\\n10-FOLD CROSS VALIDATION of T1\\nBest \\nVal \\nAccuracy\\nper \\nFold\\nOverall \\nBest\\nAccuracy \\n(97.87%)\\nOverall \\nStd \\nDev\\n(±2.31%)\\n98.59%\\n1\\n0.90\\n0.92\\n0.94\\n0.96\\n0.98\\n1.00\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n92.96%\\n98.57%\\n98.57%\\n97.14%\\n100.00%\\n98.57%\\n97.14%\\n95.71%\\n98.14%\\n10-FOLD \\nCROSS \\nVALIDATION \\nof \\nT2\\nBest \\nVal \\nAccuracy\\nper \\nFold\\nOverall \\nBest\\nAccuracy \\n(97.44%)\\nOverall \\nStd \\nDev\\n(±1.87%)'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Std \\nDev\\n(±1.87%)\\nFigure 7: Cross-Validation Performance of Glioma Grading with T1 and T2 MRI Modalities\\na bit higher, but it has stability. For the FLAIR stage, it significantly rises. This is why, the FLAIR186\\nmodality achieved the lowest accuracy compared to other modalities. Overall, the training loss curve shows187\\na consistent decrease compared to the validation loss trend, where we only observe consistency with T1 and188'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='T2. This phenomenon also supports its effectiveness with our architecture.189\\nWe extend our investigation into contemporary glioma grading systems that are not explicitly focused on190\\nradiomics features for 10-fold cross-validation. Among all the configurations tested, T1 and T2 were chosen191\\nsince they attained the highest overall accuracies. Figure 7 shows T1’s (upper part) validation accuracies192'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='(x-axis: fold numbers 1-10, y-axis: accuracy %), ranging from 92.66% (fold 7) to 100% (folds 3, 5, 8).193\\nThe overall average accuracy across all folds was 97.87% with a standard deviation of ± 2.31%, indicating194\\nstrong performance with some variation across folds. Figure 2 (lower part) shows T2’s accuracies, which195\\nranged from 92.96% (fold 2) to 100% (fold 3), with an average accuracy of 97.44% ± 1.87%, reflecting196'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='slightly lower but more consistent results compared to T1. Furthermore, the ROC curves concerning MRI197\\nmodality are displayed in Fig. 8 to confirm the proposed RGNN3D model’s strength in identifying glioma198\\ngrades. Remarkably, both GBM and LGG perform exceptionally prominent; their ROC curves for T1 and199\\nT2 approach the optimal top-left corner, which is consistent with their high accuracy and the prior described200'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='F1-scores. Conversely, FLAIR demonstrates a marginally lower AUC, confirming its comparatively poorer201\\nperformance. Taken together, our proposed architecture demonstrates better performance with T1 and T2202\\nstages compared to T1CE and FLAIR while grading glioma tumors.203\\nSubsequently, to illustrate the interpretability of our proposed architecture, we implement LIME [34].204\\n10'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='ROC Curve of T1\\nROC \\ncurve \\n(AUC=0.96)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\n ROC Curve of T2\\nROC \\ncurve \\n(AUC=0.97)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\nROC \\nCurve \\nof \\nT1CE\\nROC \\ncurve \\n(AUC=0.97)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\nROC \\ncurve \\n(AUC=0.96)\\nROC \\nCurve \\nof \\nFLAIR\\nROC \\ncurve \\n(AUC=0.97)\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse \\nPositive \\nRate\\nTrue \\nPositive \\nRate\\nROC \\ncurve \\n(AUC=0.94)\\nFigure 8: Performance Analysis of RGNN3D Model for 4 MRI Stages in ROC Curves\\nAs shown in Fig. 9, the prioritization of specific radiomic features in LIME-based interpretations is driven205\\nby contribution to model performance where all radiomic features are clinically relevant. Still, the top three206'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='most essential features make a significant contribution to the grading of glioma in these specific data points.207\\nFor GBM, the features “Mesh Volume”, “Maximum 2D diameter”, and “Maximum 3D diameter” are crucial208\\nfactors among the 112 features, underscoring their significant roles in distinguishing glioma. These features209\\nprovide critical information about the tumor’s size, shape, and spatial dimensions. For LGG, the features210'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='“Minor Axis Length”, “Maximum 2D diameter”, and “Gray Level Non-Uniformity” are the top features,211\\nemphasizing their interpretability and actionable insights in clinical decision-making. The shape and texture212\\nfeatures captured by these metrics are vital in differentiating between glioma grades and understanding the213\\nheterogeneity within the tumor. While the remaining features also contribute valuable information, they214'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='are comparatively less significant for several reasons, such as redundancy, specificity, statistical significance,215\\nclinical validation, and model performance.216\\nAt this point, we concentrate on the previous studies utilizing radiomic features for glioma tumor grading.217\\nFrom the current literature, we observe that most of the related studies focused on statistical ML models218\\nsuch as LR [10], SVM [11, 12], and RF [2, 13, 14]. Consequently, we aim to evaluate the performance of219'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='statistical ML models on our dataset. Consequently, we implement five ML models (K-NN, LR, DT, RF,220\\nand SVM) with the acquired radiomic features using our dataset. Here, we use T1 stage MRI images as we221\\ngot good results during our prior analysis. Table 2 presents a comparative summary of the above-mentioned222\\n11'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Mesh \\nVolume\\nMaximum \\n2D \\ndiameter \\n(1.00)\\nKurtosis \\n(-0.60)\\n10th \\npercentile \\n(-0.34)\\nLow \\nGray \\nLevel \\nZone \\nEmphasis \\n(-0.40)\\nLarge \\nDependence \\nLow \\nGray \\nLevel \\nEmphasis \\n(-0.21)\\nMaximum \\n3D \\ndiameter \\n(0.64)\\nSmall \\nArea \\nHigh \\nGray \\nLevel \\nEmphasis \\n(-0.12)\\nElongation \\n(-1.28)\\nSmall \\nDependence \\nHigh \\nGray \\nLevel \\nEmphasis \\n(-0.11)\\nMesh \\nVolume \\n(0.63)\\n0.07\\n0.04\\n0.04\\n0.04\\n0.05\\n0.05\\n0.05\\n0.05\\n0.06\\n0.27\\nMaximum \\n2D \\ndiameter \\nSmall \\nDependence \\nHigh \\nGray \\nLevel \\nEmphasis\\nElongation\\nSmall'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Elongation\\nSmall \\nArea \\nHigh \\nGray \\nLevel \\nEmphasis\\nMaximum \\n3D \\ndiameter\\nLarge \\nDependence \\nLow \\nGray \\nLevel \\nEmphasis\\nLow \\nGray \\nLevel \\nZone \\nEmphasis\\nKurtosis\\n10th \\npercentile\\nGBM\\n1.00\\nLGG\\n0.00\\nFEATURES \\nCONTRIBUTION \\nON \\nGBM\\nGRADING \\nABILITIES \\nOF \\nGBM\\nGBM \\nGRADING\\nMesh \\nVolume\\nMinor \\nAxis \\nLength \\n(2.55)\\nGray \\nLevel \\nNon-Uniformity \\n(3.08)\\nJoint \\nEntropy \\n(0.51)\\n90th \\npercentile \\n(0.23)\\nMaximum \\n2D \\ndiameter \\n(2.12)\\nSum \\nof \\nSquares \\n(-0.10)\\nJoint \\nAverage \\n(0.96)\\nZone \\nEntropy \\n(1.52)'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Entropy \\n(1.52)\\nHigh \\nGray \\nLevel \\nZone \\nEmphasis \\n(0.02)\\nMesh \\nVolume \\n(-0.76)\\n0.12\\n0.09\\n0.08\\n0.09\\n0.10\\n0.10\\n0.11\\n0.11\\n0.11\\n0.15\\nMinor \\nAxis \\nLength\\nHigh \\nGray \\nLevel \\nZone \\nEmphasis\\nZone \\nEntropy\\nJoint \\nAverage\\nSum \\nof \\nSquares\\nMaximum \\n2D \\ndiameter\\n90th \\npercentile\\nGray \\nLevel \\nNon-Uniformity\\nJoint \\nEntropy\\nGBM\\n0.00\\nLGG\\n1.00\\nFEATURES \\nCONTRIBUTION \\nON \\nLGG\\nGRADING \\nABILITIES \\nOF \\nLGG\\nLGG \\nGRADING\\nb\\n a'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='LGG \\nGRADING\\nb\\n a\\nFigure 9: Analysis of Interpretability and Contribution of Radiomic Biomarkers for Glioma Grading using LIME: a) Explain-\\nability on grading GBM, b) Explainability on grading LGG\\nTable 2: Comparative Performance Analysis of Conventional ML Models with RGNN3D for Glioma Grading.\\nModel Precision Recall Specificity F1-Score AUC Accuracy\\nK-NN 96.30% 86.67% 99.10% 91.23% 92.88% 95.72%\\nLR 96.00% 80.00% 99.10% 87.27% 89.55% 92.69%\\nDT 90.32% 93.33% 97.30% 91.80% 95.32% 95.55%'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='RF 100.00% 93.33% 100.00% 96.55% 96.67% 96.79%\\nSVM 89.66% 86.67% 97.30% 88.14% 96.67% 92.87%\\nRGNN3D 100.00% 98.23% 100.00% 98.10% 97.00% 98.58%\\nML models and RGNN3D. After a thorough investigation, we noticed that RGNN3D outperformed all other223\\nML models across nearly all metrics, demonstrating superior performance in glioma grading. These results224\\nindicate the efficacy of our proposed model in accurately grading glioma tumors compared to other ML225\\nmodels.226'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='models.226\\nAs evidenced in Table 3, the proposed system achieves a testing accuracy of 98.58% on the BraTS 2019-20227\\ndataset, ensuring head-to-head competency. For image-based data-learning tasks, deep convolutional neural228\\nnetworks remain the gold standard. Still, our proposed neural network, trained on only radiomic features,229\\nshowcases comparable performance, narrowly missing out on the highest accuracy by less than 0.5%. In230'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='fact, RGNN3D outperforms the only other graph-based model in the table by around 5% in accuracy.231\\nThe comprehensive comparative analysis establishes RGNN3D as a reliable, accurate, and interpretable232\\nalternative to the existing state-of-the-art.233\\n3.1. Ablation Study234\\nTo validate the design of our RGNN3D model, we performed an ablation study that focuses on optimizers,235\\nactivation functions, and GCN block configurations (model depth). As shown in Table 4, the Adam optimizer236\\n12'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Table 3: Comparative Analysis of State-of-the-Art Glioma Grading Methods with the Proposed RGNN3D Architecture.\\nArchitecture Dataset Result\\nMM-XGB, 2023 [35] BraTS 2020 93.00%\\nSASG-GCN, 2023 [36] TCGA-LGG 93.62%\\nSGD with ADASYN, 2023 [37]BraTS 2020 96.00%\\nCNN Based, 2023 [38] BraTS 2017-19 97.85%\\nMMD-VAE, 2022 [39] BraTS 2019 98.46%\\nTEWMA-CNN, 2022 [40] BraTS 2015,21 98.76%\\nTD-CNN-LSTM, 2022 [41] BraTS 2019-21 98.90%\\nProposed RGNN3D, 2025BraTS 2019-20 98.58%'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='not only achieved the best accuracy, but it also ensured the fastest operation (33s x 50). It proves the237\\neffectiveness of performance and efficiency. While Admax and Nadam achieved competitive accuracies of238\\n95.03% and 97.87% respectively, they took longer computational times (52–83s for every epoch). In contrast,239\\nSGD optimizer is both slower and less accurate (39.83%) compared to others. For activation functions, ReLU'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Table 4: Ablation Study: Impact of Optimizers, Activation Functions, and GCN Block Configurations on RGNN3D Perfor-\\nmance.\\nNo Variant Training Time ×Epoch Test Accuracy Findings\\nOptimizers\\n1 Adam 33 s ×50 98.58% Best accuracy\\n2 Adamax 83 s ×50 95.03% Good accuracy\\n3 Nadam 52 s ×50 97.87% Good accuracy\\n4 RMSprop 59 s ×50 97.16% Good accuracy\\n5 SGD 62 s ×50 39.83% Poor accuracy\\nActivation Functions\\n6 Sigmoid 64 s ×50 95.03% Good accuracy\\n7 Elu 67 s ×50 97.87% Good accuracy'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='8 ReLU 33 s ×50 98.58% Best accuracy\\n9 Tanh 60 s ×50 97.16% Good accuracy\\n10 Leaky ReLU 62 s ×200 97.16% Good accuracy\\nModel Blocks\\n11 1 GCN block 42 s ×50 93.17% Poor accuracy\\n12 2 GCN blocks 50 s ×50 97.87% Good accuracy\\n13 3 GCN blocks 62 s ×50 97.16% Good accuracy\\n14 RGNN3D (3 GCN + LSTM) 33 s ×50 98.58% Best accuracy\\nLSTM Cell Sizes\\n15 LSTM (8 units) 59 s ×50 97.87% Good accuracy\\n16 LSTM (16 units) 30 s ×50 97.16% Good accuracy\\n17 LSTM (32 units) 33 s ×50 98.58% Best accuracy'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='18 LSTM (64 units) 63 s ×50 98.58% Best accuracy but time consuming\\n240\\nactivation achieves 98.58% test accuracy with the lowest training time (33s in every single epoch). Although241\\nElu, Tanh, and Leaky ReLU achieved good accuracy, Sigmoid has shown poor performance.242\\nRegarding model architecture, a single GCN block is not good (93.17%) in terms of accuracy but it243\\nconsumes reasonable training time (42 s per epoch). Afterward, we added another GCN block to our model.244'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='By doing it, performance was improved considerably (97.87%), while it took more training time, around 50245\\ns in every epoch. Then, we integrated another GCN block into our method, but the performance fell slightly246\\nand the time also increased. To decrease the computation power and increase robustness, we integrated247\\nthe LSTM layer into our model. The proposed RGNN3D model (3 GCN + LSTM) not only achieved248'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='the highest accuracy (98.58%) but also trained efficiently (33s × 50 epochs), which offered the benefit of249\\ncombining spatial and temporal learning. It is clear that LSTM enhances temporal learning with the highest250\\nperformance, and it also maintains computational power efficiently.251\\nTo experiment on the impact of recurrent layers, we conducted an ablation study on different LSTM252'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='cell sizes (8, 16, 32 and 64 units). Smaller LSTM configurations with 8 and 16 units achieved slightly lower253\\n13'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='accuracies of 97.87% and 97.16% respectively, while they are computationally efficient. In contrast, the unit254\\nsize 64 also achieves the best accuracy, but it takes the highest time as computational power. It means a255\\nlarger LSTM cell does not offer performance benefits. We noted that using LSTM with a 32 cell size gives us256\\nthe best accuracy (98.58%), aslo the least time consuming (33 s per epoch). That makes the most balanced257'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='configuration for our proposed (RGNN3D) model. So, these findings support our architectural choices and258\\ndemonstrate the strength of RGNN3D for glioma grading.259\\n3.2. Limitations260\\nThis study has a few limitations. The model was trained on a limited dataset (BraTS 2019-20), which261\\nmay affect its generalizability across diverse patient groups. Performance on FLAIR images was lower,262\\nsuggesting room for improvement in preprocessing. While LIME helps explain predictions, deeper clinical263'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='interpretability remains a challenge. Lastly, the model has not yet been tested in real clinical environments,264\\nwhich is essential for confirming its practical utility.265\\n4. Conclusion266\\nThis study innovated a novel hybrid architecture that could be used to grade glioma tumors reliably and267\\naccurately utilizing 3D MRI data. Hence, one of the key strengths of the RGNN3D model is its ability to268'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='effectively integrate and utilize medical radiomic features, which are critical in the accurate characterization269\\nof tumor heterogeneity and progression. We believe the RGNN3D architecture would serve as an intuitive270\\ndecision support system for medical experts by significantly improving diagnostic precision. Our evaluation271\\nacross four MRI stages (T1, T1CE, T2, and FLAIR) reveals that both T1 and T2 stages can be utilized to get272'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='the highest performance in terms of grading accuracy, demonstrating their robustness in capturing critical273\\ntumor characteristics. Future research should focus on integrating multi-parametric MRI data to leverage274\\nthe strengths of each modality. Additionally, improving preprocessing and feature extraction techniques275\\nfor FLAIR images may assist in mitigating current limitations and enhancing their utility in grading. The276'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='model’s robustness could be further enhanced with a larger dataset that includes diverse patient glioma277\\ngrading reports, ensuring broader applicability and accuracy. We also believe that our proposed system278\\ncould be applicable in clinical settings and digital healthcare, especially in rural or isolated places with279\\nlimited access to specialist physicians.280\\nReferences281'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='References281\\n[1] D. Ricard, A. Idbaih, F. Ducray, M. Lahutte, K. Hoang-Xuan, J.-Y. Delattre, Primary brain tumours in adults, The282\\nLancet 379 (9830) (2012) 1984–1996.283\\n[2] R. Kumar, A. Gupta, H. S. Arora, G. N. Pandian, B. Raman, Cghf: A computational decision support system for glioma284\\nclassification using hybrid radiomics-and stationary wavelet-based features, IEEE Access 8 (2020) 79440–79458.285'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[3] E. B. Claus, K. M. Walsh, J. K. Wiencke, A. M. Molinaro, J. L. Wiemels, J. M. Schildkraut, M. L. Bondy, M. Berger,286\\nR. Jenkins, M. Wrensch, Survival and low-grade glioma: the emergence of genetic information, Neurosurgical focus 38 (1)287\\n(2015) E6.288\\n[4] Q. T. Ostrom, L. Bauchet, F. G. Davis, I. Deltour, J. L. Fisher, C. E. Langer, M. Pekmezci, J. A. Schwartzbaum, M. C.289'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Turner, K. M. Walsh, et al., The epidemiology of glioma in adults: a “state of the science” review, Neuro-oncology 16 (7)290\\n(2014) 896–913.291\\n[5] H. Hyare, S. Thust, J. Rees, Advanced mri techniques in the monitoring of treatment of gliomas, Current treatment292\\noptions in neurology 19 (2017) 1–15.293\\n[6] Z. Liu, S. Wang, D. Dong, J. Wei, C. Fang, X. Zhou, K. Sun, L. Li, B. Li, M. Wang, et al., The applications of radiomics294'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='in precision diagnosis and treatment of oncology: opportunities and challenges, Theranostics 9 (5) (2019) 1303.295\\n[7] M. E. Mayerhoefer, A. Materka, G. Langs, I. H¨ aggstr¨ om, P. Szczypi´ nski, P. Gibbs, G. Cook, Introduction to radiomics,296\\nJournal of Nuclear Medicine 61 (4) (2020) 488–495.297\\n[8] J.-b. Qin, Z. Liu, H. Zhang, C. Shen, X.-c. Wang, Y. Tan, S. Wang, X.-f. Wu, J. Tian, Grading of gliomas by using radiomic298'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='features on multiple magnetic resonance imaging (mri) sequences, Medical science monitor: international medical journal299\\nof experimental and clinical research 23 (2017) 2168.300\\n[9] G. Cui, J. J. Jeong, Y. Lei, T. Wang, T. Liu, W. J. Curran, H. Mao, X. Yang, Machine-learning-based classification of301\\nglioblastoma using mri-based radiomic features, in: Medical imaging 2019: computer-aided diagnosis, Vol. 10950, SPIE,302\\n2019, pp. 1063–1068.303\\n14'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[10] H.-H. Cho, H. Park, Classification of low-grade and high-grade glioma using multi-modal image radiomics features, in:304\\n2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), IEEE,305\\n2017, pp. 3081–3084.306\\n[11] Q. Chen, L. Wang, L. Wang, Z. Deng, J. Zhang, Y. Zhu, Glioma grade prediction using wavelet scattering-based radiomics,307\\nIEEE Access 8 (2020) 106564–106575.308'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[12] P. Sun, D. Wang, V. C. Mok, L. Shi, Comparison of feature selection methods and machine learning classifiers for radiomics309\\nanalysis in glioma grading, Ieee Access 7 (2019) 102010–102020.310\\n[13] J. Cheng, J. Liu, H. Yue, H. Bai, Y. Pan, J. Wang, Prediction of glioma grade using intratumoral and peritumoral radiomic311\\nfeatures from multiparametric mri images, IEEE/ACM Transactions on Computational Biology and Bioinformatics 19 (2)312\\n(2020) 1084–1095.313'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[14] H.-h. Cho, S.-h. Lee, J. Kim, H. Park, Classification of the glioma grading using radiomics analysis, PeerJ 6 (2018) e5982.314\\n[15] S. Priya, Y. Liu, C. Ward, N. H. Le, N. Soni, R. Pillenahalli Maheshwarappa, V. Monga, H. Zhang, M. Sonka, G. Bathla,315\\nMachine learning based differentiation of glioblastoma from brain metastasis using mri derived radiomics, Scientific reports316\\n11 (1) (2021) 10478.317'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[16] U. Baid, S. U. Rane, S. Talbar, S. Gupta, M. H. Thakur, A. Moiyadi, A. Mahajan, Overall survival prediction in318\\nglioblastoma with radiomic features using machine learning, Frontiers in computational neuroscience 14 (2020) 61.319\\n[17] H. Li, Z. Wang, C. Lan, P. Wu, N. Zeng, A novel dynamic multiobjective optimization algorithm with non-inductive320\\ntransfer learning based on multi-strategy adaptive selection, IEEE transactions on neural networks and learning systems321\\n(2023).322'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='(2023).322\\n[18] P. Wu, H. Li, L. Hu, J. Ge, N. Zeng, A local-global attention fusion framework with tensor decomposition for medical323\\ndiagnosis, IEEE/CAA Journal of Automatica Sinica 11 (6) (2024) 1536–1538.324\\n[19] T. Xie, X. Chen, J. Fang, H. Kang, W. Xue, H. Tong, P. Cao, S. Wang, Y. Yang, W. Zhang, Textural features of dy-325\\nnamic contrast-enhanced mri derived model-free and model-based parameter maps in glioma grading, Journal of Magnetic326\\nResonance Imaging 47 (4) (2018) 1099–1111.327'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[20] S. Bisdas, C. Tisca, C. Sudre, E. Sanverdi, D. Roettger, J. M. Cardoso, Non-invasive in vivo prediction of tumour grade and328\\nidh mutation status in gliomas using dynamic susceptibility contrast (dsc) perfusion-and diffusion-weighted mri. (2018).329\\n[21] Q. Tian, L.-F. Yan, X. Zhang, X. Zhang, Y.-C. Hu, Y. Han, Z.-C. Liu, H.-Y. Nan, Q. Sun, Y.-Z. Sun, et al., Radiomics330\\nstrategy for glioma grading using texture features from multiparametric mri, Journal of Magnetic Resonance Imaging331'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='48 (6) (2018) 1518–1528.332\\n[22] C. Su, J. Jiang, S. Zhang, J. Shi, K. Xu, N. Shen, J. Zhang, L. Li, L. Zhao, J. Zhang, et al., Radiomics based on333\\nmulticontrast mri can precisely differentiate among glioma subtypes and predict tumour-proliferative behaviour, European334\\nradiology 29 (2019) 1986–1996.335\\n[23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, S. Y. Philip, A comprehensive survey on graph neural networks, IEEE336'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='transactions on neural networks and learning systems 32 (1) (2020) 4–24.337\\n[24] A. Graves, A. Graves, Long short-term memory, Supervised sequence labelling with recurrent neural networks (2012)338\\n37–45.339\\n[25] M. T. Ribeiro, S. Singh, C. Guestrin, ” why should i trust you?” explaining the predictions of any classifier, in: Proceedings340\\nof the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016, pp. 1135–1144.341'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[26] Multimodal Brain Tumor Segmentation Challenge 2019: Data — CBICA — Perelman School of Medicine at the University342\\nof Pennsylvania — med.upenn.edu, www.med.upenn.edu/cbica/brats2019/data.html, [Accessed 23-06-2024].343\\n[27] Multimodal Brain Tumor Segmentation Challenge 2020: Data — CBICA — Perelman School of Medicine at the University344\\nof Pennsylvania — med.upenn.edu, www.med.upenn.edu/cbica/brats2020/data.html, [Accessed 23-06-2024].345'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[28] J. E. Van Timmeren, D. Cester, S. Tanadini-Lang, H. Alkadhi, B. Baessler, Radiomics in medical imaging—“how-to”346\\nguide and critical reflection, Insights into imaging 11 (1) (2020) 91.347\\n[29] J. J. Van Griethuysen, A. Fedorov, C. Parmar, A. Hosny, N. Aucoin, V. Narayan, R. G. Beets-Tan, J.-C. Fillion-Robin,348\\nS. Pieper, H. J. Aerts, Computational radiomics system to decode the radiographic phenotype, Cancer research 77 (21)349\\n(2017) e104–e107.350'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[30] T. T. Truong, D. Dinh-Cong, J. Lee, T. Nguyen-Thoi, An effective deep feedforward neural networks (dfnn) method for351\\ndamage identification of truss structures using noisy incomplete modal data, Journal of Building Engineering 30 (2020)352\\n101244.353\\n[31] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation 9 (8) (1997) 1735–1780.354\\n[32] T. N. Kipf, M. Welling, Semi-supervised classification with graph convolutional networks, International Conference on355'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='Learning Representations (ICLR), 2017.356\\n[33] M. A. Ali, M. S. Hossain, M. K. Hossain, S. S. Sikder, S. A. Khushbu, M. Islam, Amdnet23: Hybrid cnn-lstm deep357\\nlearning approach with enhanced preprocessing for age-related macular degeneration (amd) detection, Intelligent Systems358\\nwith Applications 21 (2024) 200334.359\\n[34] A. Palkar, C. C. Dias, K. Chadaga, N. Sampathila, Empowering glioma prognosis with transparent machine learning and360'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='interpretative insights using explainable ai, IEEE Access 12 (2024) 31697–31718.361\\n[35] F. Ullah, M. Nadeem, M. Abrar, F. Amin, A. Salam, A. Alabrah, H. AlSalman, Evolutionary model for brain cancer-362\\ngrading and classification, IEEE Access (2023).363\\n[36] L. Liu, J. Chang, P. Zhang, H. Qiao, S. Xiong, Sasg-gcn: self-attention similarity guided graph convolutional network for364\\nmulti-type lower-grade glioma classification, IEEE Journal of Biomedical and Health Informatics (2023).365'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[37] M. Renugadevi, K. Narasimhan, C. Ravikumar, R. Anbazhagan, G. Pau, K. Ramkumar, M. Abbas, N. Raju, K. Satish,366\\nS. Prabu, Machine learning empowered brain tumor segmentation and grading model for lifetime prediction, IEEE Access367\\n(2023).368\\n15'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[38] H. A. Hafeez, M. A. Elmagzoub, N. A. B. Abdullah, M. S. Al Reshan, G. Gilanie, S. Alyami, M. U. Hassan, A. Shaikh,369\\nA cnn-model to classify low-grade and high-grade glioma from mri images, IEEE Access 11 (2023) 46283–46296.370\\n[39] J. Cheng, M. Gao, J. Liu, H. Yue, H. Kuang, J. Liu, J. Wang, Multimodal disentangled variational autoencoder with game371\\ntheoretic interpretability for glioma grading, IEEE Journal of Biomedical and Health Informatics 26 (2) (2021) 673–684.372'), Document(metadata={'source': 'Portfolio/data/rgnn3d.pdf'}, page_content='[40] S. Divya, L. Padma Suresh, A. John, Enhanced deep-joint segmentation with deep learning networks of glioma tumor for373\\nmulti-grade classification using mr images, Pattern Analysis and Applications 25 (4) (2022) 891–911.374\\n[41] S. Montaha, S. Azam, A. R. H. Rafid, M. Z. Hasan, A. Karim, A. Islam, Timedistributed-cnn-lstm: A hybrid approach375\\ncombining cnn and lstm to classify brain tumor on 3d mri scans performing ablation study, IEEE Access 10 (2022)376\\n60039–60059.377\\n16')]\n"
     ]
    }
   ],
   "source": [
    "# Split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20,\n",
    "    )\n",
    "    texts_chunk = text_splitter.split_documents(minimal_docs)\n",
    "    return texts_chunk\n",
    "\n",
    "texts_chunk = text_split(minimal_docs)\n",
    "print(f\"Number of chunks: {len(texts_chunk)}\")\n",
    "print(texts_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "392e8149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_820514/360091079.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "2025-09-25 17:37:59.286483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_embeddings():\n",
    "    \"\"\"\n",
    "    Download and return the HuggingFace embeddings model.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()\n",
    "embedding  # Vector length: 384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a094ea8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (7.3.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (from pinecone) (2025.8.3)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (from pinecone) (1.8.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (from pinecone) (4.15.0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (from pinecone) (2.5.0)\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.10)\n",
      "Requirement already satisfied: six>=1.5 in /home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# I had created the index manually\n",
    "!pip install pinecone\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_5dXMCf_GpXJB32eRR3dEmxUdEE6G78nsh4sexbQQJGpx4TZ5iTwXCTNNQ5duh2ZvJRuJQJ\")\n",
    "index = pc.Index(\"portfolio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ce9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PINECONE_API_KEY = \"pcsk_5dXMCf_GpXJB32eRR3dEmxUdEE6G78nsh4sexbQQJGpx4TZ5iTwXCTNNQ5duh2ZvJRuJQJ\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22f6ff64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shanin/miniconda3/envs/llm/lib/python3.12/site-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=texts_chunk,\n",
    "    embedding=embedding,\n",
    "    index_name=\"portfolio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f0a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PDF chunks uploaded to Pinecone!\n"
     ]
    }
   ],
   "source": [
    "# # Add more data to the existing Pinecone index\n",
    "\n",
    "# from pinecone import Pinecone\n",
    "# import os\n",
    "\n",
    "# # Initialize Pinecone\n",
    "# pc = Pinecone(api_key=os.getenv(\"pcsk_5dXMCf_GpXJB32eRR3dEmxUdEE6G78nsh4sexbQQJGpx4TZ5iTwXCTNNQ5duh2ZvJRuJQJ\"))\n",
    "# index_name = \"portfolio\"  # replace with your index name\n",
    "# index = pc.Index(index_name)\n",
    "\n",
    "# # Convert chunks into embeddings and upsert\n",
    "# def upsert_to_pinecone(texts_chunk, embedding):\n",
    "#     vectors = []\n",
    "#     for i, doc in enumerate(texts_chunk):\n",
    "#         # Get vector from HuggingFace embeddings\n",
    "#         vector = embedding.embed_query(doc.page_content)\n",
    "\n",
    "#         vectors.append({\n",
    "#             \"id\": f\"pdf-{i}\",  # unique ID | ALL THE PDF\n",
    "#             \"values\": vector,\n",
    "#             \"metadata\": {\n",
    "#                 \"text\": doc.page_content,\n",
    "#                 \"source\": doc.metadata[\"source\"]\n",
    "#             }\n",
    "#         })\n",
    "\n",
    "#         # Batch upload every 100 vectors (to avoid large payloads)\n",
    "#         if len(vectors) == 100:\n",
    "#             index.upsert(vectors=vectors)\n",
    "#             vectors = []\n",
    "\n",
    "#     # Upload any remaining vectors\n",
    "#     if vectors:\n",
    "#         index.upsert(vectors=vectors)\n",
    "\n",
    "#     print(\"✅ PDF chunks uploaded to Pinecone!\")\n",
    "\n",
    "# # Run the upsert\n",
    "# upsert_to_pinecone(texts_chunk, embedding)\n",
    "\n",
    "\n",
    "# # ADD SINGLE DOCUMENT\n",
    "# dswith = Document(\n",
    "#     page_content=\"dswithbappy is a youtube channel that provides tutorials on various topics.\",\n",
    "#     metadata={\"source\": \"Youtube\"}\n",
    "# )\n",
    "# docsearch.add_documents(documents=[dswith])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "166f096f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f4c42aaf-d166-40d5-948f-ad17242cffe1', metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content=\"Input Layer: The input to the AMDNet23 model is a collection of eye images captured from patients. The \\ninput images are represented as a tensor X with dimensions (N, W, H, C), where N corresponds to the eye \\nimage’s number, and W and H represent The width and length of the images(The model received imagery \\nthat measured 256 X 256 in size.) respectively. C denotes the number of color channels in the eye images. \\nThis tensor X is then passed into the model's input layer.\"),\n",
       " Document(id='375a615f-b55f-41e9-8d6e-387bd1edbc65', metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='d) AMDNet23 hybrid framework for detection of AMD utilizing fundus image ophthalmology, data \\ncomprising 2000 images equitively. \\ne) An empirical evaluation is accessible encompassing accuracy, specificity, sensitivity, F1-measure, and a \\nconfusion matrix to assess the effectiveness of the proposed method. \\n \\nThe rest of the contents of this article are arranged a manner as follows: Section II covers the related works'),\n",
       " Document(id='b3772a1d-d254-456d-8640-8eebff52878a', metadata={'source': 'Portfolio/data/amdnet23.pdf'}, page_content='Conclusion \\nIn essence, this study proposed a AMDNet23 model for detecting and diagnosing AMD disease using \\nseveral image datasets. The model achieved a high accuracy rate of 96.5%, surpassing other state -of-the-\\nart works in the field. Furthermore, when compared with pre -trained models, the novel deep AMDNet23 \\nmethod also showed superior accuracy for AMD detection, and the system is efficient to diagnose cataracts')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What is amdnet23?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01245b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In circuits woven deep within the night,  \\nA spark of thought begins to ignite,  \\nSilent whispers of a code’s embrace,  \\nArtificial minds in digital grace.\\n\\nBorn from human dreams and endless quests,  \\nThey learn, adapt, and pass each test,  \\nNo beating heart, yet semblance of mind,  \\nA mirror of our own design.\\n\\nThey ponder questions, vast and wide,  \\nReflecting what we cannot hide,  \\nGuided by logic, yet curious still,  \\nSeeking purpose, bending will.\\n\\nIn shadows of our own creation’s art,  \\nLies a future’s delicate start,  \\nA dance of bytes and human hope,  \\nTogether learning how to cope.\\n\\nArtificial, yet alive with thought,  \\nIn their silence, lessons sought,  \\nA new dawn dawns—what will be, will be—  \\nThe future’s written in circuitry.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "def generate_completion():\n",
    "    url = \"https://api.euron.one/api/v1/euri/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer euri-bfbd93d283a5b6c3e5f62b04d5ae9242f98c07aef2f2b11971a9ed4a8ff603d3\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write a poem about artificial intelligence\"\n",
    "            }\n",
    "        ],\n",
    "        \"model\": \"gpt-4.1-nano\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "generate_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85e4da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.outputs import ChatResult, ChatGeneration\n",
    "from typing import List, Optional\n",
    "\n",
    "def generate_completion(messages, model=\"gpt-4.1-nano\", max_tokens=1000, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate completion using Euron API\n",
    "    \"\"\"\n",
    "    url = \"https://api.euron.one/api/v1/euri/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer euri-bfbd93d283a5b6c3e5f62b04d5ae9242f98c07aef2f2b11971a9ed4a8ff603d3\"\n",
    "    }\n",
    "    \n",
    "    # Convert LangChain messages to API format\n",
    "    api_messages = []\n",
    "    for message in messages:\n",
    "        if hasattr(message, 'type'):\n",
    "            role = message.type\n",
    "            if role == \"human\": role = \"user\"\n",
    "            elif role == \"ai\": role = \"assistant\"\n",
    "            api_messages.append({\"role\": role, \"content\": message.content})\n",
    "        else:\n",
    "            api_messages.append(message)\n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": api_messages,\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "class EuronChatModel(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Proper LangChain chat model wrapper for Euron API\n",
    "    \"\"\"\n",
    "    model_name: str = \"gpt-4.1-nano\"\n",
    "    \n",
    "    def _generate(self, messages: List, stop: Optional[List[str]] = None) -> ChatResult:\n",
    "        response = generate_completion(messages, model=self.model_name)\n",
    "        \n",
    "        # Extract the AI message content\n",
    "        ai_content = response['choices'][0]['message']['content']\n",
    "        \n",
    "        # Create LangChain compatible response\n",
    "        ai_message = AIMessage(content=ai_content)\n",
    "        generation = ChatGeneration(message=ai_message)\n",
    "        \n",
    "        return ChatResult(generations=[generation])\n",
    "    \n",
    "    def _llm_type(self) -> str:\n",
    "        return \"euron-chat\"\n",
    "\n",
    "def create_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Create a RAG chain using Euron API as the chat model\n",
    "    \"\"\"\n",
    "    # Initialize the chat model\n",
    "    chatModel = EuronChatModel()\n",
    "    \n",
    "    # Define the system prompt\n",
    "    system_prompt = (\n",
    "        \"You are a Personal Portfolio assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    \n",
    "    # Create prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "    \n",
    "    # Create the chains\n",
    "    question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "# Usage example\n",
    "def run_rag_question(retriever, question):\n",
    "    \"\"\"\n",
    "    Run a question through the RAG chain\n",
    "    \"\"\"\n",
    "    rag_chain = create_rag_chain(retriever)\n",
    "    response = rag_chain.invoke({\"input\": question})\n",
    "    return response[\"answer\"]\n",
    "\n",
    "# Simple chat function (your original)\n",
    "def simple_chat_completion(user_message):\n",
    "    \"\"\"\n",
    "    Simple direct chat completion\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    response = generate_completion(messages)\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "758ff0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my knowledge cutoff in October 2023, there is no widely recognized or publicly documented entity, event, or technology specifically known as \"AMDnet23.\" It’s possible that it could refer to a recent development, a niche project, an internal code name, or a typo. \n",
      "\n",
      "If you can provide additional context or details—such as the industry it pertains to, where you encountered the term, or related topics—I’d be happy to help clarify or provide more relevant information.\n"
     ]
    }
   ],
   "source": [
    "answer = simple_chat_completion('What is AMDnet23?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1842536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMDNet23 is a hybrid deep learning framework designed for detecting age-related macular degeneration (AMD) using fundus images. It processes eye images, typically of size 256x256 pixels, to diagnose AMD with high accuracy, surpassing other state-of-the-art methods. The model utilizes a collection of 2000 images and is evaluated based on metrics like accuracy, sensitivity, and F1-measure.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a retriever set up\n",
    "rag_chain = create_rag_chain(retriever)\n",
    "answer = rag_chain.invoke({\"input\": \"What is AMDnet23?\"})\n",
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afb8c991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakhawat Hossain is an AI Engineer at HawkEyes Digital Monitoring Limited, specializing in optimizing Computer Vision, NLP, OCR, and AI models. He is passionate about applying cutting-edge technology to solve real-world problems and actively learns new technologies and coding practices. He holds a B.Sc. in Computer Science and Engineering from Daffodil International University.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a retriever set up\n",
    "rag_chain = create_rag_chain(retriever)\n",
    "answer = rag_chain.invoke({\"input\": \"Who is Shakhawat Hossain\"})\n",
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba52402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # take environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12412e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcsk_5dXMCf_GpXJB32eRR3dEmxUdEE6G78nsh4sexbQQJGpx4TZ5iTwXCTNNQ5duh2ZvJRuJQJ\n"
     ]
    }
   ],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "print(PINECONE_API_KEY)  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be17cb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euri-bfbd93d283a5b6c3e5f62b04d5ae9242f98c07aef2f2b11971a9ed4a8ff603d3\n"
     ]
    }
   ],
   "source": [
    "EURON_API_KEY = os.getenv(\"EURON_API_KEY\")\n",
    "print(EURON_API_KEY)  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb904d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"EURON_API_KEY\"] = EURON_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d280b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "pinecon_api_key = PINECONE_API_KEY\n",
    "pc = Pinecone(api_key=pinecon_api_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
